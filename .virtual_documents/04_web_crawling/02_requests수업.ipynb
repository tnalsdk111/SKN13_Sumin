%pip install requests -qU



































url = "https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query={}"
keyword = input("keyword:")
url = url.format(keyword)
print(url)

res = requests.get(url)
print(res)
print(res.status_code)


import requests

url = "https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query={}"
keyword = input("keyword:")
url = url.format(keyword)

res = requests.get(url) # 요청 - 응답 완료: 반환:응답데터터
print(res.status_code) # 응답 상태 코드
if res.status_code == 200:
    print(type(res))
    print(type(res.text), len(res.text)) 
    print(res.text[:200])
else:
    print("응답을 받지 못함.", res.status_code)






import requests
from pprint import pprint

base_url = "https://httpbin.org/{}"
url = base_url.format("get")
print("요청 URL:", url)

params = {
    "query":"python",  # name : value
    "fbm": 0,
    "page": 3
}

req_headers = {
    "user-agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36"
    , "Referer":"https://www.google.com/"
}

res = requests.get(url,                  # url
                   params=params,        # 요청파라미터들
                   headers=req_headers)  # 요청 header 정보들 
print("응답상태코드:",  res.status_code)
if res.status_code == 200:
    print("============응답데이터(text)==========")
    print(res.text)
    d2 = res.json() # JSON 문자열을 dictionary로 변환해서 반환.
    print("==========응답헤더============")
    print(res.headers)





url = base_url.format("post")
print(url)
# post방식은 요청 파라미터를 url뒤에 붙이지 않고 dictionary로 정의해서 함수에 전달.
params = {
    "query":"python",
    "fbm": 0,
    "page": 3
}
req_headers = {
    "user-agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
    , "Referer":"https://www.google.com/"
}

res = requests.post(
    url,
    data=params,      
    headers=req_headers
)

if res.status_code == 200:
    print(res.text)     # -> str
    print(res.json())  # JSON -> dict
else:
    print("실패:", res.status_code)


d2['args']





from bs4 import BeautifulSoup
import requests

url = 'http://www.pythonscraping.com/pages/warandpeace.html'

user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"

res = requests.get(url, headers={"user-agent":user_agent})

if res.status_code == 200:

    soup = BeautifulSoup(res.text, "lxml")
    green_list = soup.select("span.green")
    search_names = []
    for tag in green_list:
        search_names.append(tag.text.replace("\n", ' '))
else:
    print("실패:", res.status_code)


result = [name.replace("\n", " ") for name in search_names]
result





url = "https://www.kia.com/content/dam/kwp/kr/ko/main-kv-contents/202311/kv_the_new_carnival_pc.jpg"

res = requests.get(url, headers={"user-agent":user_agent})

if res.status_code == 200:
    file = res.content  # binary 데이터 -> bytes으로 반환.
    print(type(file))
    with open("car.jpg", "wb") as fo:
        fo.write(file)








from datetime import datetime
datetime.now().strftime("%Y-%m-%d-%H-%M-%S")+".csv"


# 실습\daum_new_list.py
## https://news.daum.net

# pip install requests beautifulsoup4
import requests
from bs4 import BeautifulSoup

url = "https://news.daum.net"
# 뉴스제목: <a>의 content, 링크주소: <a>의 href 속성값
a_selector = "body > div.container-doc > main > section > div > div.content-article > div.box_g.box_news_issue > ul > li > div > div > strong > a"
# user-agent: 1.개발자도구>콘솔: navigator.userAgent, 2. google검색: my user agent 검색
user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'

def get_daum_news_list():
    """
    다음 뉴스 기사 목록을 크롤링하는 함수.
    news.daum.net의 기사 목록에서 "제목", "링크" 들을 수집.

    aguments
    return
        DataFrame: 조회결과들을 담은 DataFrame(표)
    raise
        Exception: 처리 실패시 발생
    """
    # 1. 요청
    res = requests.get(url, headers={"user-agent":user_agent})
    
    # 2. 응답 페이지에서 필요한 정보 추출
    if res.status_code == 200:
        soup = BeautifulSoup(res.text, "lxml")
        a_list = soup.select(a_selector)
        result_list = []
        for a_tag in a_list:
            title = a_tag.get_text()
            link = a_tag.get("href")
            result_list.append([title.strip(), link])
        
        return result_list
    else:
        raise Exception(f"요청 실패. 응답코드: {res.status_code}")      

if __name__ == "__main__":
    result = get_daum_news_list()
# 저장할 디렉토리를 생성
    import os
    from datetime import datetime
    import pandas as pd
    save_dir = "daum_news_list"
    os.makedirs(save_dir, exist_ok=True)
    
    # 저장할 파일명 - 특정 기간마다 크롤링 수행할 경우 실행 날짜/시간을 이용해서 만들어 준다.
    d = datetime.now().strftime("%Y-%m-%d-%H-%M-%S")
    file_path = f"{save_dir}/{d}.csv"
    # DataFrame 생성
    result_df = pd.DataFrame(result, columns=['제목', "링크주소"])
    # csv 파일로 저장.
    result_df.to_csv(file_path, index=False)











import requests
import json


url = 'https://api.odcloud.kr/api/15127133/v1/uddi:ea3c3b5a-3bd8-4faf-b155-bb2af3cc3377'
with open('api_key.json', 'rt') as fr:
    key_dict = json.load(fr)

key = key_dict['apikey']
params ={'serviceKey' : key_dict["apikey"],
         'pageNo' : 1, 
         'perPage': 20,}

response = requests.get(url, params=params)
if response.status_code == 200:
    result = response.json()
    print(type(result))
    from pprint import pprint
    print(len(result['data']))
    pprint(result['data'])


result
