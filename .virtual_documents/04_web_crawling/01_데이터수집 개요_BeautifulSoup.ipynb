

















import os
print(os.getcwd())


## example.html ìƒì„±
# ë¬¸ì„œ ë‚´ìš©
html_content = """
<html>
  <head>
    <title>Example Page</title>
  </head>
  <body>
    <h1>Hello, BeautifulSoup!</h1>
    <p>This is a paragraph.</p>
    <a href="https://example.com">Visit Example.com</a>
  </body>
</html>
"""

# íŒŒì¼ ìƒì„±
with open("example.html", "w", encoding="utf-8") as f:
    f.write(html_content)

print("example.html ìƒì„± ì™„ë£Œ!")



## íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°

# HTMLì„ íŒŒì‹±í•˜ê¸° ìœ„í•œ BeautifulSoup ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.
from bs4 import BeautifulSoup

# example.html íŒŒì¼ì„ ì½ê¸° ëª¨ë“œë¡œ ì—°ë‹¤.
# r: read, ì½ê¸° ì „ìš© / t: text, í…ìŠ¤íŠ¸ ëª¨ë“œ 
with open("example.html", "rt", encoding='utf-8') as fr:
    html_doc = fr.read() # htmlì˜ ì „ì²´ ë‚´ìš©ì„ html_docì— ë¬¸ìì—´ë¡œ ì €ì¥í•œë‹¤.

# ì˜ ë¶ˆëŸ¬ì™€ì¡ŒëŠ”ì§€ í™•ì¸ - ì²˜ìŒ 50ê¸€ìë§Œ ì¶œë ¥í•´ì„œ ë‚´ìš©ì„ í™•ì¸í•œë‹¤. 
print(html_doc[:50])
print(type(html_doc)) # str


## html_docì„ beautifulsoupìœ¼ë¡œ íŒŒì‹±í•´ì„œ HTML êµ¬ì¡°ë¥¼ ë¶„ì„í•œë‹¤.
soup = BeautifulSoup(html_doc, "lxml")


## êµ¬ì¡°í™”ëœ HTMLì´ ì˜ˆì˜ê²Œ ì¶œë ¥ëœë‹¤. 
print(soup.prettify())

















## example_news.html íŒŒì¼ ìƒì„±

# ë¬¸ì„œ ë‚´ìš©
html_content = """
<html>
  <body>
    <div class="news">
      <h2>ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤</h2>
      <p class="content">ë‚ ì”¨ê°€ ë§‘ìŠµë‹ˆë‹¤.</p>
    </div>
    <div class="news">
      <h2>ìŠ¤í¬ì¸  ë‰´ìŠ¤</h2>
      <p class="content">ì¶•êµ¬ì—ì„œ ìŠ¹ë¦¬í–ˆìŠµë‹ˆë‹¤.</p>
    </div>
  </body>
</html>
"""

# íŒŒì¼ ìƒì„±
# open() í•¨ìˆ˜ë¡œ íŒŒì¼ ì—´ê¸°
# ì£¼ìš” íŒŒë¼ë¯¸í„°: íŒŒì¼ëª…, ëª¨ë“œ, ì¸ì½”ë”©
with open("example_news.html", "w", encoding = "utf-8") as f: 
    f.write(html_content)


## íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°

# HTMLì„ íŒŒì‹±í•˜ê¸° ìœ„í•œ BeautifulSoup ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤. 
from bs4 import BeautifulSoup

# example_news.htmlì„ ì½ê¸° ëª¨ë“œë¡œ ì—°ë‹¤.
with open("example_news.html", "rt", encoding = "utf-8") as fr:
    html_doc = fr.read()

# ì˜ ë¶ˆëŸ¬ì™€ì¡ŒëŠ”ì§€ í™•ì¸
print(html_doc[:100])








# html_docì„ BeautifulSoupìœ¼ë¡œ íŒŒì‹±
soup = BeautifulSoup(html_doc, "lxml")





print(html_doc)


print(soup) # íŒŒì‹± ê²°ê³¼ ì¶œë ¥


print(soup.prettify())





### 1ë‹¨ê³„: íƒœê·¸ëª…, í´ë˜ìŠ¤ë¡œ ìš”ì†Œ ì°¾ê¸°


# ëª¨ë“  ë‰´ìŠ¤ êµ¬ì—­ ê°€ì ¸ì˜¤ê¸°
print("1. ëª¨ë“  ë‰´ìŠ¤ êµ¬ì—­ ê°€ì ¸ì˜¤ê¸°: class = \"news\"ì¸ ê²ƒ\n")
print(soup.find_all("div", class_="news"), "\n\n") 

# ê° ë‰´ìŠ¤ì˜ ë³¸ë¬¸ë§Œ
print("2. ê° ë‰´ìŠ¤ì˜ ë³¸ë¬¸ë§Œ ê°€ì ¸ì˜¤ê¸° - div(êµ¬ì—­ ë‚˜ëˆ„ëŠ” íƒœê·¸) ì•„ë˜ p(ë³¸ë¬¸ íƒœê·¸):\n")
print(soup.select("div.news > p.content"))  


### 2ë‹¨ê³„: í…ìŠ¤íŠ¸ êº¼ë‚´ê¸°


print("ë‰´ìŠ¤ ì œëª© ê°€ì ¸ì˜¤ê¸°:")
print(soup.find("h2").get_text())
# 1. find("h1"): "h2" íƒœê·¸ë¥¼ ê°€ì§„ ìš”ì†Œë¥¼ ì°¾ì•„ì„œ
# 2. get_text(): íƒìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¨ë‹¤. 





title_tag = soup.find("h1")

if title_tag:
    print("ë‰´ìŠ¤ ì œëª©:", title_tag.get_text())
else:
    print("h1 íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ ğŸ˜¥")



### ì ì‹œ ìˆ˜ì •!

# <p> íƒœê·¸ ì„ íƒ
p_tag = soup.find("p", class_="content")
# (ì°¸ê³ ) ì²« ë²ˆì§¸ p íƒœê·¸ ë’¤ì— ë‚´ìš©ì„ ë¶™ì¼ ê±°ë¼ find()ë¥¼ ì”€. 
# ë‘ ë²ˆì§¸ ê²ƒì„ ìˆ˜ì •í•˜ê³  ì‹¶ë‹¤ë©´ find_all() ì¨ì„œ ë‹¤ ë¶ˆëŸ¬ì˜¤ì.

# <a> íƒœê·¸ ìƒˆë¡œ ë§Œë“¤ê¸°
new_a = soup.new_tag("a", href="https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=%EB%82%A0%EC%94%A8")
new_a.string = "ì˜¤ëŠ˜ ë‚ ì”¨ ë³´ê¸°"

# <p> íƒœê·¸ ë°”ë¡œ **ë’¤ì—** <a> íƒœê·¸ ì¶”ê°€
p_tag.insert_after(new_a)

# ê²°ê³¼ ë³´ê¸°
print(soup.prettify())


# ì‹¤ì œ html íŒŒì¼ì— ì €ì¥í•˜ë ¤ë©´ open() í•¨ìˆ˜ë¥¼ í†µí•´ write() í•´ì£¼ì–´ì•¼ í•œë‹¤.
with open("example_news.html", "w", encoding="utf-8") as f:
    f.write(str(soup))  # ìˆ˜ì •ëœ soup ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•´ì„œ ì €ì¥


### 3ë‹¨ê³„: ì†ì„± êº¼ë‚´ê¸°


print(soup.find("h2"))





from bs4 import BeautifulSoup

with open("example.html", "rt", encoding="utf-8") as fr:
    html_doc = fr.read()

soup = BeautifulSoup(html_doc, "lxml")


result = soup.find_all("div")


print(len(result))
result


tag1 = result[0]
print("content:", tag1.text, tag1.get_text())
print("classì†ì„±ê°’:", tag1.get("class"), tag1['class'])


result = soup.find("div")
print(type(result))
print("-"*50)
print(result)


# íƒœê·¸ì˜ contentì™€ attribute ì¡°íšŒ

print("content text:", result.text)
print("-"*50)
print("content text:", result.get_text())
print("-"*50)
print("attribueì˜ value:", result.get("class"))
print("-"*50)
print("attribueì˜ value:", result["class"])


# íƒœê·¸ì˜ ëª¨ë“  ìì‹ ìš”ì†Œë“¤ ì¡°íšŒ
result.contents


from pprint import pprint

result = soup.find_all("a") 
print(result)
print("-"*50)
result = soup.find_all(["a", "span"])  #í•œë²ˆì— ì—¬ëŸ¬ì´ë¦„ì˜ íƒœê·¸ë“œì„ ì¡°íšŒ.
print(result)
print("-"*50)
result = soup.find_all("div", attrs={"class":"name"}) # íƒœê·¸ì´ë¦„ + ì†ì„±
print(result)
print("-"*50)
result = soup.find_all("div", attrs={"class":"animal_info", "id":"animal1"}) # ì†ì„± ì¡°ê±´ì´ ì—¬ëŸ¬ê°œ
print(result)
print("-"*50)
result = soup.find_all("a", attrs={"href":"https://www.coexaqua.com"})

# import re
# result = soup.find_all("a", attrs={"href":re.compile(r".com$")}) # ì •ê·œí‘œí˜„ì‹-.comìœ¼ë¡œ ëë‚˜ëŠ”.

pprint(result)


result_list = []
for tag in result:
    print(tag.text, tag['href'])
    result_list.append([tag.text, tag['href']]) # list[text, href]



result_list





from bs4 import BeautifulSoup

with open("example.html", "rt", encoding="utf-8") as fr:
    html_doc = fr.read()

soup = BeautifulSoup(html_doc, "lxml")



# css selectorë¥¼ ì´ìš©í•œ ì¡°íšŒ

result = soup.select("a")         #  íƒœê·¸ì´ë¦„(a)  
# result = soup.select("a, span") # íƒœê·¸ì´ë¦„(ì—¬ëŸ¬ê°œ)
# result = soup.select("ul a")    # ulì˜ ìì†ì¸ aíƒœê·¸ ì°¾ëŠ”ë‹¤.

# result = soup.select_one("#animal1")             # ëª¨ë“  íƒœê·¸ì¤‘ id=animal1
# result = soup.select("ul + div")                 # ulì˜ ë‹¤ìŒ í˜•ì œ íƒœê·¸ì¤‘ div
# result = soup.select("body > div:nth-child(3)")  # bodyì˜ 3ë²ˆì§¸ ìì‹ div

# result = soup.select("a[href]")                        # href ì†ì„±ì´ ìˆëŠ” a íƒœê·¸ë“¤
# result = soup.select("a[href='http://www.naver.com']") # href='http://www.naver.com' ì†ì„±ì„ ê°€ì§„ a íƒœê·¸
# result = soup.select('a[href$=".do"]')                 # $=  href ì†ì„±ê°’ì´ .doë¡œ ëë‚˜ëŠ” aíƒœê·¸ë“¤
# result = soup.select('a[href^="https"]')               # =^  href ì†ì„±ê°’ì´ httpsë¡œ ì‹œì‘í•˜ëŠ” aíƒœê·¸

pprint(result)


for tag in result:
    print(tag.text, tag['href'], tag.name)



