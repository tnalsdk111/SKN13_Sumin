

















import os
print(os.getcwd())


## example.html 생성
# 문서 내용
html_content = """
<html>
  <head>
    <title>Example Page</title>
  </head>
  <body>
    <h1>Hello, BeautifulSoup!</h1>
    <p>This is a paragraph.</p>
    <a href="https://example.com">Visit Example.com</a>
  </body>
</html>
"""

# 파일 생성
with open("example.html", "w", encoding="utf-8") as f:
    f.write(html_content)

print("example.html 생성 완료!")



## 파일 불러오기

# HTML을 파싱하기 위한 BeautifulSoup 라이브러리를 불러온다.
from bs4 import BeautifulSoup

# example.html 파일을 읽기 모드로 연다.
# r: read, 읽기 전용 / t: text, 텍스트 모드 
with open("example.html", "rt", encoding='utf-8') as fr:
    html_doc = fr.read() # html의 전체 내용을 html_doc에 문자열로 저장한다.

# 잘 불러와졌는지 확인 - 처음 50글자만 출력해서 내용을 확인한다. 
print(html_doc[:50])
print(type(html_doc)) # str


## html_doc을 beautifulsoup으로 파싱해서 HTML 구조를 분석한다.
soup = BeautifulSoup(html_doc, "lxml")


## 구조화된 HTML이 예쁘게 출력된다. 
print(soup.prettify())

















## example_news.html 파일 생성

# 문서 내용
html_content = """
<html>
  <body>
    <div class="news">
      <h2>오늘의 뉴스</h2>
      <p class="content">날씨가 맑습니다.</p>
    </div>
    <div class="news">
      <h2>스포츠 뉴스</h2>
      <p class="content">축구에서 승리했습니다.</p>
    </div>
  </body>
</html>
"""

# 파일 생성
# open() 함수로 파일 열기
# 주요 파라미터: 파일명, 모드, 인코딩
with open("example_news.html", "w", encoding = "utf-8") as f: 
    f.write(html_content)


## 파일 불러오기

# HTML을 파싱하기 위한 BeautifulSoup 라이브러리를 불러온다. 
from bs4 import BeautifulSoup

# example_news.html을 읽기 모드로 연다.
with open("example_news.html", "rt", encoding = "utf-8") as fr:
    html_doc = fr.read()

# 잘 불러와졌는지 확인
print(html_doc[:100])








# html_doc을 BeautifulSoup으로 파싱
soup = BeautifulSoup(html_doc, "lxml")





print(html_doc)


print(soup) # 파싱 결과 출력


print(soup.prettify())





### 1단계: 태그명, 클래스로 요소 찾기


# 모든 뉴스 구역 가져오기
print("1. 모든 뉴스 구역 가져오기: class = \"news\"인 것\n")
print(soup.find_all("div", class_="news"), "\n\n") 

# 각 뉴스의 본문만
print("2. 각 뉴스의 본문만 가져오기 - div(구역 나누는 태그) 아래 p(본문 태그):\n")
print(soup.select("div.news > p.content"))  


### 2단계: 텍스트 꺼내기


print("뉴스 제목 가져오기:")
print(soup.find("h2").get_text())
# 1. find("h1"): "h2" 태그를 가진 요소를 찾아서
# 2. get_text(): 택스트를 가져온다. 





title_tag = soup.find("h1")

if title_tag:
    print("뉴스 제목:", title_tag.get_text())
else:
    print("h1 태그를 찾을 수 없습니다 😥")



### 잠시 수정!

# <p> 태그 선택
p_tag = soup.find("p", class_="content")
# (참고) 첫 번째 p 태그 뒤에 내용을 붙일 거라 find()를 씀. 
# 두 번째 것을 수정하고 싶다면 find_all() 써서 다 불러오자.

# <a> 태그 새로 만들기
new_a = soup.new_tag("a", href="https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=%EB%82%A0%EC%94%A8")
new_a.string = "오늘 날씨 보기"

# <p> 태그 바로 **뒤에** <a> 태그 추가
p_tag.insert_after(new_a)

# 결과 보기
print(soup.prettify())


# 실제 html 파일에 저장하려면 open() 함수를 통해 write() 해주어야 한다.
with open("example_news.html", "w", encoding="utf-8") as f:
    f.write(str(soup))  # 수정된 soup 객체를 문자열로 변환해서 저장


### 3단계: 속성 꺼내기


print(soup.find("h2"))





from bs4 import BeautifulSoup

with open("example.html", "rt", encoding="utf-8") as fr:
    html_doc = fr.read()

soup = BeautifulSoup(html_doc, "lxml")


result = soup.find_all("div")


print(len(result))
result


tag1 = result[0]
print("content:", tag1.text, tag1.get_text())
print("class속성값:", tag1.get("class"), tag1['class'])


result = soup.find("div")
print(type(result))
print("-"*50)
print(result)


# 태그의 content와 attribute 조회

print("content text:", result.text)
print("-"*50)
print("content text:", result.get_text())
print("-"*50)
print("attribue의 value:", result.get("class"))
print("-"*50)
print("attribue의 value:", result["class"])


# 태그의 모든 자식 요소들 조회
result.contents


from pprint import pprint

result = soup.find_all("a") 
print(result)
print("-"*50)
result = soup.find_all(["a", "span"])  #한번에 여러이름의 태그드을 조회.
print(result)
print("-"*50)
result = soup.find_all("div", attrs={"class":"name"}) # 태그이름 + 속성
print(result)
print("-"*50)
result = soup.find_all("div", attrs={"class":"animal_info", "id":"animal1"}) # 속성 조건이 여러개
print(result)
print("-"*50)
result = soup.find_all("a", attrs={"href":"https://www.coexaqua.com"})

# import re
# result = soup.find_all("a", attrs={"href":re.compile(r".com$")}) # 정규표현식-.com으로 끝나는.

pprint(result)


result_list = []
for tag in result:
    print(tag.text, tag['href'])
    result_list.append([tag.text, tag['href']]) # list[text, href]



result_list





from bs4 import BeautifulSoup

with open("example.html", "rt", encoding="utf-8") as fr:
    html_doc = fr.read()

soup = BeautifulSoup(html_doc, "lxml")



# css selector를 이용한 조회

result = soup.select("a")         #  태그이름(a)  
# result = soup.select("a, span") # 태그이름(여러개)
# result = soup.select("ul a")    # ul의 자손인 a태그 찾는다.

# result = soup.select_one("#animal1")             # 모든 태그중 id=animal1
# result = soup.select("ul + div")                 # ul의 다음 형제 태그중 div
# result = soup.select("body > div:nth-child(3)")  # body의 3번째 자식 div

# result = soup.select("a[href]")                        # href 속성이 있는 a 태그들
# result = soup.select("a[href='http://www.naver.com']") # href='http://www.naver.com' 속성을 가진 a 태그
# result = soup.select('a[href$=".do"]')                 # $=  href 속성값이 .do로 끝나는 a태그들
# result = soup.select('a[href^="https"]')               # =^  href 속성값이 https로 시작하는 a태그

pprint(result)


for tag in result:
    print(tag.text, tag['href'], tag.name)



