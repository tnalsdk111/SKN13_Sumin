
































# 웹 사이트에 요청을 보내기 위해 requests 라이브러리를 사용한다. 
import requests

# 네이버 검색 URL
# query={} 부분에 나중에 검색어가 들어갈 예정 (format으로 채워넣을 자)
url = "https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query={}"

# 사용자에게 검색어를 입력받는다. -> keyword
keyword = input("keyword:")
# 그리고 format(keyword)로 url의 {} 자리에 채워 넣는다.
url = url.format(keyword)

# 완성된 url에 get 요청을 보낸다.
# 서버가 응답을 주면 그 응답을 res에 저장한다. 
res = requests.get(url)

# 응답 코드 출력
# 200이면 성공!
print(res.status_code)
if res.status_code == 200:
    print(type(res)) # <class 'requests.models.Response'>
    print(type(res.text), len(res.text)) # 응답 본문 문자열, 그 길
    print(res.text[:200]) # 응답을 앞 200글자만 조회
else:
    print("응답을 받지 못함.", res.status_code)



## BeautifulSoup으로 꺼내는 것도 같이 해보자!

import requests
from bs4 import BeautifulSoup
from datetime import datetime # 날짜와 시간

keyword = input("검색어 입력 (예: 날씨): ")
url = f"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query={keyword}"

res = requests.get(url)

if res.status_code == 200:
    soup = BeautifulSoup(res.text, "lxml")

    weather_info = soup.find("div", class_="main_pack")



    if weather_info:
        # 날씨 요약과 상태 분리
        cast_txt = weather_info.find("p", class_="summary").get_text() # 전체 요약문
        weather_status = cast_txt.split()[-1] # 상태: 비
        weather_summary = cast_summary = " ".join(cast_txt.split()[:-1]) # 상태 제외한 요약만 따로 저장
        
        # 현재 온도 추출
        temp_box = weather_info.find("div", class_="temperature_text")
        if temp_box:
            current_temp = temp_box.get_text(strip=True).replace("현재 온도", "").replace("°", "")

        # 날짜 및 시간 출력
        now = datetime.now().strftime("%Y년 %m월 %d일 %H:%M:%S")

        # 날씨 상태에 따른 이모지 지정
        emoji_map = {
            "맑음": "☀️",
            "흐림": "☁️",
            "구름": "🌥️",
            "비": "🌧️",
            "눈": "❄️",
            "소나기": "🌦️",
            "안개": "🌫️"
        }
        emoji = emoji_map.get(weather_status, "🌈")  # 기본 이모지는 무지개

        # 날씨 상태에 따른 추가 문구 지정
        add_str_map = {
            "맑음": "햇빛이 쨍쨍! 기분이 좋아요",
            "흐림": "날이 흐려요. 혹시 모르니 작은 우산을 챙겨보아요!",
            "구름": "구름이 있어요. 강아지 모양 구름을 찾아보세요! 🐶",
            "비": "비도.. 오고.. 그래서.. 네 생각이 났어.. 🎵☂️",
            "눈": "눈이 와요! 오늘은 밖으로 뛰어나가 눈사람을 만들어요 ☃️",
            "소나기": "소나기가 와요. 우산 필수!!",
            "안개": "안개가 끼네요. 운전 조심하세요!"
        }

        add_str = add_str_map.get(weather_status, "오늘도 힘찬 하루! ⭐")

        # 출력
        print(f"\n📆현재 시각: {now}")
        print(f"☑️ 날씨 요약: {weather_summary}")
        print(f"{emoji} 현재 날씨: {weather_status} ({add_str})")
        print(f"🌡️ 현재 온도: {current_temp}°C")
    else:
        print("날씨 정보가 없어요 😥")
else:
    print("응답 실패", res.status_code)









import requests
from pprint import pprint

base_url = "https://httpbin.org/{}"
url = base_url.format("get")
print("요청 URL:", url)

params = {
    "query":"python",  # name : value
    "fbm": 0,
    "page": 3
}

req_headers = {
    "user-agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36"
    , "Referer":"https://www.google.com/"
}

res = requests.get(url,                  # url
                   params=params,        # 요청파라미터들
                   headers=req_headers)  # 요청 header 정보들 
print("응답상태코드:",  res.status_code)
if res.status_code == 200:
    print("============응답데이터(text)==========")
    print(res.text)
    d2 = res.json() # JSON 문자열을 dictionary로 변환해서 반환.
    print("==========응답헤더============")
    print(res.headers)





url = base_url.format("post")
print(url)
# post방식은 요청 파라미터를 url뒤에 붙이지 않고 dictionary로 정의해서 함수에 전달.
params = {
    "query":"python",
    "fbm": 0,
    "page": 3
}
req_headers = {
    "user-agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
    , "Referer":"https://www.google.com/"
}

res = requests.post(
    url,
    data=params,      
    headers=req_headers
)

if res.status_code == 200:
    print(res.text)     # -> str
    print(res.json())  # JSON -> dict
else:
    print("실패:", res.status_code)





from bs4 import BeautifulSoup
import requests

url = 'http://www.pythonscraping.com/pages/warandpeace.html'

user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"

res = requests.get(url, headers={"user-agent":user_agent})

if res.status_code == 200:

    soup = BeautifulSoup(res.text, "lxml")
    green_list = soup.select("span.green")
    search_names = []
    for tag in green_list:
        search_names.append(tag.text.replace("\n", ' '))
else:
    print("실패:", res.status_code)


result = [name.replace("\n", " ") for name in search_names]
result





url = "https://www.kia.com/content/dam/kwp/kr/ko/main-kv-contents/202311/kv_the_new_carnival_pc.jpg"

res = requests.get(url, headers={"user-agent":user_agent})

if res.status_code == 200:
    file = res.content  # binary 데이터 -> bytes으로 반환.
    print(type(file))
    with open("car.jpg", "wb") as fo:
        fo.write(file)








from datetime import datetime
datetime.now().strftime("%Y-%m-%d-%H-%M-%S")+".csv"


# 실습\daum_new_list.py
## https://news.daum.net

# pip install requests beautifulsoup4
import requests
from bs4 import BeautifulSoup

url = "https://news.daum.net"
# 뉴스제목: <a>의 content, 링크주소: <a>의 href 속성값
a_selector = "body > div.container-doc > main > section > div > div.content-article > div.box_g.box_news_issue > ul > li > div > div > strong > a"
# user-agent: 1.개발자도구>콘솔: navigator.userAgent, 2. google검색: my user agent 검색
user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'

def get_daum_news_list():
    """
    다음 뉴스 기사 목록을 크롤링하는 함수.
    news.daum.net의 기사 목록에서 "제목", "링크" 들을 수집.

    aguments
    return
        DataFrame: 조회결과들을 담은 DataFrame(표)
    raise
        Exception: 처리 실패시 발생
    """
    # 1. 요청
    res = requests.get(url, headers={"user-agent":user_agent})
    
    # 2. 응답 페이지에서 필요한 정보 추출
    if res.status_code == 200:
        soup = BeautifulSoup(res.text, "lxml")
        a_list = soup.select(a_selector)
        result_list = []
        for a_tag in a_list:
            title = a_tag.get_text()
            link = a_tag.get("href")
            result_list.append([title.strip(), link])
        
        return result_list
    else:
        raise Exception(f"요청 실패. 응답코드: {res.status_code}")      

if __name__ == "__main__":
    result = get_daum_news_list()
    
    # 저장할 디렉토리를 생성
    import os
    from datetime import datetime
    import pandas as pd
    save_dir = "daum_news_list"
    os.makedirs(save_dir, exist_ok=True)
    
    # 저장할 파일명 - 특정 기간마다 크롤링 수행할 경우 실행 날짜/시간을 이용해서 만들어 준다.
    d = datetime.now().strftime("%Y-%m-%d-%H-%M-%S")
    file_path = f"{save_dir}/{d}.csv"
    # DataFrame 생성
    result_df = pd.DataFrame(result, columns=['제목', "링크주소"])
    # csv 파일로 저장.
    result_df.to_csv(file_path, index=False)











import requests
import json


url = 'https://api.odcloud.kr/api/15127133/v1/uddi:ea3c3b5a-3bd8-4faf-b155-bb2af3cc3377'
with open('api_key.json', 'rt') as fr:
    key_dict = json.load(fr)

key = key_dict['apikey']
params ={'serviceKey' : key_dict["apikey"],
         'pageNo' : 1, 
         'perPage': 20,}

response = requests.get(url, params=params)
if response.status_code == 200:
    result = response.json()
    print(type(result))
    from pprint import pprint
    print(len(result['data']))
    pprint(result['data'])


result
