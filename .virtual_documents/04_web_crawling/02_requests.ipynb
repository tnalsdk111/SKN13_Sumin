
































# ì›¹ ì‚¬ì´íŠ¸ì— ìš”ì²­ì„ ë³´ë‚´ê¸° ìœ„í•´ requests ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œë‹¤. 
import requests

# ë„¤ì´ë²„ ê²€ìƒ‰ URL
# query={} ë¶€ë¶„ì— ë‚˜ì¤‘ì— ê²€ìƒ‰ì–´ê°€ ë“¤ì–´ê°ˆ ì˜ˆì • (formatìœ¼ë¡œ ì±„ì›Œë„£ì„ ì)
url = "https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query={}"

# ì‚¬ìš©ìì—ê²Œ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥ë°›ëŠ”ë‹¤. -> keyword
keyword = input("keyword:")
# ê·¸ë¦¬ê³  format(keyword)ë¡œ urlì˜ {} ìë¦¬ì— ì±„ì›Œ ë„£ëŠ”ë‹¤.
url = url.format(keyword)

# ì™„ì„±ëœ urlì— get ìš”ì²­ì„ ë³´ë‚¸ë‹¤.
# ì„œë²„ê°€ ì‘ë‹µì„ ì£¼ë©´ ê·¸ ì‘ë‹µì„ resì— ì €ì¥í•œë‹¤. 
res = requests.get(url)

# ì‘ë‹µ ì½”ë“œ ì¶œë ¥
# 200ì´ë©´ ì„±ê³µ!
print(res.status_code)
if res.status_code == 200:
    print(type(res)) # <class 'requests.models.Response'>
    print(type(res.text), len(res.text)) # ì‘ë‹µ ë³¸ë¬¸ ë¬¸ìì—´, ê·¸ ê¸¸
    print(res.text[:200]) # ì‘ë‹µì„ ì• 200ê¸€ìë§Œ ì¡°íšŒ
else:
    print("ì‘ë‹µì„ ë°›ì§€ ëª»í•¨.", res.status_code)



## BeautifulSoupìœ¼ë¡œ êº¼ë‚´ëŠ” ê²ƒë„ ê°™ì´ í•´ë³´ì!

import requests
from bs4 import BeautifulSoup
from datetime import datetime # ë‚ ì§œì™€ ì‹œê°„

keyword = input("ê²€ìƒ‰ì–´ ì…ë ¥ (ì˜ˆ: ë‚ ì”¨): ")
url = f"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query={keyword}"

res = requests.get(url)

if res.status_code == 200:
    soup = BeautifulSoup(res.text, "lxml")

    weather_info = soup.find("div", class_="main_pack")



    if weather_info:
        # ë‚ ì”¨ ìš”ì•½ê³¼ ìƒíƒœ ë¶„ë¦¬
        cast_txt = weather_info.find("p", class_="summary").get_text() # ì „ì²´ ìš”ì•½ë¬¸
        weather_status = cast_txt.split()[-1] # ìƒíƒœ: ë¹„
        weather_summary = cast_summary = " ".join(cast_txt.split()[:-1]) # ìƒíƒœ ì œì™¸í•œ ìš”ì•½ë§Œ ë”°ë¡œ ì €ì¥
        
        # í˜„ì¬ ì˜¨ë„ ì¶”ì¶œ
        temp_box = weather_info.find("div", class_="temperature_text")
        if temp_box:
            current_temp = temp_box.get_text(strip=True).replace("í˜„ì¬ ì˜¨ë„", "").replace("Â°", "")

        # ë‚ ì§œ ë° ì‹œê°„ ì¶œë ¥
        now = datetime.now().strftime("%Yë…„ %mì›” %dì¼ %H:%M:%S")

        # ë‚ ì”¨ ìƒíƒœì— ë”°ë¥¸ ì´ëª¨ì§€ ì§€ì •
        emoji_map = {
            "ë§‘ìŒ": "â˜€ï¸",
            "íë¦¼": "â˜ï¸",
            "êµ¬ë¦„": "ğŸŒ¥ï¸",
            "ë¹„": "ğŸŒ§ï¸",
            "ëˆˆ": "â„ï¸",
            "ì†Œë‚˜ê¸°": "ğŸŒ¦ï¸",
            "ì•ˆê°œ": "ğŸŒ«ï¸"
        }
        emoji = emoji_map.get(weather_status, "ğŸŒˆ")  # ê¸°ë³¸ ì´ëª¨ì§€ëŠ” ë¬´ì§€ê°œ

        # ë‚ ì”¨ ìƒíƒœì— ë”°ë¥¸ ì¶”ê°€ ë¬¸êµ¬ ì§€ì •
        add_str_map = {
            "ë§‘ìŒ": "í–‡ë¹›ì´ ì¨ì¨! ê¸°ë¶„ì´ ì¢‹ì•„ìš”",
            "íë¦¼": "ë‚ ì´ íë ¤ìš”. í˜¹ì‹œ ëª¨ë¥´ë‹ˆ ì‘ì€ ìš°ì‚°ì„ ì±™ê²¨ë³´ì•„ìš”!",
            "êµ¬ë¦„": "êµ¬ë¦„ì´ ìˆì–´ìš”. ê°•ì•„ì§€ ëª¨ì–‘ êµ¬ë¦„ì„ ì°¾ì•„ë³´ì„¸ìš”! ğŸ¶",
            "ë¹„": "ë¹„ë„.. ì˜¤ê³ .. ê·¸ë˜ì„œ.. ë„¤ ìƒê°ì´ ë‚¬ì–´.. ğŸµâ˜‚ï¸",
            "ëˆˆ": "ëˆˆì´ ì™€ìš”! ì˜¤ëŠ˜ì€ ë°–ìœ¼ë¡œ ë›°ì–´ë‚˜ê°€ ëˆˆì‚¬ëŒì„ ë§Œë“¤ì–´ìš” â˜ƒï¸",
            "ì†Œë‚˜ê¸°": "ì†Œë‚˜ê¸°ê°€ ì™€ìš”. ìš°ì‚° í•„ìˆ˜!!",
            "ì•ˆê°œ": "ì•ˆê°œê°€ ë¼ë„¤ìš”. ìš´ì „ ì¡°ì‹¬í•˜ì„¸ìš”!"
        }

        add_str = add_str_map.get(weather_status, "ì˜¤ëŠ˜ë„ í˜ì°¬ í•˜ë£¨! â­")

        # ì¶œë ¥
        print(f"\nğŸ“†í˜„ì¬ ì‹œê°: {now}")
        print(f"â˜‘ï¸ ë‚ ì”¨ ìš”ì•½: {weather_summary}")
        print(f"{emoji} í˜„ì¬ ë‚ ì”¨: {weather_status} ({add_str})")
        print(f"ğŸŒ¡ï¸ í˜„ì¬ ì˜¨ë„: {current_temp}Â°C")
    else:
        print("ë‚ ì”¨ ì •ë³´ê°€ ì—†ì–´ìš” ğŸ˜¥")
else:
    print("ì‘ë‹µ ì‹¤íŒ¨", res.status_code)









import requests
from pprint import pprint

base_url = "https://httpbin.org/{}"
url = base_url.format("get")
print("ìš”ì²­ URL:", url)

params = {
    "query":"python",  # name : value
    "fbm": 0,
    "page": 3
}

req_headers = {
    "user-agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36"
    , "Referer":"https://www.google.com/"
}

res = requests.get(url,                  # url
                   params=params,        # ìš”ì²­íŒŒë¼ë¯¸í„°ë“¤
                   headers=req_headers)  # ìš”ì²­ header ì •ë³´ë“¤ 
print("ì‘ë‹µìƒíƒœì½”ë“œ:",  res.status_code)
if res.status_code == 200:
    print("============ì‘ë‹µë°ì´í„°(text)==========")
    print(res.text)
    d2 = res.json() # JSON ë¬¸ìì—´ì„ dictionaryë¡œ ë³€í™˜í•´ì„œ ë°˜í™˜.
    print("==========ì‘ë‹µí—¤ë”============")
    print(res.headers)





url = base_url.format("post")
print(url)
# postë°©ì‹ì€ ìš”ì²­ íŒŒë¼ë¯¸í„°ë¥¼ urlë’¤ì— ë¶™ì´ì§€ ì•Šê³  dictionaryë¡œ ì •ì˜í•´ì„œ í•¨ìˆ˜ì— ì „ë‹¬.
params = {
    "query":"python",
    "fbm": 0,
    "page": 3
}
req_headers = {
    "user-agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
    , "Referer":"https://www.google.com/"
}

res = requests.post(
    url,
    data=params,      
    headers=req_headers
)

if res.status_code == 200:
    print(res.text)     # -> str
    print(res.json())  # JSON -> dict
else:
    print("ì‹¤íŒ¨:", res.status_code)





from bs4 import BeautifulSoup
import requests

url = 'http://www.pythonscraping.com/pages/warandpeace.html'

user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"

res = requests.get(url, headers={"user-agent":user_agent})

if res.status_code == 200:

    soup = BeautifulSoup(res.text, "lxml")
    green_list = soup.select("span.green")
    search_names = []
    for tag in green_list:
        search_names.append(tag.text.replace("\n", ' '))
else:
    print("ì‹¤íŒ¨:", res.status_code)


result = [name.replace("\n", " ") for name in search_names]
result





url = "https://www.kia.com/content/dam/kwp/kr/ko/main-kv-contents/202311/kv_the_new_carnival_pc.jpg"

res = requests.get(url, headers={"user-agent":user_agent})

if res.status_code == 200:
    file = res.content  # binary ë°ì´í„° -> bytesìœ¼ë¡œ ë°˜í™˜.
    print(type(file))
    with open("car.jpg", "wb") as fo:
        fo.write(file)








from datetime import datetime
datetime.now().strftime("%Y-%m-%d-%H-%M-%S")+".csv"


# ì‹¤ìŠµ\daum_new_list.py
## https://news.daum.net

# pip install requests beautifulsoup4
import requests
from bs4 import BeautifulSoup

url = "https://news.daum.net"
# ë‰´ìŠ¤ì œëª©: <a>ì˜ content, ë§í¬ì£¼ì†Œ: <a>ì˜ href ì†ì„±ê°’
a_selector = "body > div.container-doc > main > section > div > div.content-article > div.box_g.box_news_issue > ul > li > div > div > strong > a"
# user-agent: 1.ê°œë°œìë„êµ¬>ì½˜ì†”: navigator.userAgent, 2. googleê²€ìƒ‰: my user agent ê²€ìƒ‰
user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'

def get_daum_news_list():
    """
    ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ì„ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜.
    news.daum.netì˜ ê¸°ì‚¬ ëª©ë¡ì—ì„œ "ì œëª©", "ë§í¬" ë“¤ì„ ìˆ˜ì§‘.

    aguments
    return
        DataFrame: ì¡°íšŒê²°ê³¼ë“¤ì„ ë‹´ì€ DataFrame(í‘œ)
    raise
        Exception: ì²˜ë¦¬ ì‹¤íŒ¨ì‹œ ë°œìƒ
    """
    # 1. ìš”ì²­
    res = requests.get(url, headers={"user-agent":user_agent})
    
    # 2. ì‘ë‹µ í˜ì´ì§€ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ
    if res.status_code == 200:
        soup = BeautifulSoup(res.text, "lxml")
        a_list = soup.select(a_selector)
        result_list = []
        for a_tag in a_list:
            title = a_tag.get_text()
            link = a_tag.get("href")
            result_list.append([title.strip(), link])
        
        return result_list
    else:
        raise Exception(f"ìš”ì²­ ì‹¤íŒ¨. ì‘ë‹µì½”ë“œ: {res.status_code}")      

if __name__ == "__main__":
    result = get_daum_news_list()
    
    # ì €ì¥í•  ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±
    import os
    from datetime import datetime
    import pandas as pd
    save_dir = "daum_news_list"
    os.makedirs(save_dir, exist_ok=True)
    
    # ì €ì¥í•  íŒŒì¼ëª… - íŠ¹ì • ê¸°ê°„ë§ˆë‹¤ í¬ë¡¤ë§ ìˆ˜í–‰í•  ê²½ìš° ì‹¤í–‰ ë‚ ì§œ/ì‹œê°„ì„ ì´ìš©í•´ì„œ ë§Œë“¤ì–´ ì¤€ë‹¤.
    d = datetime.now().strftime("%Y-%m-%d-%H-%M-%S")
    file_path = f"{save_dir}/{d}.csv"
    # DataFrame ìƒì„±
    result_df = pd.DataFrame(result, columns=['ì œëª©', "ë§í¬ì£¼ì†Œ"])
    # csv íŒŒì¼ë¡œ ì €ì¥.
    result_df.to_csv(file_path, index=False)











import requests
import json


url = 'https://api.odcloud.kr/api/15127133/v1/uddi:ea3c3b5a-3bd8-4faf-b155-bb2af3cc3377'
with open('api_key.json', 'rt') as fr:
    key_dict = json.load(fr)

key = key_dict['apikey']
params ={'serviceKey' : key_dict["apikey"],
         'pageNo' : 1, 
         'perPage': 20,}

response = requests.get(url, params=params)
if response.status_code == 200:
    result = response.json()
    print(type(result))
    from pprint import pprint
    print(len(result['data']))
    pprint(result['data'])


result
