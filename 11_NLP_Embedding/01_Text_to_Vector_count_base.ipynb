{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a241325d-ff0d-4a85-a151-fa3eca3953e9",
   "metadata": {},
   "source": [
    "# NLP(Natural Language Processing)\n",
    "\n",
    "## 자연어\n",
    "- 사람이 일상적으로 사용하는 언어를 말한다.\n",
    "    - 어떤 목적을 가지고 사람이 만든 것이 아니라 **자연적으로 만들어진 언어**를 말한다.\n",
    "- **인공언어**\n",
    "    - 특정 목적을 위해 사람이 인위적으로 만든 언어로 자연어의 대척점에 있는 언어 개념.\n",
    "    - 예: 프로그래밍 언어\n",
    "\n",
    "## 자연어 처리 (NLP)\n",
    "- 자연어 처리(Natural Language Process)는 컴퓨터가 인간의 언어를 이해하고 분석하는 분야를 말한다.\n",
    "- 자연어 처리도 오래된 분야인데 딥러닝이 적용되면서 획기적인 발전을 이루었다.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f9958",
   "metadata": {},
   "source": [
    "# Text to Vector(Text Vectorization)\n",
    "\n",
    "1. **Text Vectorization의 정의**\n",
    "   - 자연어(텍스트) 데이터를 컴퓨터가 이해할 수 있는 숫자 벡터로 변환하는 과정.\n",
    "   - 비정형 텍스트를 머신러닝 모델이 처리할 수 있는 수치 형태로 표현하며, 이 과정을 통해 컴퓨터는 단어와 문장, 문단을 수학적 표현으로 다룰 수 있다.\n",
    "   ![nlp_text_to_vector_outline](figures/nlp_text_to_vector_outline.png)\n",
    "2. **Feature 추출**\n",
    "   - 자연어 텍스트에서 output을 계산하는데 필요한 **중요한 특징(특성-feature)들**을 뽑아내어 활용할 수 있게 한다.\n",
    "   - 텍스트의 의미적, 문법적 특성을 수치화한다.\n",
    "   - 단어나 문장 간의 관계를 벡터 공간에서 표현 할 수 있다.\n",
    "   - 텍스트 데이터의 패턴을 발견하고 분석하는데 필수적인 전처리 과정이 feature 추출이다.\n",
    "\n",
    "3. **활용 분야**\n",
    "   - 벡터화된 텍스트는 머신러닝 알고리즘이나 딥러닝 모델에 입력되어 분류, 군집화, 감성 분석 등 다양한 자연어 처리 작업에 활용된다.\n",
    "   - 벡터화된 데이터는 두 텍스트 간의 유사도를 비교하는 데에도 사용될 수 있다. 이를 통해 비슷한 문장, 단어, 문서 등을 효과적으로 찾을 수 있다.\n",
    "## 주요 방법론\n",
    "   1. **Count-Based Word Representation**\n",
    "       - 단어가 나오는 횟수(출연 빈도수) 기반 표현방식\n",
    "       - Bag of word, TF-IDF\n",
    "   2. **Word Embedding**\n",
    "       - 단어를 **고정된 Vector로 표현**한다. 고정됐다는 것은 문맥에 상관없이 같은 단어는 같은 값으로 표현한다.\n",
    "       - Word2Vec, Fastext, Glove\n",
    "   3. **Contextualized Word Embedding**\n",
    "       - 문맥을 고려한 Word Embedding 방식. 단어가 문맥에 따라 의미가 바뀌는 **동적 Vector 표현방식**이다.\n",
    "       - ELMo, BERT, GPT2, RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e66796e-dc71-403b-a119-d7024720e2f0",
   "metadata": {},
   "source": [
    "# Count-Based Word Representation\n",
    "\n",
    "1. One-Hot Encoding\n",
    "2. Bag Of Words(BoW)\n",
    "    - DTM(Document Term Matrix)/TDM(Term Document Matrix)\n",
    "    - TF-IDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209171a4-6362-4db1-817e-e9316f7889d7",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "-   One-Hot Encoding을 이용해 단어를 표현하는 방법.\n",
    "    -   전체 vocabulary size 크기에 vector로 구성하며 각 index는 vocabulary를 구성하는 단어들을 의미한다. Value는 표현하는 단어는 1 나머지는 0으로 구성한다.\n",
    "    -   각 단어(토큰)들을 서로 다른 숫자 vector로 만드는 방식이다. 그렇다 보니 각 단어의 차이점 이외의 정보는 전혀 포함되지 않는다. \n",
    "    -   보통 Word Embedding 모델의 입력으로 사용된다.\n",
    "\n",
    "### 구현방법\n",
    "\n",
    "1. 토큰화(Tokenization)\n",
    "    - 문장을 최소단위(token)로 나눈다.\n",
    "    - 토큰화는 모든 Embedding의 시작이다.\n",
    "    - ex) `\"나는 축구를 좋아합니다. 친구는 야구를 좋아합니다.\"` -> `[\"나는\", \"축구를\", \"좋아합니다\", \".\", \"친구는\", \"야구를\", \"좋아합니다\", \".\"]`\n",
    "2. Vocabulary(vocab - 어휘/단어 사전) 구성\n",
    "    - Token - index 형식으로 구성한 어휘 사전을 만든다.\n",
    "    - `['.', '나는', '야구를', '좋아합니다', '축구를', '친구는']`\n",
    "3. Vocab의 모든 token들을 one-hot encoding 처리한다.  \n",
    "   ![ohe](figures/onehotvector.png)\n",
    "4. 문장을 one-hot vector를 이용해 행렬로 구성한다.\n",
    "\n",
    "-   ex) 나는 야구를 좋아합니다.\n",
    "\n",
    "```\n",
    "[\n",
    "  [0, 1, 0, 0, 0, 0],   -- 나는\n",
    "  [0, 0, 1, 0, 0, 0],   -- 야구를\n",
    "  [0, 0, 0, 1, 0, 0]    -- 좋아합니다\n",
    "  [1, 0, 0, 0, 0, 0]    -- .\n",
    "]\n",
    "```\n",
    "\n",
    "> ### 어휘 사전(단어 사전, vocaburay, vocab)\n",
    ">\n",
    "> -   사용할 모든 단어(토큰)들을 모아놓은 집합.\n",
    "> -   보통 단어-정수index 쌍의 구조로 저장한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e33f9c-0b0d-4f79-945a-304a136bf158",
   "metadata": {},
   "source": [
    "### One-Hot Encoding의 문제\n",
    "\n",
    "1. 단어 수가 늘어날 수록 vector의 차원도 늘어난다.\n",
    "2. 단어의 의미를 표현할 수없다. 그래서 단어 또는 문장의 유사도와 같이 의미를 알아야 하는 분야에서는 사용할 수 없다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ef9d87-75a8-4c18-a013-76acf2b5d134",
   "metadata": {},
   "source": [
    "## Bag Of Words(BoW)\n",
    "\n",
    "-   **단어의 출현 빈도에만 집중해 단어를 표현**(word representation) 하는 방법으로 단어의 순서는 전혀 고려하지 않는다.\n",
    "    - Concept: **많이 나온 단어가 중요한 단어이다.**\n",
    "\n",
    "### DTM/TDM\n",
    "\n",
    "-   문서안에서 문서를 구성하는 단어들이 몇 번 나왔는지를 표현하는 행렬로 Vector화 한다.\n",
    "-   DTM\n",
    "    -   행: 문서, 열: 단어\n",
    "-   TDM\n",
    "    -   행: 단어, 열: 문서\n",
    "-   Value: 출연 횟수\n",
    "\n",
    "#### 구현방법\n",
    "\n",
    "1. 어휘 사전(vocabulary) 생성\n",
    "    - 모든 단어에 고유 정수 index를 부여\n",
    "2. 문장을 embeddig 할 때 각 단어 index에 등장 횟수를 기록한다.\n",
    "\n",
    "#### scikit-learn CountVectorizer 이용\n",
    "\n",
    "-   빈도수 기반 Vector화 지원 Text 전처리 클래스\n",
    "-   **주요 생성자 매개변수**\n",
    "    -   stop_word :stopword 지정\n",
    "        -   str: \"english\" - 자체 제공 불용어는 제공됨\n",
    "        -   list: stopword 리스트\n",
    "    -   max_df: 특정 횟수 이상나오는 것은 무시하도록 설정(무시할 횟수/비율 지정)\n",
    "        -   int(횟수), float(비율)\n",
    "    -   min_df: 특정 횟수 이하로 나오는 것은 무시하도록 설정(무시할 횟수/비율 지정)\n",
    "    -   max_features: 최대 token 수\n",
    "        -   빈도수가 높은 순서대로 정렬 후 지정한 max_features 개수만큼만 사용한다.\n",
    "    -   ngram_range: n_gram 범위 지정\n",
    "        -   n_gram:\n",
    "        -   튜플 (범위 최소값, 범위 최대값)\n",
    "        -   (1, 2) : 토큰화된 단어를 1개씩 그리고 순서대로 2개씩 묶어서 Feature를 추출\n",
    "    -   tokenizer: 토큰화 처리 함수\n",
    "-   **메소드/ 속성**\n",
    "    -   fit(X)\n",
    "        -   학습\n",
    "        -   매개변수: raw document - 문장을 원소로 가지는 1차원 배열형태(list, ndarray)\n",
    "        -   **Train(훈련) 데이터셋 으로 학습한다. Test 데이터셋은 Train 셋으로 학습한 CountVectorizer를 이용해 변환만 한다.**\n",
    "    -   transform(X)\n",
    "        -   DTM 변환\n",
    "    -   fit_transform(X)\n",
    "        -   학습/변환 한번에 처리\n",
    "    -   vocabulary\\_: 어휘 사전\n",
    "\n",
    "> #### n-gram\n",
    ">\n",
    "> -   N 개의 단어(token)을 묶어서 하나의 토큰으로 처리하는 방식을 n-gram이라고 한다. (n은 몇개 토큰을 하나의 단위로 묶을지 개수)\n",
    ">     -   uni-gram (n=1), bi-gram (n=2), tri-gram (n=3), 4개부터는 n-gram으로 표기(4-gram, 5-gram, ..)\n",
    ">     -   Embedding이나 언어모델을 만들때 적용하는 기법이다.\n",
    "> -   BoW에 n-gram을 적용하면 n개의 단어를 묶어서 하나의 tokne으로 처리한다.\n",
    "> -   언어모델에 n-gram을 적용하면 이전/이후 n 개의 단어를 이용해 다음 단어를 유추한다.\n",
    "> -   n-gram의 문제\n",
    ">     -   n이 너무 크면 희소성(출현 빈도가 낮아진다)의 문제가 발생한다.\n",
    ">         -   `나는 어제 밥을 먹으러 식당에 가려다가 마음을 바꾸었다.` 이런 token이 전체 corpus에 얼마나 있을까?\n",
    ">     -   n이 너무 작으면 단어간의 관계성이 표현이 안되는 문제가 발생한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e90e2-607e-4840-a188-60608b4717c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"나는 야구를 좋아합니다. 나는 축구를 좋아하지 않습니다.\", \n",
    "        \"친구는 야구와 축구를 좋아합니다. 야구보다 축구를 더 좋아합니다.\", \n",
    "        \"나는 축구를 더 좋아합니다. 영국 축구 리그보다 이탈리아 축구 리그를 더 좋아합니다.\", \n",
    "        \"나는 농구를 좋아하지 않습니다. 배구도 좋아하지 않습니다.\", \n",
    "        \"나는 액션영화를 좋아합니다.\", \n",
    "        \"어제 비빕밥을 먹었습니다.\"]\n",
    "\n",
    "stop_words = list('은는이가을를도.,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b224d-59e3-4931-a06e-b5d8a4f9a302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30772ba-8455-4b2f-a0da-249f1cf173c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef93f4-de57-41b8-b364-6cfc2ecf21e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893ea5d8-0b33-4b10-9f1a-fad3b2e11678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1550e02-8b1d-4b4f-9826-c1e61d8cf087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f3c27c1-4f94-483a-a002-c447e494ab88",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "-   개별 문서에 많이 나오는 단어가 높은 값을 가지도록 하되 동시에 여러 문서에 자주 나오는 단어에는 페널티를 주는 방식\n",
    "-   어떤 문서에 특정 단어가 많이 나오면 그 단어는 해당 문서를 설명하는 중요한 단어일 수 있다. 그러나 그 단어가 다른 문서에도 많이 나온다면 언어 특성이나 주제상 많이 사용되는 단어 일 수 있다.\n",
    "-   각 문서의 길이가 길고 문서개수가 많은 경우 Count 방식 보다 TF-IDF 방식이 더 좋은 예측 성능을 내는 경우가 많다.\n",
    "\n",
    "#### 구현방법\n",
    "\n",
    "-   TF (Term Frequency) 정의: 해당 단어가 **해당 문서에** 몇번 나오는지를 나타내는 지표\n",
    "-   DF (Document Frequency) 정의: 해당 단어가 **몇개의 문서에** 나오는지를 나타내는 지표\n",
    "-   IDF (Inverse Document Frequency) 정의: DF에 역수로 $\\cfrac{\\text{전체 문서수}}{\\text{해당 단어가 나오는 문서수}}$\n",
    "-   TF-IDF 정의: $TF * \\left(\\log \\cfrac{\\text{전체 문서수}}{\\text{해당 단어가 나오는 문서수}} \\right)$\n",
    "    -   log는 전체 문서의 수가 많으면 값의 단위가 너무 커지므로 log를 취한다.\n",
    "    -   scikit-learn의 경우 분모가 0이 되는 것을 방지하기 위해 **분모에 1을 더하고** $\\log(0)$도 계산이 안되므로 **분자에도 1을 더했으며** 그 계산 결과에 **1을 더하여 계산**함.\n",
    "        -   $TF * \\left(\\log \\cfrac{\\text{전체 문서수 + 1}}{\\text{해당 단어가 나오는 문서수 + 1}} + 1\\right)$\n",
    "\n",
    "#### scikit-learn TfidfVectorizer 이용\n",
    "\n",
    "-   TF-IDF 기반 텍스트 벡터화 전처리 클래스\n",
    "\n",
    "#### 주요 생성자 매개변수\n",
    "\n",
    "-   stop_word :stopword 지정\n",
    "    -   str: \"english\" - 영문 불용어는 제공됨\n",
    "    -   list: stopword 리스트\n",
    "-   max_df: 특정 횟수 이상나오는 것은 무시하도록 설정(무시할 횟수/비율 지정)\n",
    "    -   int(횟수), float(비율)\n",
    "-   min_df: 특정 횟수 이하로 나오는 것은 무시하도록 설정(무시할 횟수/비율 지정)\n",
    "-   max_features: 최대 token 수\n",
    "    -   빈도수가 높은 순서대로 정렬 후 지정한 max_features 개수만큼만 사용한다.\n",
    "-   ngram_range: n_gram 범위 지정\n",
    "    -   n_gram:\n",
    "    -   튜플 (범위 최소값, 범위 최대값)\n",
    "    -   (1, 2) : 토큰화된 단어를 1개씩 그리고 순서대로 2개씩 묶어서 Feature를 추출\n",
    "-   tokenizer: 토큰화 처리 함수\n",
    "\n",
    "#### 메소드/Attribute\n",
    "\n",
    "-   fit(X)\n",
    "    -   학습\n",
    "    -   매개변수: 문장을 가진 1차원 배열형태(list, ndarray)\n",
    "    -   **Train(훈련)+Test(테스트) 데이터셋 으로 학습한다.**\n",
    "-   transform(X)\n",
    "    -   DTM 변환\n",
    "-   fit_transform(X)\n",
    "    -   학습/변환 한번에 처리\n",
    "-   vocabulary\\_: 어휘 사전\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd911ddf-7d9a-49bf-801a-0c0c604553b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb07335-9a21-4072-a99a-aa278c1a0c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bfbcd-091b-468c-9e01-32908c8fb9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f23f17-d109-4aa0-9975-9067e828224a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a655da-c86c-4c7e-94e0-1e53f3374138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
