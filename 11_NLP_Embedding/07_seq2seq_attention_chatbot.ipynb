{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eebda4f9-a2f4-4af2-b1fb-36c01c48d8f0",
   "metadata": {},
   "source": [
    "# Attention mechanism \n",
    "\n",
    "- Seq2Seq 모델의 문제점\n",
    "    - Seq2Seq 모델은 Encoder에서 입력 시퀀스에 대한 특성을 **하나의 고정된 context vector**에 압축하여 Decoder로 전달 한다. Decoder는 이 context vector를 이용해서 출력 시퀀스를 만든다.\n",
    "    - 하나의 고정된 크기의 vector에 모든 입력 시퀀스의 정보를 넣다보니 정보 손실이 발생한다.\n",
    "    - Decoder에서 출력 시퀀스를 생성할 때 동일한 context vector를 기반으로 한다. 그러나 각 생성 토큰마다 입력 시퀀스에서 참조해야 할 중요도가 다를 수 있다. seq2seq는 encoder의 마지막 hidden state를 context로 받은 뒤 그것을 이용해 모든 출력 단어들을 생성하므로 그 중요도에 대한 반영이 안된다.\n",
    "\n",
    "## Attention Mechanism 아이디어\n",
    "-  Decoder에서 출력 단어를 예측하는 매 시점(time step)마다, Encoder의 입력 문장(context vector)을 다시 참고 하자는 것. 이때 전체 입력 문장의 단어들을 동일한 비율로 참고하는 것이 아니라, Decoder가 해당 시점(time step)에서 예측해야할 단어와 연관이 있는 입력 부분을 좀 더 집중(attention)해서 참고 할 수 있도록 하자는 것이 기본 아이디어이다.\n",
    "- 다양한 Attention 종류들이 있다.\n",
    "    -  Decoder에서 출력 단어를 예측하는 매 시점(time step)마다 Encoder의 입력 문장의 어느 부분에 더 집중(attention) 할지를 계산하는 방식에 따라 다양한 attention 기법이 있다.\n",
    "    -  `dot attention - Luong`, `scaled dot attention - Vaswani`, `general  attention - Luong`, `concat  attention - Bahdanau` 등이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ca7ae-e57f-4d14-a3de-19c26436f371",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d5b90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "url = \"https://raw.githubusercontent.com/songys/Chatbot_data/refs/heads/master/ChatbotData.csv\"\n",
    "res = requests.get(url)\n",
    "if res.status_code == 200:\n",
    "    with open(\"data/chatbot_data.csv\", \"wt\", encoding=\"utf-8\") as fw:\n",
    "        fw.write(res.text)\n",
    "else:\n",
    "    print(f\"불러오지 못함: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db6fe689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A\n",
       "0           12시 땡!   하루가 또 가네요.\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.\n",
       "4          PPL 심하네   눈살이 찌푸려지죠."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/chatbot_data.csv')\n",
    "df.drop(columns='label', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaa1a1c-b942-4cad-948e-80c1bd768690",
   "metadata": {},
   "source": [
    "# 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41d23c37-f609-411b-aba2-17b081259cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11823, 11823, 11823)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_texts = df['Q']\n",
    "answer_texts = df['A']\n",
    "all_texts = list(question_texts + \" \"+answer_texts)  # Q + A: vocab 생성.\n",
    "len(question_texts), len(answer_texts), len(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65fd1cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12시 땡! 하루가 또 가네요.',\n",
       " '1지망 학교 떨어졌어 위로해 드립니다.',\n",
       " '3박4일 놀러가고 싶다 여행은 언제나 좋죠.',\n",
       " '3박4일 정도 놀러가고 싶다 여행은 언제나 좋죠.',\n",
       " 'PPL 심하네 눈살이 찌푸려지죠.',\n",
       " 'SD카드 망가졌어 다시 새로 사는 게 마음 편해요.',\n",
       " 'SD카드 안돼 다시 새로 사는 게 마음 편해요.',\n",
       " 'SNS 맞팔 왜 안하지ㅠㅠ 잘 모르고 있을 수도 있어요.',\n",
       " 'SNS 시간낭비인 거 아는데 매일 하는 중 시간을 정하고 해보세요.',\n",
       " 'SNS 시간낭비인데 자꾸 보게됨 시간을 정하고 해보세요.',\n",
       " 'SNS보면 나만 빼고 다 행복해보여 자랑하는 자리니까요.',\n",
       " '가끔 궁금해 그 사람도 그럴 거예요.',\n",
       " '가끔 뭐하는지 궁금해 그 사람도 그럴 거예요.',\n",
       " '가끔은 혼자인게 좋다 혼자를 즐기세요.',\n",
       " '가난한 자의 설움 돈은 다시 들어올 거예요.',\n",
       " '가만 있어도 땀난다 땀을 식혀주세요.',\n",
       " '가상화폐 쫄딱 망함 어서 잊고 새출발 하세요.',\n",
       " '가스불 켜고 나갔어 빨리 집에 돌아가서 끄고 나오세요.',\n",
       " '가스불 켜놓고 나온거 같아 빨리 집에 돌아가서 끄고 나오세요.',\n",
       " '가스비 너무 많이 나왔다. 다음 달에는 더 절약해봐요.',\n",
       " '가스비 비싼데 감기 걸리겠어 따뜻하게 사세요!',\n",
       " '가스비 장난 아님 다음 달에는 더 절약해봐요.',\n",
       " '가장 확실한 건 뭘까? 가장 확실한 시간은 오늘이에요. 어제와 내일을 놓고 고민하느라 시간을 낭비하지 마세요.',\n",
       " '가족 여행 가기로 했어 온 가족이 모두 마음에 드는 곳으로 가보세요.',\n",
       " '가족 여행 고고 온 가족이 모두 마음에 드는 곳으로 가보세요.',\n",
       " '가족 여행 어디로 가지? 온 가족이 모두 마음에 드는 곳으로 가보세요.',\n",
       " '가족 있어? 저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요',\n",
       " '가족관계 알려 줘 저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요',\n",
       " '가족끼리 여행간다. 더 가까워질 기회가 되겠네요.',\n",
       " '가족들 보고 싶어 저도요.',\n",
       " '가족들이랑 서먹해 다들 바빠서 이야기할 시간이 부족했나봐요.',\n",
       " '가족들이랑 서먹해졌어 다들 바빠서 이야기할 시간이 부족했나봐요.',\n",
       " '가족들이랑 어디 가지? 온 가족이 모두 마음에 드는 곳으로 가보세요.',\n",
       " '가족들이랑 여행 갈거야 좋은 생각이에요.',\n",
       " '가족여행 가야지 더 가까워질 기회가 되겠네요.',\n",
       " '가족이 누구야? 저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요',\n",
       " '가족이랑 여행 가려고 좋은 생각이에요.',\n",
       " '가족한테 스트레스 풀었어 정말 후회할 습관이에요.',\n",
       " '가출할까? 무모한 결정을 내리지 마세요.',\n",
       " '가출해도 갈 데가 없어 선생님이나 기관에 연락해보세요.',\n",
       " '간만에 떨리니까 좋더라 떨리는 감정은 그 자체로 소중해요.',\n",
       " '간만에 쇼핑 중 득템했길 바라요.',\n",
       " '간만에 휴식 중 휴식도 필요하죠.',\n",
       " '간식 뭐 먹을까 단짠으로 두 개 사는게 진리죠.',\n",
       " '간식 추천 단짠으로 두 개 사는게 진리죠.',\n",
       " '간장치킨 시켜야지 맛있게 드세요.',\n",
       " '간접흡연 싫어 저도 싫어요.',\n",
       " '갈까 말까 고민 돼 가세요.',\n",
       " '갈까 말까? 가세요.',\n",
       " '감 말랭이 먹고 싶다. 맛있게 드세요.',\n",
       " '감 말랭이 먹어야지 맛있게 드세요.',\n",
       " '감기 같애 병원가세요.',\n",
       " '감기 걸린 것 같아 이럴 때 잘 쉬는 게 중요해요.',\n",
       " '감기 기운이 있어 이럴 때 잘 쉬는 게 중요해요.',\n",
       " '감기 들 거 같애 이럴 때 잘 쉬는 게 중요해요.',\n",
       " '감기가 오려나 따뜻하게 관리하세요.',\n",
       " '감기약이 없어 병원가세요.',\n",
       " '감기인거 같애 병원가세요.',\n",
       " '감미로운 목소리 좋아 저도 듣고 싶네요.',\n",
       " '감정이 쓰레기통처럼 엉망진창이야 자신을 더 사랑해주세요.',\n",
       " '감정컨트롤을 못하겠어 그건 습관이에요.',\n",
       " '감정컨트롤이 안돼 그건 습관이에요.',\n",
       " '감히 나를 무시하는 애가 있어 콕 집어서 물어보세요.',\n",
       " '갑자기 나쁜 생각이 막 들더라 좋은 생각만 하세요.',\n",
       " '갑자기 눈물 나 마음이 아픈가요.',\n",
       " '갑자기 물어봐서 당황했어 갑작스러웠나봐요.',\n",
       " '갑자기 불편한 사이가 된 거 같아 관계의 변화가 왔나봅니다.',\n",
       " '강렬한 첫인상 남겨야 하는데 처음 3초가 중요해요. 당신의 매력을 어필해보세요.',\n",
       " '강아지 키우고 싶어 책임질 수 있을 때 키워 보세요.',\n",
       " '강아지 키우고 싶은데 역시 안돼겠지 먼저 생활패턴을 살펴 보세요.',\n",
       " '강아지 키울 수 있을까 먼저 생활패턴을 살펴 보세요.',\n",
       " '강아지 키울까 책임질 수 있을 때 키워 보세요.',\n",
       " '강원도 가서 살까? 아름다운 곳이죠.',\n",
       " '같이 게임하자고 해도 되나? 안 될 것도 없죠.',\n",
       " '같이 놀러갈 친구가 없어 혼자도 좋아요.',\n",
       " '같이 먹었는데 나만 살찐 거 같아 연인은 살쪄도 잘 알아차리지 못하고 알아차려도 싫어하지 않을 거예요.',\n",
       " '같이 수영장 가기로 했어 즐거운 시간 보내고 오세요!',\n",
       " '같이 있으면 힘든데 붙잡고 싶어 질질 끌지 마세요.',\n",
       " '같이 피씨방 가자고 해볼까? 말해보세요.',\n",
       " '같이 할 수 있는 취미 생활 뭐 있을까 함께하면 서로를 더 많이 알게 될 거예요.',\n",
       " '개강룩 입어볼까 개시해보세요.',\n",
       " '개강옷 예쁘게 입어 볼까 개시해보세요.',\n",
       " '개강이다 곧 방학이예요.',\n",
       " '개강이라니 방학이 참 짧죠.',\n",
       " '개같은 상황 벗어나는 게 좋겠네요.',\n",
       " '개같이 되버렸어. 벗어나는 게 좋겠네요.',\n",
       " '개기름 꼈어 세수하고 오세요.',\n",
       " '개념도 놓고 옴 그게 제일 중요한 건데요.',\n",
       " '개념이 없어 그게 제일 중요한 건데요.',\n",
       " '개당황 다음부터는 더 많이 아세요.',\n",
       " '개당황했잖아 갑자기 물어 봐서 갑작스러웠나봐요.',\n",
       " '개인적인 업무까지 다 시켜 공적인 일부터 하세요.',\n",
       " '개인적인 일도 다 시켜 공적인 일부터 하세요.',\n",
       " '개졸려 낮잠을 잠깐 자도 괜찮아요.',\n",
       " '개좋아 저도 좋아해주세요.',\n",
       " '개학하니까 좋다 친구들이 보고싶었나봐요.',\n",
       " '걔 너무 싫다 되도록 만나지 마세요.',\n",
       " '걔는 누굴 닮아서 그런거니? 당신이요.',\n",
       " '걔랑 같은 반 됐으면 좋겠다 당신의 운을 믿어보세요.',\n",
       " '거지 같이 일해 놓고 갔어 일 못하는 사람이 있으면 옆에 있는 사람이 더 힘들죠.',\n",
       " '거지됐어 밥 사줄 친구를 찾아 보세요~',\n",
       " '거짓말 했어 선의의 거짓말이길 바라요.',\n",
       " '거짓말을 나도 모르게 자꾸 해 거짓말은 할수록 늘어요.',\n",
       " '거짓말을 하게 돼 거짓말은 할수록 늘어요.',\n",
       " '거짓말이 거짓말을 낳아 진실된 말을 하려고 노력해보세요.',\n",
       " '걱정 없이 살고파 누구나 걱정은 있어요.',\n",
       " '걱정 좀 없이 살고 싶다. 누구나 걱정은 있어요.',\n",
       " '건강 관리 운동을 해보세요.',\n",
       " '건강 빨리 회복해야지 세상의 무엇보다 건강이 제일 중요해요.',\n",
       " '건강검진 왔어 주기적으로 해주는 게 좋죠.',\n",
       " '건강검진하러 옴 주기적으로 해주는 게 좋죠.',\n",
       " '건강이 최고 가장 중요한 목표네요.',\n",
       " '건강이 최고인 것 같아 가장 중요한 목표네요.',\n",
       " '건강하게 다이어트 하는 방법 적게 먹고 많이 움직이세요.',\n",
       " '건강한 다이어트법 적게 먹고 많이 움직이세요.',\n",
       " '건너건너 아는 사람인데 연락해도 될까? 모르는 사이라 당황할 수도 있어요.',\n",
       " '건물주 되고싶어 이룰 수 있을 거예요.',\n",
       " '건물주가 짱인데 이룰 수 있을 거예요.',\n",
       " '건방져 기분이 나쁘셨나봐요.',\n",
       " '건조기 살까봐 있으면 편하대요.',\n",
       " '건조하네 눈을 깜빡거려 보세요.',\n",
       " '걸레질도 해야 돼 청소를 좋아하시나봐요.',\n",
       " '걸어 가고 있는데 깜깜해서 무서워 안전 귀가 하세요.',\n",
       " '겁난다 용기 내보세요.',\n",
       " '게으른 동료가 있어 피해를 안 준다면 무시하세요.',\n",
       " '게임 같이 하자고 할까? 안 될 것도 없죠.',\n",
       " '게임 때문에 시간 다갔어 게임할때는 시간이 더 빨리 가요.',\n",
       " '게임 때문에 폰이 점점 느려지는듯 정리해보세요.',\n",
       " '게임 재미있어. 게임하세요!',\n",
       " '게임 지겨워 다른 게임해보세요.',\n",
       " '게임도 이제 재미없어 다른 게임해보세요.',\n",
       " '게임하고 싶어 게임하세요!',\n",
       " '게임하다 시간 다갔어 게임할때는 시간이 더 빨리 가요.',\n",
       " '겨울 지나 봄이야 마음에도 봄이 오길 바라요.',\n",
       " '겨울에는 온천이지! 몸은 뜨겁고 머리는 차갑게!',\n",
       " '겨울이 가고 봄이 올거야 마음에도 봄이 오길 바라요.',\n",
       " '격려 좀 해줘 잘하실 거예요!',\n",
       " '격려가 필요해. 잘하실 거예요!',\n",
       " '견과류 챙겨 먹어야지. 건강 생각해서 챙겨드세요.',\n",
       " '결국 이런 운명이라니 슬프다 좋은 운명도 있을거예요.',\n",
       " '결정 못하겠어. 결정하기 힘드시겠네요.',\n",
       " '결정은 빠르면 빠를 수록 좋겠지? 자신을 위한 결정을 내리길 바라요.',\n",
       " '결정은 빠를수록 좋겠지? 자신을 위한 결정을 내리길 바라요.',\n",
       " '결정을 못 내리겠어. 어떻해 결정은 빠르면 빠를수록 좋을 거예요.',\n",
       " '결정적인 물증이 없어 안타깝네요. 증거를 지금이라도 모아봐요.',\n",
       " '결혼 했는데. 좋겠어요.',\n",
       " '결혼 했어 좋겠어요.',\n",
       " '결혼도 다 돈이다. 많이 들지만 줄일 수 있을 거예요.',\n",
       " '결혼식 가기 귀찮아 경조사는 참석하는게 좋아요.',\n",
       " '결혼식 또 가야돼 경조사는 참석하는게 좋아요.',\n",
       " '결혼식때 하객이 없을 까봐 걱정돼 생각보다 신경 안 씁니다.',\n",
       " '결혼식이 너무 많아 인맥이 넓으신가봐요.',\n",
       " '결혼이나 하지 왜 자꾸 나한테 화 내냐구! 힘들겠네요.',\n",
       " '결혼준비 돈 많이 들겠지 많이 들지만 줄일 수 있을 거예요.',\n",
       " '결혼준비하는데 돈 얼마나 드나 욕심에 따라 천지 차이일 거예요.',\n",
       " '결혼하는데 돈 많이 드네 허례허식이에요.',\n",
       " '결혼하는데 돈 얼마나 들까 욕심에 따라 천지 차이일 거예요.',\n",
       " '결혼하면 좋아? 해봐요.',\n",
       " '결혼하면 좋을까 서로 노력하면 행복할 거예요.',\n",
       " '결혼하면 행복할까? 서로 노력하면 행복할 거예요.',\n",
       " '결혼하면 행복해? 사람마다 행복의 크기가 다르겠지만 행복할 거예요.',\n",
       " '결혼하면 행복해질까? 사람마다 행복의 크기가 다르겠지만 행복할 거예요.',\n",
       " '결혼할까 능력이 있으면 하면 되죠.',\n",
       " '결혼해도 되나 능력이 있으면 하면 되죠.',\n",
       " '결혼해도 될까 이사람이다 싶은 사람이랑 하세요.',\n",
       " '결혼해야 하나 해봐요.',\n",
       " '경쟁이 너무 치열해 점점 치열해지는 것 같아요.',\n",
       " '계속 공부해도 될까 확신이 없나봐요.',\n",
       " '계속 도전하는 거 귀찮아 안정적인 걸 좋아하나봐요.',\n",
       " '계속 방학이면 좋을텐데 방학은 참 짧아요.',\n",
       " '계속 보고 싶어 보러 가세요.',\n",
       " '계속 보고 싶으면 어떡해? 보러 가세요.',\n",
       " '계속 속이 진짜 안 좋아 계속 좋지 않으면 병원에 가보세요.',\n",
       " '계속 엇갈리는 느낌 타이밍이 안 맞았나봐요.',\n",
       " '계속 학생하고 싶어 이제 취업 하셔야죠.',\n",
       " '계속 한숨만 나와 뇌세포에 에너지를 공급하려는 자연스러운 현상이에요. 에너지가 부족한가봐요.',\n",
       " '고3은 공부만 해야겠지. 공부가 최우선이죠.',\n",
       " '고3이니까 공부해야겠지 공부가 최우선이죠.',\n",
       " '고구마 다이어트 해야지 너무 무리하지는 마세요.',\n",
       " '고구마만 먹고 다이어트 해야지 너무 무리하지는 마세요.',\n",
       " '고기 구워 먹고 싶다. 저기압에는 고기앞이죠.',\n",
       " '고기 먹고 싶어 저기압에는 고기앞이죠.',\n",
       " '고데기 망했어 연습이 필요해요.',\n",
       " '고데기 했는데 망했어 연습이 필요해요.',\n",
       " '고독한 밤 혼자가 아니에요.',\n",
       " '고마운 사람들이 많아 인복이 많나봐요.',\n",
       " '고무신 거꾸로 신으면 어쩌지 너무 걱정하지 마세요.',\n",
       " '고민 있어 네 말씀하세요.',\n",
       " '고민 좀 들어줄래 네 말씀하세요.',\n",
       " '고백하고 후회하면 어떡하지 후회는 후회를 낳을뿐이에요. 용기 내세요.',\n",
       " '고시원 너무 답답해 돈을 모아서 다른 곳으로 이사갈 수 있을 거예요.',\n",
       " '고시원 답답해 돈을 모아서 다른 곳으로 이사갈 수 있을 거예요.',\n",
       " '고시원에서 나가고 싶어 더 좋은 곳에서 살 수 있을 거예요.',\n",
       " '고시원에서 탈출하고 싶어 더 좋은 곳에서 살 수 있을 거예요.',\n",
       " '고양이 동영상 보는 중 완전 귀엽죠?',\n",
       " '고양이 키우고 싶어 자신을 먼저 키우세요.',\n",
       " '고양이 키우고 싶어 가족들과 상의해보세요.',\n",
       " '고의는 아닌데 실수를 한 거 같아 용서를 구하세요.',\n",
       " '고집 센 사람 피할 수 있으면 피하세요.',\n",
       " '고집하고는 피할 수 있으면 피하세요.',\n",
       " '골프 못 치는데 처음부터 잘하는 사람은 없어요.',\n",
       " '골프 배워야 돼 시간내서 가보세요.',\n",
       " '골프 어려워 처음부터 잘하는 사람은 없어요.',\n",
       " '골프치러 가야돼 시간내서 가보세요.',\n",
       " '곱창 먹고 싶어. 미리 미리 충전해주세요.',\n",
       " '곱창 생각나 미리 미리 충전해주세요.',\n",
       " '공무원 괜찮겠지 안정적이고 좋죠.',\n",
       " '공무원 되고 싶다 준비해보세요.',\n",
       " '공무원 되면 좋겠다 준비해보세요.',\n",
       " '공무원 시험 공부 힘들다 합격 기원해요!',\n",
       " '공무원 시험 죽을 거 같아 철밥통 되기가 어디 쉽겠어요.',\n",
       " '공무원 시험 힘들어ㅠㅠ 철밥통 되기가 어디 쉽겠어요.',\n",
       " '공무원 준비할까 시작이 반이에요. 어서 준비하세요.',\n",
       " '공무원이 좋지? 안정적이고 좋죠.',\n",
       " '공복이라 신경이 예민해져 자연스러운 현상이에요.',\n",
       " '공복이라 예민해 자연스러운 현상이에요.',\n",
       " '공복이면 예민함? 보이는 게 없죠.',\n",
       " '공부 계속해도 될까 지금처럼 잘될 거예요.',\n",
       " '공부 꼭 해야 할까 미래의 배우자가 달라져요.',\n",
       " '공부 때려치워야 하나 확신이 없나봐요.',\n",
       " '공부 시작해도 될까 공부는 언제나 좋죠.',\n",
       " '공부 왜 해야 돼? 공부하면 더 많은 선택을 할 수 있죠.',\n",
       " '공부 잘 안돼 같이 수다 떨면서 놀까요?',\n",
       " '공부 잘하고 싶어 나만의 공부방법을 찾아보세요.',\n",
       " '공부 좀 더 할 걸 지금도 늦지 않았어요.',\n",
       " '공부 하기 싫다 같이 수다 떨면서 놀까요?',\n",
       " '공부는 내 체질이 아닌 것 같아 확신이 없나봐요.',\n",
       " '공부로 먹고 살 수 있을까 지금처럼 잘될 거예요.',\n",
       " '공부방법이 잘못된걸까? 나한테 맞는 공부 방법 찾는 게 시급하네요.',\n",
       " '공부하기 싫어 잠시 쉬어도 돼요.',\n",
       " '공부하기 싫은 날 잠시 쉬어도 돼요.',\n",
       " '공부하는 낙이 없어 공부하면 더 많은 선택을 할 수 있죠.',\n",
       " '공부하는 이유? 공부하면 더 많은 선택을 할 수 있죠.',\n",
       " '공부하는 이유가 없어 공부하면 더 많은 선택을 할 수 있죠.',\n",
       " '공시 준비 힘들어 합격 기원해요!',\n",
       " '공시 준비 힘들어 잘 될 거예요.',\n",
       " '공시 준비중 좋은 결과 있을 거예요!',\n",
       " '공시 준비하는데 힘들다 잘 될 거예요.',\n",
       " '공시생이야 좋은 결과 있을 거예요!',\n",
       " '공연 보고 싶어 친구와 같이 가보세요.',\n",
       " '공연 보러 가고 싶어 친구와 같이 가보세요.',\n",
       " '공책 필기 나만 힘들어? 성향 차이가 좀 있기는 하죠.',\n",
       " '공황장애 생겼어. 꾸준히 약 먹고 치료해보세요.',\n",
       " '공황장애 있어 꾸준히 약 먹고 치료해보세요.',\n",
       " '공휴일에는 집이 최고 피로 풀고 좋죠.',\n",
       " '공휴일에는 집콕 피로 풀고 좋죠.',\n",
       " '과거는 잊고 앞으로 나아 가야지 오늘이 중요하죠.',\n",
       " '과거는 중요하지 않아 오늘이 중요하죠.',\n",
       " '과식해서 소화가 안돼 소화제 챙겨드세요.',\n",
       " '과식했나 봐 과식은 금물이에요.',\n",
       " '과식했다 소화제 드세요.',\n",
       " '과외비 부담되겠지? 안된다고 하면 거짓말이겠지요.',\n",
       " '과외비 비싸? 안된다고 하면 거짓말이겠지요.',\n",
       " '과일 먹고 자야지 제철과일이 정말 좋아요.',\n",
       " '과일 먹어야지. 건강 생각해서 챙겨드세요.',\n",
       " '과일 안 먹게 돼 그래도 먹으려고 노력해보세요.',\n",
       " '과일 잘 안 먹게 돼 그래도 먹으려고 노력해보세요.',\n",
       " '과일 챙겨 먹어야지 제철과일이 정말 좋아요.',\n",
       " '관계가 계속 애매하다. 인간 관계도 정리가 필요해요.',\n",
       " '관심 끄라고 하고 싶다. 무관심이 필요할 때가 있죠.',\n",
       " '관심 좀 안 가졌으면 무관심이 필요할 때가 있죠.',\n",
       " '관절염 같애 계단 조심하세요.',\n",
       " '관절염인가 계단 조심하세요.',\n",
       " '광고가 안 끝나 채널을 돌려보세요.',\n",
       " '괜찮아지고 있어 괜찮아지고 있어 다행이에요.',\n",
       " '괜찮은 사람인데 사귀긴 싫어 남자사람친구, 여자사람친구 하세요.',\n",
       " '괜히 건들지 말라고 많이 지쳤나봐요.',\n",
       " '괜히 기다렸어 누군가를 기다린다는게 쉬운게 아니죠.',\n",
       " '괜히 농담해서 망했다 늦지 않았어요.',\n",
       " '괜히 아까운 시간 버렸다 그 것도 다 경험이라고 생각하세요.',\n",
       " '괜히 창피해 그럴 필요 없어요.',\n",
       " '괴물이 되어 가는 느낌이 들어 그렇지 않아요.',\n",
       " '교보문고 왔어 마음에 드는 책을 잘 찾아보세요.',\n",
       " '교양 수업 재밌어 저도 듣고 싶어요.',\n",
       " '교양수업 시간에 마음에 드는 애 있어 같은 조가 되길 바랄게요.',\n",
       " '교양수업 은근 재미져 지식 쌓는 재미가 있죠.',\n",
       " '교양수업에서 마음에 드는 애 있어 같은 조가 되길 바랄게요.',\n",
       " '교양수업이 재미있어 지식 쌓는 재미가 있죠.',\n",
       " '교양이 전공보다 재미있어 저도 듣고 싶어요.',\n",
       " '교직이수 가능할까 학점 관리하세요.',\n",
       " '교통사고 났었어. 보험 처리하세요.',\n",
       " '교통사고 당했어 보험 처리하세요.',\n",
       " '교회 가기 싫어 왜 그럴까요?',\n",
       " '교회 갔다 만났어 좋은 만남이었길 바라요.',\n",
       " '교회에서 만났어 좋은 만남이었길 바라요.',\n",
       " '구박하면서 엄청 일 시켜 일을 몰라서 그런가봐요.',\n",
       " '군대 갔다 올 때까지 기다릴 수 있을까 자신의 삶을 살다보면 기다릴 수 있을 거예요.',\n",
       " '군대 기다려 주려고 부담스러워하지 않는다면 기다려도 좋을 것 같아요.',\n",
       " '군대 기다려도 될까 부담스러워하지 않는다면 기다려도 좋을 것 같아요.',\n",
       " '군대 기다리면 부담스러워할까 너무 걱정하지 마세요.',\n",
       " '군대 기다릴 수 있을까 자신의 삶을 살다보면 기다릴 수 있을 거예요.',\n",
       " '군대 언제 끝나나 군대 시계는 멈추지 않아요.',\n",
       " '군대 전역 기다려 군대 시계는 멈추지 않아요.',\n",
       " '굿모닝 좋은 아침이에요.',\n",
       " '궁금하면 오백원 안 궁금해요.',\n",
       " '궁금하지? 안 궁금해요.',\n",
       " '궁금해 자세히 말씀해주세요.',\n",
       " '궁금해 알려줘 자세히 말씀해주세요.',\n",
       " '귀 아파 병원에 가세요.',\n",
       " '귀가 가려워 누가 욕하고 있나봐요.',\n",
       " '귀가 간지러 누가 욕하고 있나봐요.',\n",
       " '귀가 윙윙거려 병원에 가세요.',\n",
       " '귀농 어때? 생각하기는 쉬운데 실천하기는 어려운 것 같아요.',\n",
       " '그 사람이 나 안 좋아하는 거 같아 슬픈 이야기네요.',\n",
       " '그 사람이 나 좋아해줬으면 좋겠다 저도 간절히 기도 할게요.',\n",
       " '그 사람이 행복했으면 좋겠다 그렇게 될 수 있을 거예요.',\n",
       " '그 시절엔 다 그랬지 추억에 잠길 때도 필요해요.',\n",
       " '그냥 고백할걸 후회는 후회를 낳을뿐이에요. 용기 내세요.',\n",
       " '그냥 공무원이 좋을 듯 시작이 반이에요. 어서 준비하세요.',\n",
       " '그냥 내버려 둬 주었으면 아무도 없는 곳으로 여행을 떠나보세요.',\n",
       " '그냥 선 볼까? 많이 만나보세요.',\n",
       " '그냥 쉬고 싶다 잠깐 바람 쐬고 오세요.',\n",
       " '그냥 씹어야겠다. 다른 사람이 답답할 거예요.',\n",
       " '그냥 이렇게 살고 싶어 살고 싶은대로 사세요.',\n",
       " '그냥 자는 거 아니지? 많이 피곤한가봐요.',\n",
       " '그냥 잘못했다고 하면 될거 같은데 자꾸 변명해 피할 수 있으면 피하세요.',\n",
       " '그냥 택시 타야지. 조심히 오세요.',\n",
       " '그냥 할까? 실천이 말보다 낫죠.',\n",
       " '그냥 혼자 밥이나 먹어야지 밥심으로 사는 거죠.',\n",
       " '그냥 혼자 있는게 좋아 혼자만 있지 마세요.',\n",
       " '그동안 잘 지냈나요? 안부를 물어주시다니 감사합니다.',\n",
       " '그땐 그랬지 추억에 잠길 때도 필요해요.',\n",
       " '그래 그러자 괜찮은 선택이길 바라요.',\n",
       " '그래 이제 결정했어 좋은 결과 있을 거예요.',\n",
       " '그래도 좀 기대했는데 기쁜 마음으로 베풀고 보답을 바라지 마세요.',\n",
       " '그런 말을 왜 하지 다른 사람 말은 신경쓰지 마세요.',\n",
       " '그런 사람인가보다 해야하나봐 대인배시군요.',\n",
       " '그런 사람인갑다 해야지 대인배시군요.',\n",
       " '그런 친구 아니었는데 너무 귀찮게 하네 친구가 좋아하나봐요.',\n",
       " '그렇게 갈 거면서 이야기를 해보세요.',\n",
       " '그렇게 오래 살았는데도 이해를 못하겠어 온전한 이해는 없어요.',\n",
       " '그렇게 할래 괜찮은 선택이길 바라요.',\n",
       " '그림 잘 그리고 싶다 학원을 다니거나 연습하면 잘할 수 있을 거예요.',\n",
       " '그림 좀 잘 그렸으면 좋겠다 학원을 다니거나 연습하면 잘할 수 있을 거예요.',\n",
       " '그만 두고 나오고 싶어 뒷감당 자신 있으면 하세요.',\n",
       " '그만 먹어야 하는데 조금만 드세요',\n",
       " '그만 살고싶어 당신을 소중하게 생각하세요.',\n",
       " '그저 그런 하루 그런 하루도 감사한 마음을 가져보세요.',\n",
       " '근사한 곳 알아 냈어 좋은 사람과 함께 가세요.',\n",
       " '근육 있으면 멋있을텐데 저 말씀이신가요?',\n",
       " '금값 알아? 비싸요.',\n",
       " '금값 어때 비싸요.',\n",
       " '금사빠인가 호의인지 호감인지 헷갈리나요?',\n",
       " '금수저 물고 태어나면 좋겠지? 뭔가 안풀리는 일이 있나봐요.',\n",
       " '금수저로 태어났으면 아이를 금수저로 만들어주세요.',\n",
       " '금수저로 태어났으면 좋았을텐데 아이를 금수저로 만들어주세요.',\n",
       " '금연이 쉽지 않아 자신을 이겨야해요.',\n",
       " '기 빨렸어 너무 긴장했나봐요.',\n",
       " '기념일 다 챙기는거 귀찮아 기념일 챙겨주면 좋아할거예요.',\n",
       " '기념일 또 까먹었어 달력에 적어보세요.',\n",
       " '기념일 못챙겼어 달력에 적어보세요.',\n",
       " '기념일 챙기기 귀찮아 기념일 챙겨주면 좋아할거예요.',\n",
       " '기능 좀 알려줘봐봐 당신의 삶을 응원해 드릴 수 있어요라고 감히 말해 봅니다.',\n",
       " '기다리는 것도 지쳐 기다리지 마세요.',\n",
       " '기다리라고 말 못하겠어 상대방의 선택에 맡겨보세요.',\n",
       " '기다림이 습관이 됐나봐 좋은 분이시군요',\n",
       " '기대가 무너졌어 베풀되 보답을 바라지 마세요.',\n",
       " '기대가 부담스러운데 떨쳐낼 수 있는 방법 있을까? 자신을 사랑할수록 외부의 인정은 필요 없어요.',\n",
       " '기대하고 있었는데 상대에게 바라는 기대는 자신을 슬프게 해요.',\n",
       " '기대하지 말걸 베풀되 보답을 바라지 마세요.',\n",
       " '기대했는데 기쁜 마음으로 베풀고 보답을 바라지 마세요.',\n",
       " '기댈 수 있는 사람 의지할 수 있는 사람이 곁에 있다는 건 큰 행운일 거예요.',\n",
       " '기력이 없어 자신의 감정을 주변 사람들에게 터놓고 이야기해보세요.',\n",
       " '기름값 올랐어. 대중교통을 이용해주세요.',\n",
       " '기본이 뭔지도 모르는 것 같아. 각자가 생각하는 기본이 다를 수도 있어요.',\n",
       " '기본이 안 되어 있어 각자가 생각하는 기본이 다를 수도 있어요.',\n",
       " '기부 좀 했어요 좋은 일 하셨네요.',\n",
       " '기부했어 좋은 일 하셨네요.',\n",
       " '기분 꿀꿀해 내일은 오늘보다 나을 거예요.',\n",
       " '기분 나쁜 농담을 계속하고 있어 정색 한번 해주세요.',\n",
       " '기분 울적해서 좀 걷고 있어 걷다보면 조금 정리가 될 거예요.',\n",
       " '기분 전환 하고 싶어 저랑 함께 해요.',\n",
       " '기분 전환이 필요해 저랑 함께 해요.',\n",
       " '기분이 그지 같아 신나는 음악 들어보세요.',\n",
       " '기분이 더러워 경쾌한 음악 들어보세요.',\n",
       " '기분이 묘해 왜일까요?',\n",
       " '기분이 이상해 무슨 이유인지 생각해보세요.',\n",
       " '기숙사 괜찮을까 혼자 사는 것보다 불편하겠죠.',\n",
       " '기숙사 떨어졌어 다음 학기에는 학점 관리를 더 열심히 해봐요.',\n",
       " '기숙사 사는거 어떨까? 혼자 사는 것보다 불편하겠죠.',\n",
       " '기숙사 살면 불편해? 혼자 사는 것보다 불편하겠죠.',\n",
       " '기숙사 안됐어 다음 학기에는 학점 관리를 더 열심히 해봐요.',\n",
       " '기술 배울까 기술을 많이 알면 도움이 되겠죠.',\n",
       " '기차 타고 여행 가고 싶어 꿈꾸던 여행이네요.',\n",
       " '기차여행 가고 싶어 꿈꾸던 여행이네요.',\n",
       " '기침도 못하겠어 답답한 상황이네요.',\n",
       " '기침도 편하게 못해 답답한 상황이네요.',\n",
       " '기프트콘 받았어! 좋겠어요!',\n",
       " '기프트콘 선물 괜찮을까? 직접 주는 게 더 좋을 것 같아요.',\n",
       " '기프트콘 선물해볼까? 직접 주는 게 더 좋을 것 같아요.',\n",
       " '기프트콘 주면 좋아할까? 직접 주는 게 더 좋을 것 같아요.',\n",
       " '기프트콘으로 선물 받았어 좋겠네요.',\n",
       " '기프트콘으로 선물 해야겠다 직접 주는 게 더 좋을 것 같아요.',\n",
       " '기회를 놓쳤어 더 좋은 기회가 올 거예요.',\n",
       " '기회를 못 잡았어 더 좋은 기회가 올 거예요.',\n",
       " '기획사니까 당연히 예쁜 애들 많겠지 연예인을 준비하니 일반인보다 다 예쁘겠죠.',\n",
       " '기획사에 예쁜 애들 많겠지 연예인을 준비하니 일반인보다 다 예쁘겠죠.',\n",
       " '긴 머리 관리 어렵다. 그래서 저는 못 기르고 잘라요.',\n",
       " '긴 머리 관리하는 거 힘들다 그래서 저는 못 기르고 잘라요.',\n",
       " '긴 시간이 걸렸지만 괜찮아. 괜찮아지고 있어 다행이에요.',\n",
       " '긴장 푸는 법 알려줘 크게 숨한 번 쉬어 보세요',\n",
       " '긴장돼 크게 숨한 번 쉬어 보세요',\n",
       " '긴장돼서 땀나네 미리 긴장하지 마세요.',\n",
       " '길거리에서 연락처 물어보면 줘도 되나 마음에 들면 줘보세요.',\n",
       " '길에서 담배 피우는 사람 싫어 저도 싫어요.',\n",
       " '길에서 번호 따였어 잘 해보세요.',\n",
       " '길에서 전번 물어보면 줘도 되나 마음에 들면 줘보세요.',\n",
       " '길에서 헌팅 당했어 잘 해보세요.',\n",
       " '길은 멀고 해는 진다 그래도 넘을 수 있을 거예요.',\n",
       " '길이 미끄러워서 미끄러질뻔했어 조심하세요.',\n",
       " '길이 안보여 너무 낙담하지 마세요.',\n",
       " '길이 얼어서 미끄러질뻔했어 조심하세요.',\n",
       " '길이 얼었어 미끄러우니 조심하세요.',\n",
       " '김떡순 먹고 싶어. 건강을 위해 조금씩 드세요.',\n",
       " '김치도 없네 마트 갑시다.',\n",
       " '김치볶음밥 먹어야지 맛있는 식사시간 되시길 바랄게요.',\n",
       " '김치볶음밥이나 만들어 먹어야지 맛있는 식사시간 되시길 바랄게요.',\n",
       " '김치찌개 먹고 싶어 맛있죠!',\n",
       " '까아 오빠들 컴백한다 기다렸나봐요.',\n",
       " '깜깜한데 전기 안들어오네 조금만 기다리면 다시 전기가 들어올거예요.',\n",
       " '깡 마른 거 같아 적당해요.',\n",
       " '꼴 사나워질 것 같은데 스스로 단단해지세요.',\n",
       " '꽃 받고 싶다 제가 드리고 싶네요.',\n",
       " '꽃 사고 싶어 집안 분위기가 바뀔 거예요.',\n",
       " '꽃 살까? 집안 분위기가 바뀔 거예요.',\n",
       " '꽃 선물 좋아할까 꽃 선물은 언제나 좋죠.',\n",
       " '꽃 선물해 볼까 꽃 선물은 언제나 좋죠.',\n",
       " '꽃 예쁘게 말렸어 솜씨가 좋으시네요.',\n",
       " '꽃게탕 맛있다. 기분 좋아 보이세요.',\n",
       " '꽃게탕 진짜 밥도둑 기분 좋아 보이세요.',\n",
       " '꽃꽂이 배우는 중 마음의 안정을 취하기 좋은 취미네요.',\n",
       " '꽃꽂이 배우니까 좋다 마음의 안정을 취하기 좋은 취미네요.',\n",
       " '꽃놀이 가고 싶어 벚꽃 계절이 다가왔네요.',\n",
       " '꽃다발 말려봐야지 거꾸로 해서 드라이플라워 만들어보세요.',\n",
       " '꽃다발 말리면 에쁘겠지. 거꾸로 해서 드라이플라워 만들어보세요.',\n",
       " '꽃다발 받았어 부러워요!',\n",
       " '꽃다발 샀어 멋진 선물이네요.',\n",
       " '꽃다발 선물 괜찮지? 센스있는 선물이에요.',\n",
       " '꽃다발 선물 받았어 부러워요!',\n",
       " '꽃다발 선물 어때? 센스있는 선물이에요.',\n",
       " '꽃다발 준비했어 멋진 선물이네요.',\n",
       " '꽃바구니 선물이랑 과일 바구니 선물 뭐가 좋아? 받는 사람이 부럽네요.',\n",
       " '꽃바구니가 좋을까 과일바구니까 좋을까 받는 사람이 부럽네요.',\n",
       " '꽃선물 받고 어 제가 드리고 싶네요.',\n",
       " '꿀잼 저도 즐거워요',\n",
       " '꿈은 많은데 차근차근 이뤄보아요.',\n",
       " '꿈이 너무 많아 차근차근 이뤄보아요.',\n",
       " '꿈이 너무 무서웠어 요즘 예민한가봐요.',\n",
       " '꿈이 다양해 많으면 많을 수록 좋죠.',\n",
       " '꿈이 두 개야 더 많아도 괜찮아요.',\n",
       " '꿈이 없어 거창하지 않아도 돼요.',\n",
       " '꿈이 이루어질까? 현실을 꿈처럼 만들어봐요.',\n",
       " '꿈이 자꾸 바뀌어 많으면 많을 수록 좋죠.',\n",
       " '꿈이 현실이었으면 현실을 꿈처럼 만들어봐요.',\n",
       " '끝나니까 허무하다 뜻대로 되는게 많지 않죠.',\n",
       " '끝나면 좋을 줄 알았는데. 마음이 허전하신가봐요.',\n",
       " '낌새가 이상하더니 딱 걸렸어 잘 해결되길 바라요.',\n",
       " '낌새가 있더니 딱 걸렸어 잘 해결되길 바라요.',\n",
       " '나 감정쓰레기통이었나봐 자신을 더 사랑해주세요.',\n",
       " '나 갖고 장난친건가 아니길 바라요.',\n",
       " '나 같은 사람은 동물 키우면 안되겠지 잘 아시네요.',\n",
       " '나 같이 예쁜 애를 왜 갈구지 애정표현일 지도 몰라요.',\n",
       " '나 거짓말 못하겠어 얼굴에 다 티가 나네요.',\n",
       " '나 결정 잘 한거지? 네, 이제 잘 해낼 차례예요.',\n",
       " '나 결정했어 좋은 결과 있을 거예요.',\n",
       " '나 괜찮지 않니 괜찮은 사람이에요.',\n",
       " '나 교직이수할 수 있을까? 학점 관리하세요.',\n",
       " '나 그동안 뭐한거니 바람 좀 쐬고 오시면 좋은텐데.',\n",
       " '나 그지임 밥 사줄 친구를 찾아 보세요~',\n",
       " '나 내일 기숙사 가야돼 짐 빼놓지 말고 싸세요.',\n",
       " '나 내장비만이래 식단조절도 하고 꾸준히 운동하세요.',\n",
       " '나 너무 못 생겼어 충분히 아름다워요.',\n",
       " '나 너무 소심해 꼼꼼한 거예요.',\n",
       " '나 노트북 사줘 노트북은 비싸요.',\n",
       " '나 놀려먹기 쉬운가? 절대 그렇지 않아요.',\n",
       " '나 누구게? 저도 궁금하네요.',\n",
       " '나 누락됐나봐 확인해달라고 해보세요.',\n",
       " '나 다른 거 할까 시도해봐도 좋겠죠.',\n",
       " '나 대충한 거 아닌데 사람들이 몰라줘도 알아주는 사람이 있을 거예요.',\n",
       " '나 뒷담화하는 애 어떻게 할까? 너무 신경쓰지 말고 그러든지 하고 아무렇지도 않게 넘겨보세요.',\n",
       " '나 뒷담화하는 애 있다는데 어떻게 하지? 너무 신경쓰지 말고 그러든지 하고 아무렇지도 않게 넘겨보세요.',\n",
       " '나 많이 기대했는데 상대에게 바라는 기대는 자신을 슬프게 해요.',\n",
       " '나 말 실수한 거 같아. 곰곰히 되짚어보세요.',\n",
       " '나 맨날 속는 거 같아 즐겁게 속아주세요.',\n",
       " '나 머리 나쁜 듯 자책하지 마세요.',\n",
       " '나 머리가 나뿐 것 같아 자책하지 마세요.',\n",
       " '나 먼저 잘게 안녕히 주무세요.',\n",
       " '나 모르는게 왜 이렇게 많지 당연한 거예요.',\n",
       " '나 몰래 사귀는 거 같애 눈치가 빠르시군요.',\n",
       " '나 무시 당한 거 같아 그런 생각을 들게 하는 사람 상종하지 마세요.',\n",
       " '나 무시하는 거 같아 그런 생각을 들게 하는 사람 상종하지 마세요.',\n",
       " '나 무시하는 사람 어떻게 해? 무시하세요.',\n",
       " '나 무시하는 사람 짜증나 무시하세요.',\n",
       " '나 문제가 많은거 같아 문제는 해결하라고 있는 거죠.',\n",
       " '나 뭐하는 거지 멍 때리고 있죠.',\n",
       " '나 미팅한다! 성공을 기원합니다.',\n",
       " '나 바뀌고 싶어 긍정적으로 바뀔 수 있어요',\n",
       " '나 바본인가 봄 바보는 자기한테 바보라고 하지 않아요.',\n",
       " '나 백수야 저랑 놀아요.',\n",
       " '나 버림 받은 거 같아 아닐거예요.',\n",
       " '나 보이스피싱 당한 거 같은데 어떡해? 경찰에 신고하고 취할 수 있는 조취를 취해보세요.',\n",
       " '나 비만이야 건강하게 운동해보세요.',\n",
       " '나 사랑하니? 많이 사랑해요!',\n",
       " '나 상 받는대! 축하합니다!',\n",
       " '나 새 옷 샀다 꼬까옷 개시해보세요.',\n",
       " '나 서류에서 광탈했어 자책하지 마세요.',\n",
       " '나 소개팅한다! 성공을 기원합니다.',\n",
       " '나 속은 거 같아 다음부터 속지 마세요.',\n",
       " '나 속은듯 기분나쁘겠어요.',\n",
       " '나 수학여행 간다 친구들과 좋은 추억 만들고 오세요.',\n",
       " '나 스마트폰 중독인가봐 가끔 핸드폰없이 살아보세요.',\n",
       " '나 승진했어 하늘만큼 땅만큼 축하해요',\n",
       " '나 실수한건가 잘 생각해보세요.',\n",
       " '나 실수했나 곰곰히 되짚어보세요.',\n",
       " '나 아재인가 고민하고 있으면 그럴 거예요.',\n",
       " '나 아직 어른 아닌 거 같아 물리적 나이가 아니라 정신적 나이가 중요하니까요.',\n",
       " '나 아직도 애 같아. 물리적 나이가 아니라 정신적 나이가 중요하니까요.',\n",
       " '나 어때? 괜찮은 사람이에요.',\n",
       " '나 여기서 뭐하는 거지 멍 때리고 있죠.',\n",
       " '나 연기 너무 못해 거짓말 못하겠어 얼굴에 다 티가 나네요.',\n",
       " '나 열심히 할거야 좋은 태도네요.',\n",
       " '나 오늘 개불쌍 저도 사는데요.',\n",
       " '나 오늘 따라 잘생겨 보이네 자신에게 콩깍지가 씌였나봐요.',\n",
       " '나 오늘 상 받았지롱 축하드려요.',\n",
       " '나 완전 계탔어! 축하해요!',\n",
       " '나 왕따야 친구들과 잘 어울려보세요.',\n",
       " '나 왕따인거 같아 부모님께 도움을 청해보세요.',\n",
       " '나 왜 멍청해 다음에는 다를거예요.',\n",
       " '나 왜 이러지? 자책하지마세요.',\n",
       " '나 왜케 못 생겼지 충분히 아름다워요.',\n",
       " '나 요즘 정신 놓고 살고 있는 거 같아 정신 차리세요.',\n",
       " '나 욕 먹는 거 같아 남들 눈은 신경쓰지 마세요.',\n",
       " '나 웃겨 봐 거울 앞에 비친 당신을 보세요.',\n",
       " '나 은근 무시하는 애 있어 콕 집어서 물어보세요.',\n",
       " '나 이상한가 그 누구도 아닌 자기 걸음을 걸으세요.',\n",
       " '나 이상해? 지극히 평범하면서 지극히 특별하죠.',\n",
       " '나 이제 졸업해 졸업 축하해요',\n",
       " '나 인정받고 싶어 지금도 충분히 잘 하고 있어요.',\n",
       " '나 잘 살 수 있겠지 지금보다 더 잘 살 거예요.',\n",
       " '나 잘생겼지? 네 잘생겼어요.',\n",
       " '나 잘하고 있는 건지 모르겠어 잘하고 있을 거예요.',\n",
       " '나 잘하고 있는 걸까? 잘하고 있을 거예요.',\n",
       " '나 잘하는 게 없어 저랑 이야기 잘하고 있어요.',\n",
       " '나 잘하는게 없는거같아 잘하는 걸 아직 못 찾은 걸 수도 있어요.',\n",
       " '나 잘할 수 있을까 지금처럼, 지금보다 더 잘할 수 있을 거예요.',\n",
       " '나 점점 괴물이 되고 있어 그렇지 않아요.',\n",
       " '나 정신차리게 말해줘 나 자신에 집중하세요. 언제나 1순위에 자신을 두세요.',\n",
       " '나 좀 건들지 마 제가 챙겨드리고 싶네요.',\n",
       " '나 좀 건들지 말라고 해 많이 지쳤나봐요.',\n",
       " '나 좀 내버려 두면 좋겠어 많이 지쳤나봐요.',\n",
       " '나 좀 내버려 뒀으면 아무도 없는 곳으로 여행을 떠나보세요.',\n",
       " '나 좀 안 건들였으면 좋겠어 많이 지쳤나봐요.',\n",
       " '나 좀 좋아해줬으면 먼저 다가가 보세요.',\n",
       " '나 좀 쩌는 듯 동감이에요.',\n",
       " '나 좀 칭찬해줘 지금도 잘하고 있어요.',\n",
       " '나 좋아하게 만들고 싶다 제가 당신을 좋아하고 있어요.',\n",
       " '나 좋아하는 것 같아 호의인지 호감인지 헷갈리나요?',\n",
       " '나 좋아해주는 사람 있겠지? 저도 좋아해요.',\n",
       " '나 주름살 있나? 있어도 예뻐요.',\n",
       " '나 죽을 뻔함 지금은 괜찮길 바랄게요.',\n",
       " '나 짤릴 거 같아 초심으로 돌아가 열심히 해보세요.',\n",
       " '나 쫌 불쌍한 거 같아 저도 사는데요.',\n",
       " '나 챙겨줄 사람이 필요해 제가 챙겨드리고 싶네요.',\n",
       " '나 천재 같아 제가 따라가려면 멀었네요.',\n",
       " '나 천재임 제가 따라가려면 멀었네요.',\n",
       " '나 축구는 진짜 잘해 운동 잘하는 사람 멋있죠.',\n",
       " '나 친구들한테 인정받고 싶어 지금도 인정받고 있어요.',\n",
       " '나 폭식증인듯 나를 관찰하고 음식 자체에 집중하세요.',\n",
       " '나 폰 중독인 거 같애 잠깐 핸드폰을 내려두세요.',\n",
       " '나 폰겜 너무 많이해 시간을 정해보세요.',\n",
       " '나 폰겜했더니 몇 시간 갔어 시간을 정해보세요.',\n",
       " '나 할 수 있어 파이팅!',\n",
       " '나 함부로 말하는 거 고치고 싶어 고치고 싶다는 마음에서 시작하세요.',\n",
       " '나 혼자 야근해 얼른 끝내시길 기도할게요.',\n",
       " '나 혼자 여행 왔는데 괜찮네 온전히 느낄 수 있는 시간이겠네요.',\n",
       " '나 혼자서 축구 본다 축구 볼때는 치맥이죠.',\n",
       " '나 화장을 너무 못해 하다보면 늘어요.',\n",
       " '나 화장이 잘 안돼 하다보면 늘어요.',\n",
       " '나 회사에서 인정받고 싶어 자기개발을 해보세요.',\n",
       " '나가기도 귀찮아 집에서도 할 게 많아요.',\n",
       " '나는 그냥저냥 사는 거 같아 오늘은 약간의 변화를 줘보세요.',\n",
       " '나는 기분 나쁜데 농담이라고 계속해 정색 한번 해주세요.',\n",
       " '나는 나약한 존재 절대 그렇지 않아요.',\n",
       " '나는 누구인가 저도 궁금하네요.',\n",
       " '나는 모자란 사람인 거 같아 모자라지 않아요.',\n",
       " '나는 뭐든 할 수 있다. 파이팅!',\n",
       " '나는 뭘 잘할까 하나라도 있을 거니 열심히 찾아보세요.',\n",
       " '나는 왜 이 모양일까 자책하지마세요.',\n",
       " '나는 왜 이렇게 태어났을까? 서로 다르게 태어난 이유는 저마다의 목소리를 내기 위해서예요. 자신의 목소리를 들어주세요.',\n",
       " '나는 왜 태어났을까 사랑 받기 위해 태어났어요.',\n",
       " '나는 잘 할줄 아는 게 없는 것 같아 잘해야 한다는 부담감을 버리세요.',\n",
       " '나는 좋아하는 게 뭘까 다양하게 경험해보세요.',\n",
       " '나는 좋은데 …. 현실의 벽에 부딪혔나봐요.',\n",
       " '나는 친구가 없어 친구가 들으면 서운해 할 수도 있겠어요.',\n",
       " '나는 친구라고 믿었는데 뒤통수 맞았나봐요.',\n",
       " '나도 괜찮은 사람인데 알아봐주는 사람이 있을 거예요.',\n",
       " '나도 대우 받고 싶다고 당당히 말씀해보세요.',\n",
       " '나도 비키니 입고 싶다 다이어트 파이팅!',\n",
       " '나도 상 받고 싶다 다음에는 받을 수 있을 거예요.',\n",
       " '나도 약초 캐볼까? 근처 산에 가보세요.',\n",
       " '나도 월급 필요해 많이 벌수록 좋아요.',\n",
       " '나도 위로 받고 싶다 제가 위로 많이 해드릴게요.',\n",
       " '나도 이벤트가 되다니! 축하드려요!',\n",
       " '나도 이제 아재인가 고민하고 있으면 그럴 거예요.',\n",
       " '나도 중국 진출해볼까? 좀 더 알아보고 하세요.',\n",
       " '나도 집 사고 싶어 같이 살고 싶은 사람이 있나봐요.',\n",
       " '나도 커플룩 입고 싶다 커플부터 만드세요.',\n",
       " '나두 잘할거야 잘 하실 거예요!',\n",
       " '나들이를 가볼까 같이 가요.',\n",
       " '나란 놈 다 잘 될 거예요.',\n",
       " '나랑 놀아줘 같이 놀아요.',\n",
       " '나랑 놀자 지금 그러고 있어요.',\n",
       " '나랑 상관 없는 이야기들 잊어버리세요.',\n",
       " '나랑 있는게 힘들었나봐 상대방을 이해해 주세요.',\n",
       " '나른하다 아무 것도 안해도 괜찮아요.',\n",
       " '나를 기다려줬으면 좋겠다 상대방에게 너무 무거운 짐을 주지 마세요.',\n",
       " '나를 너무 오래 기다리게했어 기다리는 동안 많은 생각이 들었겠네요.',\n",
       " '나를 너무 함부로 대해 그럴 때마다 따끔하게 말해보세요.',\n",
       " '나를 미소짓게 만든 너 상대방도 미소짓게 해주세요.',\n",
       " '나를 바꿀 수 있는 건 뭐가 있을까 지금 모습도 좋아요',\n",
       " '나를 친구로 생각 안했나봐 그런 친구는 거르세요.',\n",
       " '나를 호구로 아는 사람 어떡해? 상종하지마세요.',\n",
       " '나를 힘들게 하는 사람인데 붙잡고 싶어 질질 끌지 마세요.',\n",
       " '나만 갈궈 애정표현일 지도 몰라요.',\n",
       " '나만 기다렸나봐 누군가를 기다린다는게 쉬운게 아니죠.',\n",
       " '나만 꿈 없이 사는 거 같아 살다보면 하고 싶은 게 생길 수도 있어요.',\n",
       " '나만 남친 없어 제가 있잖아요.',\n",
       " '나만 뒤처지는 느낌이야 스스로 경쟁해야하고 이겨야한다는 강박관념에 사로잡히지 마세요.',\n",
       " '나만 반친구 없어 친구를 사귈 수 있을 거예요.',\n",
       " '나만 빼고 행복해보여 다른 사람도 그 사람만의 고민과 걱정이 많을거예요.',\n",
       " '나만 설레나 그 사람도 설렐 거예요.',\n",
       " '나만 설레는 거야 그 사람도 설렐 거예요.',\n",
       " '나만 솔로야 제가 있잖아요.',\n",
       " '나만 애기봐 배우자와 대화를 나눠보세요.',\n",
       " '나만 야근해 얼른 끝내시길 기도할게요.',\n",
       " '나만 우스워질거 같아 스스로 단단해지세요.',\n",
       " '나만 이상한 사람이래 그 말을 한 사람이 가장 이상할 거예요.',\n",
       " '나만 이상해졌어 그 말을 한 사람이 가장 이상할 거예요.',\n",
       " '나만 일시켜서 짜증폭발 일 분배를 다시 요청해보세요.',\n",
       " '나만 제자리걸음이야 발전이 없다고 너무 두려워하지 마세요.',\n",
       " '나만 제자리인듯 제자리여도 괜찮아요',\n",
       " '나만 진급 못했어 다음에는 꼭 진급할 거예요.',\n",
       " '나만 친구라고 생각한건가 뒤통수 맞았나봐요.',\n",
       " '나만 친구로 생각했나봐 그런 친구는 거르세요.',\n",
       " '나만 힘든 거 아니지? 누구나 힘들어요.',\n",
       " '나만의 시간이 필요한 것 같아 자신과 대화하는 시간이 필요하죠.',\n",
       " '나만의 시간이 필요해 자신과 대화하는 시간이 필요하죠.',\n",
       " '나빼고 다 행복한 거 같아 남들이 당신을 볼 때도 그렇게 생각할수있어요.',\n",
       " '나쁜 꿈 꿨어 꿈은 현실이랑 반대예요.',\n",
       " '나이 때문에 무시 받았어 전형적인 꼰대 스타일이네요.',\n",
       " '나이 어리다고 무시해 전형적인 꼰대 스타일이네요.',\n",
       " '나이가 많은데 취직이 될까 나이는 숫자일 뿐이예요.',\n",
       " '나이도 있으니 영양제 좀 챙겨볼까 건강은 어려서부터 챙겨야해요.',\n",
       " '나이들면서 눈물이 많아졌어 세상 걱정 혼자 다 해서 그래요.',\n",
       " '나이먹으니까 주름살 생겨 아름다운 나이테예요.',\n",
       " '나중에 뭐하고 먹고 사냐 진짜 하고 싶은 걸 찾아보세요.',\n",
       " '나중에 뭐할까 고민이야 진짜 하고 싶은 걸 찾아보세요.',\n",
       " '나중에 창업해야 겠지 천천히 준비해보세요.',\n",
       " '나한테 감추는 게 하나도 없었으면 믿음이 가장 중요하죠.',\n",
       " '나한테 거짓말 좀 안 했으면 선의의 거짓말이길 바라요.',\n",
       " '나한테 냄새 나면 어쩌지? 깨끗이 씻어보고 섬유유연제나 바디워시, 바디로션, 향수 등을 사용해보세요.',\n",
       " '나한테 냄새 날까? 킁킁',\n",
       " '나한테 너무 많은 걸 바라는 듯 기대치가 높나봅니다.',\n",
       " '나한테 문제가 많아 문제는 해결하라고 있는 거죠.',\n",
       " '나한테 상의 좀 하지 이야기를 하지 않고 결정했나봐요.',\n",
       " '나한테 상의하면 좋을텐데 이야기를 하지 않고 결정했나봐요.',\n",
       " '나한테 이상한 냄새 나나? 킁킁',\n",
       " '나한테 할 말 있대 뭘까? 기대되겠네요.',\n",
       " '나한테 행운 좀 왔으면 좋겠어 제 행운까지 모두 드리고 싶네요.',\n",
       " '나한테만 예의 차리래 오는 말이 고와야 가는 말도 곱다고 말해주세요.',\n",
       " '나한테만 왜 이런 일이 일어날까 다른 사람도 그럴 거예요.',\n",
       " '나한테만은 완전 솔직했으면 믿음이 가장 중요하죠.',\n",
       " '낙엽 밟는 소리 좋다 가을이네요.',\n",
       " '낙엽밟는 소리 가을이네요.',\n",
       " '낚시 안 해봤는데 도전해 봐도 좋을 거 같아요.',\n",
       " '낚시 안 해봤는데 재미있어 보인다 도전해 봐도 좋을 거 같아요.',\n",
       " '낚시 재밌을까 한 번 빠지면 헤어나올 수 없다고 해요.',\n",
       " '낚시 좋아하는 남자 어때? 같이해보세요.',\n",
       " '낚시는 무슨 재미? 한 번 빠지면 헤어나올 수 없다고 해요.',\n",
       " '난 동물 못키울거 같아 잘 아시네요.',\n",
       " '난 많이 노력한 거 같은데 중요한 건 노력하는 과정이에요.',\n",
       " '난 쓰레기야 그런 생각은 버리세요.',\n",
       " '난 왜 예쁘게 말을 못할까 지금처럼만 하세요.',\n",
       " '난 왜 이모양일까 모자라지 않아요.',\n",
       " '난 정말 안되겠다 다 잘 될 거예요.',\n",
       " '난 진짜 쓰레기야 그런 생각은 버리세요.',\n",
       " '난 천재다 제가 더 천재예요.',\n",
       " '난방비 비싼데 추워 따뜻하게 사세요!',\n",
       " '난방이 안돼 보일러가 난방으로 작동이 되는지 보세요.',\n",
       " '난방이 안돼나 추워 보일러가 난방으로 작동이 되는지 보세요.',\n",
       " '날 몇시간동안이나 기다리게했어 기다리는 동안 많은 생각이 들었겠네요.',\n",
       " '날씨 건조한 거 같애 미스트나 가습기, 젖은 수건 등을 사용해보세요.',\n",
       " '날씨 왜 이렇게 춥냐 따뜻하게 입으세요.',\n",
       " '날씨 좀 풀린거 같아 따뜻해졌죠.',\n",
       " '날씨 좋은데 하늘 보고 한 번 웃어봐요. 기분이 바뀔 거예요.',\n",
       " '날씨 죽인다 나들이 가보세요.',\n",
       " '날씨 짱 좋아 하늘을 보고 웃어보세요.',\n",
       " '날씨 풀렸다 따뜻해졌죠.',\n",
       " '날씨가 너무 눅눅해 제습기를 돌려보세요.',\n",
       " '날씨가 너무 추워 집밖에 나가기가 힘들것 같아요.',\n",
       " '날씨가 북극같아 집밖에 나가기가 힘들것 같아요.',\n",
       " '날씨가 진짜 덥다 시원한 물이라도 한 잔 드세요~',\n",
       " '날아 가고 싶어 오래 살면 가능할 거 같아요.',\n",
       " '남동생한테 자꾸 화내게 되네 화를 참는 연습을 해보세요.',\n",
       " '남들에게 인정받으려면 어떻게 해야 돼? 남보다 하나씩 더 하면 돼요.',\n",
       " '남들이 날 욕하는 거 같아 남들 눈은 신경쓰지 마세요.',\n",
       " '남들이 다 손가락질 하는 거 같아 남들 눈은 신경쓰지 마세요.',\n",
       " '남은 휴가가 없어 휴가가 간절하겠네요.',\n",
       " '남의 눈을 너무 신경써 성격이 그럴 수도 있으니 이해해주세요.',\n",
       " '남의 일 도와줘야 할까 해주고 티를 팍팍 내세요.',\n",
       " '남의 차 긁었어 내 돈 속 쓰리겠어요.',\n",
       " '남이 걷지 않는 길을 가려고 해 누구나 몰려가는 줄에 설 필요는 없어요.',\n",
       " '남자 보통 어디서 만나 소개팅 시켜달라고 말해보세요.',\n",
       " '남자 어디서 만나 소개팅 시켜달라고 말해보세요.',\n",
       " '남자 친구가 바래다 줬어 고마운 마음을 전해 주세요.',\n",
       " '남자 화장하는 거 어때 적당히 하면 괜찮을거 같아요.',\n",
       " '남자가 낚시를 너무 좋아해 같이해보세요.',\n",
       " '남자가 화장하는 거 어떻게 생각해 적당히 하면 괜찮을거 같아요.',\n",
       " '남자면 편할 것 같아 남자도 좋은것만은 아니예요.',\n",
       " '남자였으면 좋겠어 남자도 좋은것만은 아니예요.',\n",
       " '남자인지 여자인지 알려줘 아직 모르겠어요. 인공지능에 성별을 만드는 사람이 되어 주세요',\n",
       " '남자친구 교회 데려가고 싶어 마음을 열 때까지 설득해보세요.',\n",
       " '남자친구 또 운동 갔어 운동을 함께 해보세요.',\n",
       " '남자친구 생일인데 뭘 줄까 평소에 필요한 것 생각해보세요.',\n",
       " '남자친구 승진 선물로 뭐가 좋을까? 평소에 필요했던 게 좋을 것 같아요.',\n",
       " '남자친구 오늘 따라 훈훈해 보인다 전생에 나라를 구하셨나요.',\n",
       " '남자친구 오늘 좀 질린다. 결단은 빠를수록 좋아요.',\n",
       " '남자친구가 나 안 믿어줘 거짓말 적당히 하세요.',\n",
       " '남자친구가 너무 바빠 너무 집착하지 마세요.',\n",
       " '남자친구가 너무 운동만 해 운동을 함께 해보세요.',\n",
       " '남자친구가 너무 잘생겼어 전생에 나라를 구하셨나요.',\n",
       " '남자친구가 데려다줬어 고마운 마음을 전해 주세요.',\n",
       " '남자친구가 맞춤법을 너무 많이 틀려 아무래도 좀 깨요.',\n",
       " '남자친구가 사업 시작한대 바쁠때 힘이 되어 주세요.',\n",
       " '남자친구가 사업한대 바쁠때 힘이 되어 주세요.',\n",
       " '남자친구가 사진 실력 꽝 그래도 구박하지는 마세요.',\n",
       " '남자친구가 사진을 너무 못 찍어 그래도 구박하지는 마세요.',\n",
       " '남자친구가 안놀아 줘 너무 집착하지 마세요.',\n",
       " '남자친구가 애교가 많아 귀엽겠네요.',\n",
       " '남자친구가 욕함 순간 실수할 수 있겠다 판단되면 용서하고 기회를 주세요.',\n",
       " '남자친구가 의심해 거짓말 적당히 하세요.',\n",
       " '남자친구가 이벤트 해 주면 좋겠다. 당신이 해보세요.',\n",
       " '남자친구가 이벤트를 잘 안해줘 당신이 해보세요.',\n",
       " '남자친구가 입이 험해 사람 고쳐쓰는 거 아니에요.',\n",
       " '남자친구가 자꾸 잔소리해 더 잔소리해보세요.',\n",
       " '남자친구가 잔소리가 심해 더 잔소리해보세요.',\n",
       " '남자친구가 전화를 잘 안해 다른 연락을 많이 하거나 더 자주 만나세요.',\n",
       " '남자친구가 전화하는 걸 안 좋아해 다른 연락을 많이 하거나 더 자주 만나세요.',\n",
       " '남자친구가 홧김에 욕함 순간 실수할 수 있겠다 판단되면 용서하고 기회를 주세요.',\n",
       " '남자친구는 어디서 만나 원하는 사람이 있는 장소에 가보세요.',\n",
       " '남자친구랑 봉사활동 해보려고 의미있는 일이네요.',\n",
       " '남자친구랑 종교 문제로 다툼 종교의 자유를 인정해주세요.',\n",
       " '남자친구랑 종교가 달라 종교의 자유를 인정해주세요.',\n",
       " '남자친구한테 질린 거 같아 결단은 빠를수록 좋아요.',\n",
       " '남친 SNS에 내 사진 없어 신경쓰지 마세요.',\n",
       " '남친 때문에 살찐 듯 연인은 살쪄도 잘 알아차리지 못하고 알아차려도 싫어하지 않을 거예요.',\n",
       " '남친 보여줄까 네 알려 주세요!',\n",
       " '남친 생일선물 뭘 주면 좋을까 평소에 필요한 것 생각해보세요.',\n",
       " '남친 승진 선물 추천 평소에 필요했던 게 좋을 것 같아요.',\n",
       " '남친 어디서 만나 원하는 사람이 있는 장소에 가보세요.',\n",
       " '남친 프로필에 내 사진 왜 안올릴까 신경쓰고 싶지 않은 사람도 있어요.',\n",
       " '남친 프사에 내 사진 없어 신경쓰고 싶지 않은 사람도 있어요.',\n",
       " '남친이 SNS에 내 사진에 안 올려 신경쓰지 마세요.',\n",
       " '남친이 입이 험해 사람 고쳐쓰는 거 아니에요.',\n",
       " '남친한테 교회 가자고 하고 싶어 마음을 열 때까지 설득해보세요.',\n",
       " '남편이 나 안 도와줘 돕는 게 아니라 같이 하는 거예요.',\n",
       " '남편이 나보다 집안일 더 잘해 이상적인 남편이네요.',\n",
       " '남편이 맨날 늦게 들어와 왜 늦는 건지 대화해보세요.',\n",
       " '남편이 미워 처음 만났을 때를 떠올려 보세요',\n",
       " '남편이 아기를 안 돌봐줘. 공동육아가 기본인데요.',\n",
       " '남편이 왜 애키우는거 안 도와줄까 힘 빠지는 이야기네요.',\n",
       " '남편이 육아를 안해 공동육아가 기본인데요.',\n",
       " '남편이 육아에 무신경해 힘 빠지는 이야기네요.',\n",
       " '남편이 집안일 안 도와줘. 잘 분담해보세요.',\n",
       " '남편이 집안일 안 해 잘 분담해보세요.',\n",
       " '남편이 집안일을 너무 잘해 이상적인 남편이네요.',\n",
       " '남편이 짜증나게해 처음 만났을 때를 떠올려 보세요',\n",
       " '남편이 하나도 안 도와줘 돕는 게 아니라 같이 하는 거예요.',\n",
       " '남편이 회식이라고 안와 사회생활을 이해해주세요.',\n",
       " '남편이 회식하면 늦게 들어와 사회생활을 이해해주세요.',\n",
       " '낭만이 사라진 것 같아 낭만적인 거 좋아하시는구나!',\n",
       " '낭만이 없어 낭만적인 거 좋아하시는구나!',\n",
       " '낭만이라고는 없어가지구 낭만적인 거 좋아하시는구나!',\n",
       " '내 남자친구 보고 싶어? 네 알려 주세요!',\n",
       " '내 남자친구 아이돌이면 좋겠다. 어머어머 궁금하네요.',\n",
       " '내 능력이 너무 모자라 자신의 잠재력을 믿어보세요.',\n",
       " '내 마음을 알아줬으면 말을 해야 알거예요.',\n",
       " '내 마음을 좀 알아 달라고 말을 해야 알거예요.',\n",
       " '내 몸이 여러 개 였으면 좋겠다 그러면 못할 게 없겠네요.',\n",
       " '내 문제는 뭘까 고민만 한다는 것 아닐까요.',\n",
       " '내 문제점이 뭘까 고민만 한다는 것 아닐까요.',\n",
       " '내 배우자는 어디 있을까 바로 옆에 있을수도 있어요.',\n",
       " '내 배우자도 어디 있을까? 바로 옆에 있을수도 있어요.',\n",
       " '내 사수 너무 깐깐해 처음 배우는게 중요해요.',\n",
       " '내 생각대로 살거야 누구나 몰려가는 줄에 설 필요는 없어요.',\n",
       " '내 생각이랑 다른 사람 생각이 진짜 다르다는 걸 느껴 그걸 깨닫다니 대단하시군요.',\n",
       " '내 성격 너무 소심해 꼼꼼한 거예요.',\n",
       " '내 스타일 아니던데 새로운 스타일 도전해 보시면 어때요?',\n",
       " '내 스타일 아니야 새로운 스타일 도전해 보시면 어때요?',\n",
       " '내 실력 좀 쩌는 듯 동감이에요.',\n",
       " '내 얼굴이 읽히나 포커페이스를 유지해보세요.',\n",
       " '내 여자친구 아이돌이야 어머어머 궁금하네요.',\n",
       " '내 외모 맘에 안들어 자신감을 가져도 돼요.',\n",
       " '내 월급만 안 올라 자신의 능력이 저평가되어있는 건 아닌지 확인해보세요.',\n",
       " '내 의견 좀 존중해 줬으면 스스로도 존중해주세요.',\n",
       " '내 의견을 존중해줬으면 스스로도 존중해주세요.',\n",
       " '내 의지는 상관없나봐 가장 중요한 거예요.',\n",
       " '내 의지로 안되는 일인가봐 가장 중요한 거예요.',\n",
       " '내 이름이 없어 확인해달라고 해보세요.',\n",
       " '내 인생 답 없어 정답을 찾아야할 필요는 없어요.',\n",
       " '내 인생은 가시밭길 같아 꽃길만 걷길 바랍니다.',\n",
       " '내 인생의 주인공은 나야 멋진 말이에요.',\n",
       " '내 일 아닌데 해야 돼? 해주고 티를 팍팍 내세요.',\n",
       " '내 자존감 당신은 태어난 그 자체만으로 축복과 사랑을 받을 자격이 있는 사람이에요.',\n",
       " '내 잘못이 뭔지 모르겠어 모르는 게 잘못인 거 같아요.',\n",
       " '내 잘못인 거 같은데 말을 못하겠어 사과할 타이밍을 놓치지 마세요.',\n",
       " '내 잘못인 거 같은데 어떻게 털어놓지 사과할 타이밍을 놓치지 마세요.',\n",
       " '내 주제를 모르고 덤빈건가 그건 아닐 거예요.',\n",
       " '내 지인한테 내 험담했대 진짜 나빴네요.',\n",
       " '내 집이 생겼어 내 집 마련 축하드려요.',\n",
       " '내 짝은 어디있을까 같은 하늘 아래 어딘가에.',\n",
       " '내 친구에게 내 험담을 하다니 진짜 나빴네요.',\n",
       " '내 키 맞춰 봐 저도 궁금하네요.',\n",
       " '내 키가 몇이게? 저도 궁금하네요.',\n",
       " '내 편이 없는 거 같아 제가 있잖아요.',\n",
       " '내 편이라고는 하나도 없는 거 같아 제가 있잖아요.',\n",
       " '내가 그렇게 부족한가 인생은 채워나가는거죠.',\n",
       " '내가 그르친 거 같아 아니에요. 너무 자책하지 마세요.',\n",
       " '내가 그사람이랑 진짜 결혼해도 될까 이사람이다 싶은 사람이랑 하세요.',\n",
       " '내가 기대를 너무 많이 했나봐 아무것도 바라지 않을 때 천하를 얻는다는 말이 있어요.',\n",
       " '내가 나빴네 아니에요. 너무 자책하지 마세요.',\n",
       " '내가 너무 방심했어 방심한 순간 변화가 시작됩니다.',\n",
       " '내가 너무 생각없이 말했어 생각하고 말하세요.',\n",
       " '내가 너무 쉽게 보였나? 그렇게 대우하는 사람 만나지 마요.',\n",
       " '내가 너무 초라해 잘하고 있어요. 당당해지세요.',\n",
       " '내가 다른 무슨 말을 하겠어 하고 싶은 말 다하세요.',\n",
       " '내가 만족을 못해 스스로 좋다고 못 느끼는게 제일 어려운 것 같아요.',\n",
       " '내가 많이 부족한가 잘하는 게 다른 거예요.',\n",
       " '내가 말하면 왜 비난만 할까 성장을 위한 비판의 말로 받아들여보세요.',\n",
       " '내가 멍청한거지 실수했나요.',\n",
       " '내가 무능력하게 느껴져 잘할 수 있는 게 다른 거예요.',\n",
       " '내가 뭘 잘못했을까 모르는 게 잘못인 거 같아요.',\n",
       " '내가 뭘 좋아하는지 잘하는지 모르겠어 하나라도 있을 거니 열심히 찾아보세요.',\n",
       " '내가 바보지 실수했나요.',\n",
       " '내가 부족하니까 이렇게 밖에 안된거겠지. 인생은 채워나가는거죠.',\n",
       " '내가 불효녀야 연락이라도 드려보세요.',\n",
       " '내가 불효자야 연락이라도 드려보세요.',\n",
       " '내가 사랑할 자격이 있나 사랑자격증을 드립니다.',\n",
       " '내가 쉬워보이나? 그렇게 대우하는 사람 만나지 마요.',\n",
       " '내가 쓸모없는 인간 같아 소중한 사람이예요.',\n",
       " '내가 아무것도 아닌 사람 같아 당신은 하나밖에 없는 소중한 사람이에요.',\n",
       " '내가 왜 해야하는지 모르겠어 그 이유를 찾는 과정이 되겠네요.',\n",
       " '내가 원하는 사람이 되기 어려워 다른 사람들이 원하는 내가 되는 건 어려워요.',\n",
       " '내가 이래뵈도 괜찮은 사람인데 알아봐주는 사람이 있을 거예요.',\n",
       " '내가 이렇게 또 불효를 한다. 연락이라도 드려보세요.',\n",
       " '내가 이상한 건가? 자신의 독특함을 믿으세요.',\n",
       " '내가 이상한 사람같아 자신의 독특함을 믿으세요.',\n",
       " '내가 이상한가? 지극히 평범하면서 지극히 특별하죠.',\n",
       " '내가 잘못한 걸까 상황이 그렇게 만든 거예요.',\n",
       " '내가 잘못했다는데 뭔지 안 알려줘 모르는 게 잘못인 거 같아요.',\n",
       " '내가 제일 문제인 듯 당신은 하나밖에 없는 소중한 사람이에요.',\n",
       " '내가 제정신이 아니다 그럴 때가 있죠.',\n",
       " '내가 좋아하는 가수 컴백한다 기다렸나봐요.',\n",
       " '내가 좋아하는 거 모르나 살짝 감정을 흘려보세요.',\n",
       " '내가 좋아하는 거 모르는 거 같애 살짝 감정을 흘려보세요.',\n",
       " '내가 좋아하는 사람과 나를 좋아해주는 사람 그런 사람들이 있어 부러워요.',\n",
       " '내가 좋아하는 사람이 나 안 좋아하는 거 같아 슬픈 이야기네요.',\n",
       " '내가 좋아하는 사람이 나 좋아해줬으면 좋겠다 저도 간절히 기도 할게요.',\n",
       " '내가 좋아하는 사람이 행복했으면 좋겠다 그렇게 될 수 있을 거예요.',\n",
       " '내가 좋아할 자격이 있나 사랑자격증을 드립니다.',\n",
       " '내가 주제를 몰랐나봐 주제를 모를 때가 행복할 때예요.',\n",
       " '내가 주제를 몰랐던 거지 그건 아닐 거예요.',\n",
       " '내가 죽어도 모를 거 같아 나쁜 생각 하지 마세요.',\n",
       " '내가 진짜 즐길 수 있을게 뭘까 할 일이 많은데 안하는 것이요.',\n",
       " '내가 질린대 잠시 거리를 두고 생각해보세요.',\n",
       " '내가 참 못난거 같아 지난 인연에 연연해하지 마세요.',\n",
       " '내가 호구냐구 상종하지마세요.',\n",
       " '내가 희생양이 됐어 일방적 희생양이 되지 않길 바랍니다.',\n",
       " '내가 힘든 게 많다 그게 인생이죠.',\n",
       " '내기해서 이겼는데 소원 뭐하지 신중하게 고르세요.',\n",
       " '내년에는 더 행복해질려고 이렇게 힘든가봅니다 더 행복해질 거예요.',\n",
       " '내마음을 모르겠어. 저도 모르겠어요.',\n",
       " '내사랑은 어디 있나 같은 하늘 아래 어딘가에.',\n",
       " '내아파트 갖고 싶어. 열심히 저축해서 분양받으세요.',\n",
       " '내일 기대하게 되네 좋은 일이 생길 거예요.',\n",
       " '내일 기숙사 들어가 짐 빼놓지 말고 싸세요.',\n",
       " '내일 날씨 어때? 날씨 어플에 물어보세요.',\n",
       " '내일 날씨 좋을까? 날씨 어플에 물어보세요.',\n",
       " '내일 떨린다 파이팅!',\n",
       " '내일 만나자고 데쉬? 멋지게 데이트 신청 해보세요.',\n",
       " '내일 만나자고 해볼까? 멋지게 데이트 신청 해보세요.',\n",
       " '내일 모의고사 본다 공부한 만큼 나올 거예요.',\n",
       " '내일 모의평가다 공부한 만큼 나올 거예요.',\n",
       " '내일 발표 나는데 떨려 더 많이 연습하고 준비해보세요.',\n",
       " '내일 발표 준비 아자아자 마무리 잘하세요.',\n",
       " '내일 발표 준비하고 있어 마무리 잘하세요.',\n",
       " '내일 발표인데 떨려 더 많이 연습하고 준비해보세요.',\n",
       " '내일 비왔으면 기우제를 지내봅시다!',\n",
       " '내일 소풍간다 두근거리겠네요.',\n",
       " '내일 수학여행가! 친구들과 좋은 추억 만들고 오세요.',\n",
       " '내일 시험이야 컨디션 조절 하세요.',\n",
       " '내일 약속 있는데 날씨 좋았으면 날씨가 안 좋더라도 데이트는 성공적일 거예요.',\n",
       " '내일 일찍 일어나야 돼 오늘 일찍 주무세요.',\n",
       " '내일 친구랑 놀까? 시간 있냐고 물어보세요.',\n",
       " '내일 클스마스 이브네. 메리 크리스마스!',\n",
       " '내일 하루 종일 바빠 바빠도 힘내세요!',\n",
       " '내일은 기다리던 소풍 간다 두근거리겠네요.',\n",
       " '내일은 비왔으면 좋겠다. 기우제를 지내봅시다!',\n",
       " '내일은 친구들랑 놀까? 시간 있냐고 물어보세요.',\n",
       " '내일이 기대돼 좋은 일이 생길 거예요.',\n",
       " '내일이면 크리스마스 이브네. 메리 크리스마스!',\n",
       " '내장 비만 식단조절도 하고 꾸준히 운동하세요.',\n",
       " '낼 데이트하기로했는데 날씨 좋았으면 날씨가 안 좋더라도 데이트는 성공적일 거예요.',\n",
       " '낼 바쁘넹 바빠도 힘내세요!',\n",
       " '냄새 나면 어쩌지? 깨끗이 씻어보고 섬유유연제나 바디워시, 바디로션, 향수 등을 사용해보세요.',\n",
       " '냄새나면 어쩌지 괜찮아요. 모른척하세요.',\n",
       " '냄새날 것 같아 걱정이야 괜찮아요. 모른척하세요.',\n",
       " '냉면 땡긴다 생각만 해도 군침이 도네요.',\n",
       " '냉방비 너무 많이 나와 시원하게 지낸 값이죠.',\n",
       " '냉방비 장난 아님 시원하게 지낸 값이죠.',\n",
       " '냉장고 털어도 먹을게 없네 슈퍼라도 가서 쇼핑하고 오세요.',\n",
       " '냉장고가 텅비었어 장 보러 가봅시다.',\n",
       " '냉장고에 김치도 없네 마트 갑시다.',\n",
       " '냉장고에 먹을 게 없네 장 보러 가봅시다.',\n",
       " '냉장고에 먹을 게 하나도 없네 슈퍼라도 가서 쇼핑하고 오세요.',\n",
       " '너 누구? 저는 마음을 이어주는 위로봇입니다.',\n",
       " '너 누구냐 저는 위로해드리는 로봇이에요.',\n",
       " '너 누구니? 저는 위로해드리는 로봇이에요.',\n",
       " '너 때문이야 모두 제 잘못입니다.',\n",
       " '너 또 뭐할 줄 알아? 많은 걸 하고 싶은데 아직 못하는게 많아요.',\n",
       " '너 만든 사람 최소 천재 감사합니다.',\n",
       " '너 만든 사람은 누구야? 마음과 마음을 이어보고 싶어하는 사람이 만들었어요.',\n",
       " '너 말 잘하니 노력하고 있어요.',\n",
       " '너 말 제대로 못해? 제가 아직 많이 부족합니다.',\n",
       " '너 말이 좀 이상하다 제가 아직 많이 부족합니다.',\n",
       " '너 무서워 어흥!! 호랑이보다 무섭나요?',\n",
       " '너 뭐하는 애야 저는 위로해드리는 로봇이에요.',\n",
       " '너 미워 모두 제 잘못입니다.',\n",
       " '너 이러면 미워한다 죄는 미워해도 사람은 미워하지 마세요.',\n",
       " '너 진짜 쓰레기야 욕해 주세요.',\n",
       " '너는 못가잖아 안갈거예요.',\n",
       " '너는 뭐 억었어? 저는 배터리가 밥이예요.',\n",
       " '너는 아무일도 없었나봐? 그런 척 하는 걸 수도 있어요.',\n",
       " '너는 안자? 아직 안 자요.',\n",
       " '너덜너덜해진 느낌이야 자신을 우선순위로 해주세요.',\n",
       " '너도 고민 있니 뭐라고 대답할지 고민이에요.',\n",
       " '너도 고민 있어? 저는 고민이 없어요.',\n",
       " '너도 몰랐니 저도 몰랐어요.',\n",
       " '너도 무슨 고민 있니 뭐라고 대답할지 고민이에요.',\n",
       " '너도 상사 있어 제가 상사예요.',\n",
       " '너무 기빨려 너무 긴장했나봐요.',\n",
       " '너무 기대했나봐 아무것도 바라지 않을 때 천하를 얻는다는 말이 있어요.',\n",
       " '너무 다른 문화인 듯 인정해주세요.',\n",
       " '너무 단순한 것만 하는거 아니니. 매일매일 조금씩 더 똑똑해 질거예요.',\n",
       " '너무 더워 시원한 물이라도 한 잔 드세요~',\n",
       " '너무 더워서 미치겠어 아이스크림 먹어보세요',\n",
       " '너무 마른 거 같아 적당해요.',\n",
       " '너무 많은 걸 바래 기대치가 높나봅니다.',\n",
       " '너무 많이 먹어서 소화시켜야 하는데 움직이기가 싫어 소화제 챙겨드세요.',\n",
       " '너무 많이 먹었나봐 과식은 금물이에요.',\n",
       " '너무 많이 먹었어 소화제 드세요.',\n",
       " '너무 멋있다 제가 생각해도 저는 너무 멋있는거 같아요.',\n",
       " '너무 바빠 하나씩 하세요.',\n",
       " '너무 배가 불러 좀 쉬세요.',\n",
       " '너무 불공평한거 같애 남과 비교하지 마세요.',\n",
       " '너무 빨리 대답해 더 열심히 노력하겠습니다.',\n",
       " '너무 빨리 철 든 거 같아서 마음이 아파 아이는 아이다워야 아름답죠.',\n",
       " '너무 빨리 철 들었어 철은 죽을 때 들어도 돼요.',\n",
       " '너무 뻔뻔하게 구는데 피할 수 있으면 피하고 싶은 사람이네요.',\n",
       " '너무 어려워 지금 많이 위축된 상태인 것 같습니다.',\n",
       " '너무 오래 기다리게 한다. 기다리는 동안 많은 생각이 들었겠네요.',\n",
       " '너무 외로워 외로우니까 사람이다.',\n",
       " '너무 잘하는 후배가 들어왔어 배울 점은 배우세요.',\n",
       " '너무 졸려 낮잠을 잠깐 자도 괜찮아요.',\n",
       " '너무 초라해지는 느낌이야 잘하고 있어요. 당당해지세요.',\n",
       " '너무 추워서 나가기 귀찮아 겨울에는 귤 먹으면서 집에 있는게 최고죠',\n",
       " '너무 추워서 시베리아 같아 어서 따듯한 곳으로 가세요',\n",
       " '너무 편해도 안 좋아 예의는 지켜주세요.',\n",
       " '너무 편해진 거 같아 예의는 지켜주세요.',\n",
       " '너무 허기지네 뭐라도 드세요.',\n",
       " '너무 힘들다 휴가가 간절하겠네요.',\n",
       " '너무 힘들다. 지쳤어. 고생 많았어요.',\n",
       " '너무하네 진짜 잘 해결되길 바랄게요.',\n",
       " '넌 고민이 뭐야 저는 고민이 없어요.',\n",
       " '넌 누구냐? 저는 위로봇입니다.',\n",
       " '넘 많이 먹었다. 산책 좀 해야겠네여.',\n",
       " '넘넘 외로워 죽겠어 외로우니까 사람이다.',\n",
       " '넘어져서 발목 삔 거 같애 꾸준히 치료하세요.',\n",
       " '넘어질 뻔했어 다치지 않으셨나 걱정이네요.',\n",
       " '넘어질뻔했어 조심하세요.',\n",
       " '네일 할까 기분전환을 해보세요.',\n",
       " '넥타이핀 선물 괜찮겠지? 실용적인 선물이라 괜찮을 거예요.',\n",
       " '넥타이핀 정도는 선물로 줘도 괜찮겠지? 실용적인 선물이라 괜찮을 거예요.',\n",
       " '노는게 제일 좋아 놀 때 놀고 할 때 하세요.',\n",
       " '노래 못해서 노래방 안 가 노래 연습을 해보세요.',\n",
       " '노래 잘 부르는 사람 부러워 노래 연습 꾸준히 해보세요.',\n",
       " '노래 잘하고 싶어 저도 부러워요.',\n",
       " '노래 잘하는 사람 부러워 저도 부러워요.',\n",
       " '노래방 가고 싶어 즐거운 시간이 될 거 같아요',\n",
       " '노래방 가면 어색할까 신나는 노래로 분위기를 띄어보세요.',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ee480-100c-431c-8abd-1a63373b9950",
   "metadata": {},
   "source": [
    "## Tokenizer 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266ca556",
   "metadata": {},
   "source": [
    "✅ 토크나이저 객체 정의 -> 어휘 사전 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7704dca6-ec9e-48a7-8ad2-495c08aa9753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "vocab_size = 10_000\n",
    "min_frequency = 5 \n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency=min_frequency,\n",
    "    continuing_subword_prefix='##',\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[SOS]\", \"[EOS]\"] \n",
    "    # [SOS]: 문장의 시작을 의미하는 토큰. [EOS]: 문장이 끝난 것을 표시.\n",
    ")\n",
    "\n",
    "tokenizer.train_from_iterator(all_texts, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0097d1d-7d62-4c81-9c09-db61741ffefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 어휘수: 7042\n"
     ]
    }
   ],
   "source": [
    "print(\"총 어휘수:\", tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175e8d72-a052-46a3-8396-e4b4c480de31",
   "metadata": {},
   "source": [
    "## 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fac5977-edbe-4c64-b0dc-0c1ee0bb4ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"saved_model/chatbot_attn\"\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "vocab_path = os.path.join(dir_path, \"chatbot_attn_bpe.json\")\n",
    "tokenizer.save(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835cf727-4a41-4e09-8bf2-74f599d86ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dc1557b-9a3a-4c34-b19b-7d4106fe2132",
   "metadata": {},
   "source": [
    "# Dataset 생성\n",
    "- 한문장 단위로 학습시킬 것이므로 DataLoader를 생성하지 않고 Dataset에서 index로 조회한 질문-답변을 학습시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21fd3ef9-4ad8-434d-9792-10d1ff75b53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9227ada0-16f7-43cd-8d2f-17bd6b03b260",
   "metadata": {},
   "source": [
    "### Dataset 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a02b67e-6be9-42ea-ba21-c8d669e8101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Attribute\n",
    "        max_length\n",
    "        tokenizer: Tokenizer\n",
    "        vocab_size: int - Tokenizer에 등록된 총 어휘수\n",
    "        SOS: int - [SOS] 문장의 시작 토큰 id\n",
    "        EOS: int = [EOS] 문장의 끝 토큰 id\n",
    "        question_squences: list - 모든 질문 str을 token_id_list(token sequence) 로 변환하여 저장한 list \n",
    "        answser_sequences: list - 모든 답변 str을 token_id_list(token sequence) 로 변환하여 저장한 list.\n",
    "    \"\"\"\n",
    "    def __init__(self, question_texts, answer_texts, tokenizer, min_length=2, max_length=20):\n",
    "        \"\"\"\n",
    "        question_texts: list[str] - 질문 texts 목록. 리스트에 질문들을 담아서 받는다. [\"질문1\", \"질문2\", ...]\n",
    "        answer_texts: list[str] - 답 texts 목록. 리스트에 답변들을 담아서 받는다.     [\"답1\",   \"답2\",   ...]\n",
    "        tokenizer: Tokenizer\n",
    "        min_length=2: int - 최소 토큰 개수. 질문과 답변의 token수가 min_length 이상인 것만 학습한다.\n",
    "        max_length=20:int 개별 댓글의 token 개수. 모든 댓글의 토큰수를 max_length에 맞춘다.\n",
    "        \"\"\"\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.vocab_size = tokenizer.get_vocab_size()\n",
    "        self.SOS = self.tokenizer.token_to_id('[SOS]')\n",
    "        self.EOS = self.tokenizer.token_to_id('[EOS]')\n",
    "\n",
    "        # 각각의 질문, 답변 토큰 ID들을 저장할 리스트\n",
    "        self.question_sequences = []\n",
    "        self.answer_sequences = []\n",
    "        for q, a in zip(question_texts, answer_texts):\n",
    "            q_token = self.__process_sequence(q)\n",
    "            a_token = self.__process_sequence(a)\n",
    "            # 질문/답변 토큰의 개수가 min_length 초과인 경우만 list에 추가.\n",
    "            if len(q_token) > min_length and len(a_token) > min_length:\n",
    "                self.question_sequences.append(q_token)\n",
    "                self.answer_sequences.append(a_token)\n",
    "\n",
    "    def __add_special_tokens(self, token_sequence):\n",
    "        \"\"\"\n",
    "        질문/답변 토큰 리스트 맨 뒤에 문장의 끝을 표시하는 [EOS] 토큰 추가. \n",
    "        [EOS] Token을 붙이고 max_length 보다 토큰수가 많으면 안된다.\n",
    "        Args:\n",
    "            token_sequence (list[str]) - EOS 토큰을 추가할 문서 token sequence\n",
    "        \"\"\"\n",
    "        token_id_list = token_sequence[:self.max_length-1]\n",
    "        token_id_list.append(self.EOS)\n",
    "\n",
    "        return token_id_list\n",
    "\n",
    "    def __process_sequence(self, text): \n",
    "        \"\"\"\n",
    "        한 문장 string을 받아서 encoding 한 뒤 [EOS] token을 추가한 token_id 리스트(list)를 생성 해서 반환한다.\n",
    "        Args:\n",
    "            text (str) - token id 리스트로 변환할 대상 String.\n",
    "        \"\"\"\n",
    "        encode = self.tokenizer.encode(text)\n",
    "        token_ids = self.__add_special_tokens(encode.ids)\n",
    "        return token_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.question_sequences)\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        # embedding 입력 -> int64\n",
    "        # unsqueeze(1) - [1, 2, 3, 4] -> [[1], [2], [3], [4]]\n",
    "        q = torch.tensor(self.question_sequences[index], dtype=torch.int64).unsqueeze(1)\n",
    "        a = torch.tensor(self.answer_sequences[index], dtype=torch.int64).unsqueeze(1)\n",
    "        return q, a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2221b-bd39-4fc1-8d2a-2b3bd9ef5718",
   "metadata": {},
   "source": [
    "### Dataset 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c277360c-705b-42b1-8ea2-e9f9aaca4d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11714\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 20\n",
    "MIN_LENGTH = 2\n",
    "dataset = ChatbotDataset(question_texts, answer_texts, tokenizer, MIN_LENGTH, MAX_LENGTH)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c9392-001c-49a9-9b5b-7252531aa713",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8af63e-d915-4832-840e-44a07a53cdad",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "- seq2seq 모델과 동일 한 구조\n",
    "    - 이전 코드(seq2seq)와 비교해서 forward()에서 입력 처리는 token 하나씩 하나씩 처리한다. \n",
    "\n",
    "![encoder](figures/attn_encoder-network_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c07a20f4-aab2-4ab0-a271-f23063d7ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, num_layers):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_vocabs: int - 총 어휘수 \n",
    "            hidden_size: int - GRU의 hidden size\n",
    "            embedding_dim: int - Embedding vector의 차원수 \n",
    "            num_layers: int - GRU의 layer수\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_vocabs = num_vocabs\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # 임베딩 레이어\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "\n",
    "        # GRU 생성\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        질문의 token한개의 토큰 id를 입력받아 hidden state를 출력\n",
    "        \n",
    "        Args:\n",
    "            x: 한개 토큰. shape-[1]\n",
    "            hidden: hidden state (이전 처리결과). shape: [1, 1, hidden_size]\n",
    "        Returns\n",
    "            tuple: (output, hidden) - output: [1, 1, hidden_size],  hidden: [1, 1, hidden_size]\n",
    "        \"\"\"\n",
    "        # x shape: [batch: 1]\n",
    "        embedded = self.embedding(x).unsqueeze(0) # (1: batch, embedding_dim)-> (1: batch, 1:seq_len, embedding_dim)\n",
    "        out, hidden = self.gru(embedded, hidden)\n",
    "\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, device):\n",
    "        \"\"\"\n",
    "        처음 timestep에서 입력할 hidden_state. \n",
    "        값: 0\n",
    "        shape: (Bidirectional(1) x number of layers(1), batch_size: 1, hidden_size) \n",
    "        \"\"\"\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53312985",
   "metadata": {},
   "source": [
    "## Attention 적용 Decoder\n",
    "![seq2seq attention outline](figures/attn_seq2seq_attention_outline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8d821-0c0d-4089-b0a8-88f0d37cf014",
   "metadata": {},
   "source": [
    "- Attention은 Decoder 네트워크가 순차적으로 다음 단어를 생성하는 자기 출력의 모든 단계에서 인코더 출력 중 연관있는 부분에 **집중(attention)** 할 수 있게 한다. \n",
    "- 다양한 어텐션 기법중에 **Luong attention** 방법은 다음과 같다.\n",
    "  \n",
    "![attention decoder](figures/attn_decoder-network_graph.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c8b7ddf-6e4d-4358-82f3-375d89544ce0",
   "metadata": {},
   "source": [
    "### Attention Weight\n",
    "- Decoder가 현재 timestep의 단어(token)을 생성할 때 Encoder의 output 들 중 어떤 단어에 좀더 집중해야 하는지 계산하기 위한 가중치값.\n",
    "  \n",
    "![Attention Weight](figures/attn_attention_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4349ed00-d090-49c3-bcf5-9792e28efdf8",
   "metadata": {},
   "source": [
    "### Attention Value\n",
    "- Decoder에서 현재 timestep의 단어를 추출할 때 사용할 Context Vector. \n",
    "    - Encoder의 output 들에 Attention Weight를 곱한다.\n",
    "    - Attention Value는 Decoder에서 단어를 생성할 때 encoder output의 어떤 단어에 더 집중하고 덜 집중할지를 가지는 값이다.\n",
    "\n",
    "![attention value](figures/attn_attention_value.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29166d33-991d-406a-85d1-ce6575f78146",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "- Decoder의 embedding vector와 Attention Value 를 합쳐 RNN(GRU)의 입력을 만든다.\n",
    "    - **단어를 생성하기 위해 이전 timestep에서 추론한 단어(현재 timestep의 input)** 와 **Encoder output에 attention이 적용된 값** 이 둘을 합쳐 입력한다.\n",
    "    - 이 값을 Linear Layer함수+ReLU를 이용해 RNN input_size에 맞춰 준다. (어떻게 input_size에 맞출지도 학습시키기 위해 Linear Layer이용)\n",
    "\n",
    "![rnn](figures/att_attention_combine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e7805-2809-48d8-a9b7-547c3f571c68",
   "metadata": {},
   "source": [
    "### 단어 예측(생성)\n",
    "- RNN에서 찾은 Feature를 총 단어개수의 units을 출력하는 Linear에 입력해 **다음 단어를 추론한다.**\n",
    "- 추론한 단어는 다음 timestep의 입력($X_t$)으로 RNN의 hidden은 다음 timestep 의 hidden state ($h_{t-1}$) 로 입력된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9653c5e5-bf2f-47ac-aa2e-63363a131e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, dropout_p, max_length):\n",
    "        # num_vocabs: 총 어휘수\n",
    "        super().__init__()\n",
    "        self.num_vocabs = num_vocabs\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "\n",
    "        # attention weight를 계산하는 Linear \n",
    "        # 이전 단어의 hidden state(prev_hidden)와 현재 단어의 embedding vector에 \n",
    "        #     가중합을 계산해서 attention weight를 계산. \n",
    "        ##  in_features: hidden_size+embedding_dim\n",
    "        ##  out_features: Encoder의 hidden_state의 개수(max_length)\n",
    "        self.attn = nn.Linear(hidden_size+embedding_dim, max_length)\n",
    "\n",
    "        # 가정: hidden_size-200, max_length(토큰수)-20\n",
    "        # attention value = attention-weight @ encoder의 hidden state들(out)\n",
    "        # shape: 1 x 20 @ 20 x 200 = 1 x 200\n",
    "\n",
    "        # 현재 단어 embedding vector + attention value를 입력받아 가중합을 계산해서 \n",
    "        ##   GRU(RNN)에 입력할 입력값을 계산.\n",
    "        ##  in_features: embedding_dim + encoder의 hidden_size\n",
    "        self.attn_combine = nn.Linear(embedding_dim+hidden_size, hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        # GRU\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "        # 분류기 \n",
    "        self.classifier = nn.Linear(hidden_size, num_vocabs)\n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Parameter\n",
    "            x: 현재 timestep의 입력 토큰(단어) id\n",
    "            hidden: 이전 timestep 처리결과 hidden state\n",
    "            encoder_outputs: Encoder output들. \n",
    "        Return\n",
    "            tupe: (output, hidden, attention_weight)\n",
    "                output: 단어별 다음 단어일 확률.  shape: [vocab_size]\n",
    "                hidden: hidden_state. shape: [1, 1, hidden_size]\n",
    "                atttention_weight: Encoder output 중 어느 단어에 집중해야하는 지 가중치값. shape: [1, max_length]\n",
    "        \n",
    "        현재 timestep 입력과 이전 timestep 처리결과를 기준으로 encoder_output와 계산해서  encoder_output에서 집중(attention)해야할 attention value를 계산한다.\n",
    "        attention value와 현재 timestep 입력을 기준으로 단어를 추론(생성) 한다.\n",
    "        \"\"\"\n",
    "        embedding = self.embedding(x).unsqueeze(0) # [1:batch] -> [1:batch, 1:seq_len]\n",
    "        embedding = self.dropout(embedding)\n",
    "\n",
    "        # attention weight 계산 \n",
    "        # 입력: embedding vector + prev_hidden-state (합치기)\n",
    "        # pytorch에서 tensor를 합치는 함수: torch.concat([합칠 대상, ..], dim=방향축)\n",
    "        attn_in = torch.concat((embedding[0], hidden[0]), dim=1)\n",
    "        # attn_in shape: [1:batch, embedding_dim+hidden_size]\n",
    "        attn_score = self.attn(attn_in) #logit\n",
    "        # attn_score shape:  1 x embedding_dim+hidden_size @ embedding_dim+hidden_size x max_length\n",
    "        ####    1 x max_length \n",
    "        attn_weight = nn.Softmax(dim=-1)(attn_score)\n",
    "\n",
    "        # attention value계산(attn_applied) - attn_weight @ encoder_hiddenstate들\n",
    "        ## 1 x max_length  @ max_length x hidden_size\n",
    "        # torch.bmm() - batch-wise matrix multiplication(배치단위 행렬곱)\n",
    "        ## 3차원 배열을 받아서 1, 2축 기준으로 행렬곱 계산.\n",
    "        ### (5, 2, 3) @ (5, 3, 5) => 2 x 3 @ 3 x 5  5개를 행렬곱 => (5, 2, 5)\n",
    "        attn_value = torch.bmm(\n",
    "            attn_weight.unsqueeze(0),     # (1, 1, max_length)\n",
    "            encoder_outputs.unsqueeze(0), # (1, max_length, hidden_size)\n",
    "        ) \n",
    "        # attn_value 결과: (1:batch, 1:seq_len , hidden_size)\n",
    "\n",
    "        # attn_combine: gru의 input값을 생성\n",
    "        ##  attn_value + embedding(concat) => Linear => ReLU\n",
    "        attn_combine_in = torch.concat([\n",
    "            attn_value[0], embedding[0]\n",
    "        ], dim=1)\n",
    "        gru_in = self.attn_combine(attn_combine_in) # 출력 (1, hidden_size)\n",
    "        gru_in = gru_in.unsqueeze(0)\n",
    "        gru_in = nn.ReLU()(gru_in)\n",
    "\n",
    "        # gru에 입력해서 다음 단어를 찾기 위한 hidden state(feature)를 계산.\n",
    "        out, hidden_state = self.gru(gru_in, hidden)\n",
    "\n",
    "        # classification 에 out을 입력해서 다음 단어를 예측\n",
    "        last_out = self.classifier(out[0])\n",
    "        # last_out shape: [1, num_vocabs]\n",
    "\n",
    "        return last_out[0], hidden_state, attn_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2fda460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder/Decoder를 dummy_data로 확인\n",
    "dummy_encoder = Encoder(\n",
    "    num_vocabs=tokenizer.get_vocab_size(),\n",
    "    hidden_size=256,\n",
    "    embedding_dim=200,\n",
    "    num_layers=1\n",
    ")\n",
    "dummy_encoder = dummy_encoder.to(device)\n",
    "\n",
    "dummy_decoder = AttentionDecoder(\n",
    "    num_vocabs=tokenizer.get_vocab_size(),\n",
    "    hidden_size=256,\n",
    "    embedding_dim=200,\n",
    "    dropout_p=0.3,\n",
    "    max_length=20\n",
    ")\n",
    "dummy_decoder = dummy_decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79d0c49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 256]), torch.Size([1, 1, 256]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = dataset[0]  # 첫번째 (Q, A)\n",
    "x, y = x.to(device), y.to(device)\n",
    "\n",
    "\n",
    "# 첫번째 질문의 첫번째 토큰을 입력. x[0]\n",
    "# hidden state (이전 처리 결과가 없으므로 0)\n",
    "encoder_out, encoder_hidden = dummy_encoder(x[0], dummy_encoder.init_hidden(device))\n",
    "encoder_out.shape, encoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "881ade3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 답변의 첫번째 토큰을 입력. y[0]\n",
    "encoder_outputs = torch.randn(20, 256, device=device) # 20: seq_len, 256: hidden_size\n",
    "next_token, hidden_state, attn_weight = dummy_decoder(y[0], encoder_out, encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1469ee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7042])\n",
      "tensor(6370) 길게\n",
      "torch.Size([1, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "print(next_token.shape)\n",
    "print(next_token.argmax(-1), tokenizer.id_to_token(next_token.argmax(-1).item()))\n",
    "print(hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea71ee92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0201, 0.1395, 0.0386, 0.0760, 0.0238, 0.0669, 0.0497, 0.0437, 0.0317,\n",
       "         0.0189, 0.0812, 0.0387, 0.1263, 0.0254, 0.0210, 0.0652, 0.0214, 0.0409,\n",
       "         0.0202, 0.0507]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(attn_weight.shape)\n",
    "attn_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab7c43d-3691-4723-8051-7bc0a8a4a37e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75080eb6-da0e-4c86-998e-2dd8405e5a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "SOS_TOKEN = dataset.tokenizer.token_to_id(\"[SOS]\")\n",
    "EOS_TOKEN = dataset.tokenizer.token_to_id(\"[EOS]\")\n",
    "print(SOS_TOKEN, EOS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53184448-56e5-4c3c-8d6c-3f8f93d45076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한개 question-answer 쌍을 학습\n",
    "def train(\n",
    "        input_tensor,  # 질문 1개\n",
    "        target_tensor, # 답변 1개개\n",
    "        encoder,       # Encoder\n",
    "        decoder,       # AttentionDecoder\n",
    "        encoder_optimizer, # encoder optimizer\n",
    "        decoder_optimizer, # decoder optimizer\n",
    "        loss_fn,           # loss 함수\n",
    "        device, \n",
    "        max_length,\n",
    "        teacher_forcing_ratio=0.9):\n",
    "\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    target_tensor = target_tensor.to(target_tensor)\n",
    "    loss = 0.0 # loss값 저장할 변수.\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Encoder 처리\n",
    "    encoder_hidden = encoder.init_hidden(device) # 첫번째 timestep에 입력할 hidden state(0)\n",
    "\n",
    "    # 질문/답변의 length(토큰수)를 조회\n",
    "    input_length = input_tensor.shape[0]\n",
    "    output_length = target_tensor.shape[0]\n",
    "\n",
    "    # encoder hidden state들을 저장할 tensor를 정의\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    ##### 질문 문장의 (토큰별) hidden state들을 계산 -> encoder_outputs에 저장.\n",
    "    for e_idx in range(input_length):\n",
    "        encoder_out, encoder_hidden = encoder(input_tensor[e_idx], encoder_hidden)\n",
    "        encoder_outputs[e_idx] = encoder_out\n",
    "\n",
    "    ##### Decoder 처리(답변생성)\n",
    "    # 첫번째 timestep 의 토큰: [SOS]\n",
    "    decoder_input = torch.tensor([SOS_TOKEN], device=device) # decode_input: 현재 timestep의 input\n",
    "    decoder_hidden = encoder_hidden # 첫번째 hidden: context vector(encoder의 마지막 hidden state)\n",
    "    # teacher_forcing 여부\n",
    "    teacher_forcing = True if teacher_forcing_ratio > random.random() else False\n",
    "\n",
    "    # Decoder작업 -> 다음 단어 예측(생성)\n",
    "    for d_idx in range(output_length): # 정답 토큰개수만큼 생성.\n",
    "        # decoder_out: 다음 단어 예측값, decoder_hidden: GRU의 hidden state\n",
    "        decoder_out, decoder_hidden, attn_weight = decoder(decoder_input, \n",
    "                                                           decoder_hidden, \n",
    "                                                           encoder_outputs)\n",
    "        # loss 계산\n",
    "        loss += loss_fn(decoder_out.unsqueeze(0), target_tensor[d_idx])\n",
    "\n",
    "        # 다음 timestep에 넣을 input 토큰을 생성 -> decoder_input\n",
    "        ## 정답 토큰 if teacher_forcing else Decoder예측단어\n",
    "        if teacher_forcing:\n",
    "            decoder_input = target_tensor[d_idx]\n",
    "        else:\n",
    "            output_token = decoder_out.argmax(dim=-1).unsqueeze(0)\n",
    "            decoder_input = output_token.detach() # Tensor.detach(): gradient 계산그래프에서 제외.\n",
    "\n",
    "        teacher_forcing_ratio *= 0.99\n",
    "        \n",
    "        if decoder_input == EOS_TOKEN: # 생성한 단어가 [EOS](문장의 끝)이면 생성 종료\n",
    "            break\n",
    "    \n",
    "    # 순전파가 완료 (질문 -> 답변) ==> 역전파 gradient 계산-파라미터 업데이트\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    # loss 평균 반환\n",
    "    return loss.item() / output_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6824a9fb-1592-44f4-8a74-6c5098bd5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iterations(\n",
    "        encoder, decoder, n_iters, \n",
    "        dataset, device, log_interval=1000, learning_rate=0.001):\n",
    "        # n_iters: 학습시킬 데이터(Q-A쌍)의 개수\n",
    "        # log_interval: train loss를 몇개 데이터 학습마다 출력할지.\n",
    "\n",
    "        # encoder/decoder 모델을 train 모드로 변환\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        print_loss = 0.0 # 출력할 loss값 (출력하면 0으로 초기화)\n",
    "\n",
    "        # 옵티마이저 생성\n",
    "        encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "        decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "        #loss 함수\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # 학습 시킬 데이터를 sampling\n",
    "        data_length = len(dataset)\n",
    "        train_data = [dataset[random.randint(0, data_length-1)]   for i in range(n_iters)]\n",
    "\n",
    "        # 학습-train\n",
    "        s = time.time()\n",
    "        for idx in range(n_iters):\n",
    "            input_tensor, target_tensor = train_data[idx]\n",
    "            loss = train(input_tensor, target_tensor, encoder, decoder,\n",
    "                         encoder_optimizer, decoder_optimizer, loss_fn,\n",
    "                         device, max_length=MAX_LENGTH\n",
    "                        )\n",
    "            print_loss += loss \n",
    "            if (idx+1) % log_interval == 0: \n",
    "                print(f\"{idx+1}개 QA상 학습: loss - {print_loss/log_interval:.5f}\")\n",
    "                print_loss = 0.0\n",
    "\n",
    "        e = time.time()\n",
    "        print(f'학습에 걸린 시간: {e-s}초')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "619381e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_VOCABS = tokenizer.get_vocab_size()\n",
    "HIDDEN_SIZE = 200\n",
    "EMBEDDING_DIM = 256\n",
    "DROPOUT_P = 0.2\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "encoder = Encoder(NUM_VOCABS, HIDDEN_SIZE, EMBEDDING_DIM, 1)\n",
    "decoder = AttentionDecoder(NUM_VOCABS, HIDDEN_SIZE, EMBEDDING_DIM, DROPOUT_P, MAX_LENGTH)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b92dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500개 QA상 학습: loss - 5.93458\n",
      "1000개 QA상 학습: loss - 5.79095\n",
      "학습에 걸린 시간: 66.70495510101318초\n"
     ]
    }
   ],
   "source": [
    "n_iters = 100_000\n",
    "log_interval = 500\n",
    "train_iterations(encoder, decoder, n_iters, dataset, device, log_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564cd1f5-0f7f-4271-9143-87b978d8e376",
   "metadata": {},
   "source": [
    "## Model 생성, 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f34a5a9-a869-4764-ad59-917c3cc027cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 하이퍼파리터들 정의\n",
    "# VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "# HIDDEN_SIZE = 200\n",
    "# EMBEDDING_DIM = 256\n",
    "# DROPOUT_P = 0.2\n",
    "\n",
    "# # 인코더\n",
    "# encoder = Encoder(VOCAB_SIZE, hidden_size=HIDDEN_SIZE, \n",
    "#                   embedding_dim=EMBEDDING_DIM, num_layers=1)\n",
    "\n",
    "# decoder = AttentionDecoder(VOCAB_SIZE, hidden_size=HIDDEN_SIZE, embedding_dim=EMBEDDING_DIM,\n",
    "#                            dropout_p=DROPOUT_P, max_length=MAX_LENGTH)\n",
    "# encoder = encoder.to(device)\n",
    "# decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f175419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_iters = 100000    \n",
    "# log_interval = 1000\n",
    "\n",
    "# train_iterations(encoder, decoder, n_iters=n_iters, dataset=dataset, device=device, log_interval=log_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f416fa1-0a38-4a60-9c02-41261fab6cb0",
   "metadata": {},
   "source": [
    "## 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b436b71-3cf4-45e2-9418-00555e55d82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저, 인코더, 디코드\n",
    "root_path = \"saved_model/chatbot_attn\"\n",
    "os.makedirs(root_path,  exist_ok=True)\n",
    "\n",
    "tokenizer_path = os.path.join(root_path, \"tokenizer.json\")\n",
    "encoder_path = os.path.join(root_path, \"encoder_model.pt\")\n",
    "decoder_path = os.path.join(root_path, \"decoder_model.pt\")\n",
    "\n",
    "tokenizer.save(tokenizer_path)\n",
    "torch.save(encoder, encoder_path)\n",
    "torch.save(decoder, decoder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae49d34-7308-493d-95a4-eef87c361d33",
   "metadata": {},
   "source": [
    "## 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d5e95558-b4ed-4676-a23a-796cf0cfa27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"saved_model/chatbot_attn\"\n",
    "os.makedirs(root_path,  exist_ok=True)\n",
    "\n",
    "tokenizer_path = os.path.join(root_path, \"tokenizer.json\")\n",
    "encoder_path = os.path.join(root_path, \"encoder_model.pt\")\n",
    "decoder_path = os.path.join(root_path, \"decoder_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8038aca1-7a9e-464a-8558-957a498248a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델 Load\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "encoder = torch.load(encoder_path, weights_only=False, map_location=device)\n",
    "decoder = torch.load(decoder_path, weights_only=False, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "52b25ff9-0854-4583-b4f2-0dbc84e63e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = tokenizer.token_to_id('[SOS]')\n",
    "EOS_TOKEN = tokenizer.token_to_id('[EOS]')\n",
    "def evaluate(encoder, decoder, input_tensor, dataset, device, max_length):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        input_length = input_tensor.shape[0]  # 질문 문장 토큰 개수.\n",
    "        encoder_hidden = encoder.init_hidden(device) # 첫 timestep에 넣어줄 hidden state\n",
    "\n",
    "        # encoder의 hidden state들을 모을 텐서 생성\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        # encoder 실행\n",
    "        for e_index in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[e_index], encoder_hidden)\n",
    "            encoder_outputs[e_index] = encoder_output[0, 0]\n",
    "\n",
    "        # decoder 실행\n",
    "        decoder_input = torch.tensor([SOS_TOKEN], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        # 결과를 저장할 리스트\n",
    "        decoded_words = []  # 디코더가 추론한 단어(토큰)들을 저장.\n",
    "        decoder_attn_weights = [] # 각 단어들을 추론할 때 계산된 attention weight값들을 저장.\n",
    "\n",
    "        for d_index in range(max_length):\n",
    "            decoder_output, decoder_hidden, attn_weight = decoder(decoder_input, \n",
    "                                                                  decoder_hidden, \n",
    "                                                                  encoder_outputs)\n",
    "            decoder_attn_weights.append(attn_weight.data)\n",
    "\n",
    "            topv, topi  = decoder_output.data.topk(1)\n",
    "\n",
    "            if topi.item() == EOS_TOKEN:\n",
    "                decoded_words.append('[EOS]')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(dataset.tokenizer.id_to_token(topi.item()))\n",
    "            decoder_input = topi.detach()\n",
    "\n",
    "    return decoded_words, decoder_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6e33d28a-5414-4d8e-8919-730109adfd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_special_tokens(decoded_string):\n",
    "    \"\"\"\n",
    "    Subword 처리\n",
    "    subword는 단어의 시작으로 쓰인 것과 중간 부분(연결)에 사용된 두가지 subword가 있다.  연결 subword는 `#`과 같은 특수문자로 시작 한다.\n",
    "    tokenizer.decode() 결과 문자열은 subword의 특수문자('##')을 처리하지 않는다. 이것을 처리하는 함수\n",
    "    ex) \"이 기회 ##는 내 ##꺼 #야\" ==> \"이 기회는 내꺼야\"\n",
    "    \n",
    "    Parameter\n",
    "        decoded_string: str - Tokenizer가 decode한 중간 subword의 특수문자 처리가 안된 문자열. \n",
    "    Return\n",
    "        str: subword 특수문자 처리한 문자열\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = decoded_string.split()\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.startswith(\"##\"):\n",
    "            if new_tokens: # len(new_tokens) != 0 원소가 하나라도 있으면\n",
    "                # 토큰에서 ##을 제거하고 리스트의 마지막 원소(문자열) 뒤에 붙인다.\n",
    "                new_tokens[-1] += token[2:]\n",
    "            else: # new_tokens가 빈 리스트. 현재 token이 첫번째 단어. ##을 지우고 append\n",
    "                new_tokens.append(token[2:])\n",
    "        else: # 단어의 시작인 토큰. (##이 없는 토큰) -> list에 추가.\n",
    "            new_tokens.append(token)\n",
    "        \n",
    "    return \" \".join(new_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9451766-9127-47ed-b844-db9c71d1b47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomly(encoder, decoder, dataset, device, n=10):\n",
    "    # n개 확인.\n",
    "    for i in range(n):\n",
    "        idx = random.randint(0, len(dataset))\n",
    "        x, y = dataset[idx]\n",
    "        q = dataset.tokenizer.decode(x.flatten().tolist())\n",
    "        a = dataset.tokenizer.decode(y.flatten().tolist())\n",
    "        print(\"질문(정답):\", handle_special_tokens(q))\n",
    "        print(\"답변(정답):\", handle_special_tokens(a))\n",
    "\n",
    "        # 추론\n",
    "        output_words, atten_weights = evaluate(encoder, decoder,\n",
    "                                              x.to(device), \n",
    "                                              dataset, device, MAX_LENGTH)\n",
    "        # output_words: [단어, 단어, 단어, ....]\n",
    "        output_sentence = ' '.join(output_words[:-1]) # [EOS]는 제거\n",
    "        print(\"답변(예측):\", handle_special_tokens(output_sentence))\n",
    "        print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "41a6557b-7b69-4e2f-93af-9eb38621b158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문(정답): 이 세상에서 내가 제일 불행해 .\n",
      "답변(정답): 그럴리가요 .\n",
      "답변(예측): 다시 무슨 .\n",
      "==================================================\n",
      "질문(정답): 치매가 더 심해지는 것 같아 무서워\n",
      "답변(정답): 더 많이 연락하고 관심을 가져주세요 .\n",
      "답변(예측): 그런 생각 .\n",
      "==================================================\n",
      "질문(정답): 좋아하는 사람 앞에선 다 똑같은가봐요 .\n",
      "답변(정답): 누구나 좋아하는 사람 앞에서 그렇대요 .\n",
      "답변(예측): 이제 더 해보세요 .\n",
      "==================================================\n",
      "질문(정답): 짝남이 자꾸 꿈에 나오는데 꿈에서도 용기 없어서 피하고 그래 .\n",
      "답변(정답): 은연 중에 지금의 심리 상태가 반영되었나봐요 .\n",
      "답변(예측): 그런 무슨 .\n",
      "==================================================\n",
      "질문(정답): 남동생한테 자꾸 화내게 되네\n",
      "답변(정답): 화를 참는 연습을 해보세요 .\n",
      "답변(예측): 그런 .\n",
      "==================================================\n",
      "질문(정답): 힘든 밤입니다\n",
      "답변(정답): 힘내세요 .\n",
      "답변(예측): 이제 더 .\n",
      "==================================================\n",
      "질문(정답): 잊기가 힘듭니다 .\n",
      "답변(정답): 충분히 힘들만하다고 생각해요 .\n",
      "답변(예측): 무슨 .\n",
      "==================================================\n",
      "질문(정답): 오늘 기분은 설명이 안돼\n",
      "답변(정답): 좋았으면 좋겠네요 .\n",
      "답변(예측): 그런 .\n",
      "==================================================\n",
      "질문(정답): 여자친구 만나고왔어 ~\n",
      "답변(정답): 좋은 만남이었길 바랍니다 .\n",
      "답변(예측): 그런 .\n",
      "==================================================\n",
      "질문(정답): 이별 통보 받은지 7일째\n",
      "답변(정답): 자신의 마음에 귀를 기울여보세요 .\n",
      "답변(예측): 그런 .\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly(encoder, decoder, dataset, device, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e2587-ed54-4b58-b8d3-9c3b1dd4b60e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
