{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e93c9500-6e1f-445a-a5e1-95e687853ae8",
   "metadata": {},
   "source": [
    "# Pytorch의 nn.Embedding\n",
    "- Pytorch의 Embedding Layer는 word2vec과 마찬가지로 word embedding vector를 찾는 **Lookup Table**이다.\n",
    "    - 단어의 **정수의 고유 index**가 입력으로 들어오면 Embedding Layer의 **그 index의 Vector**를 출력한다.\n",
    "    - 모델이 학습되는 동안 모델이 풀려는 문제에 맞는 값으로 Embedding Layer의 vector들이 업데이트 된다.\n",
    "    - Word2Vec의 embedding vector 학습을 nn.Embedding은 자신이 포함된 모델을 학습 하는 과정에서 한다고 생각하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d936d8f7-a203-4452-978f-e2da81b7dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "embed = nn.Embedding(\n",
    "    num_embeddings=20000,    # vocab size= 단어 사전의 단어 수 -> 총 몇 개 단어에 대한 embedding vector를 만들지\n",
    "    embedding_dim=200,      # embedding vector의 차원 수 -> 개별 단어를 몇 개의 숫자(feature)로 표현할지\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2d166e4e-4ced-4bc8-a188-12358670ad0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20000, 200])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2a4af570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding layer의 입력 - 문서를 구성하는 토큰들의 ID(정수여야 함!-int)를 1차원으로 묶어서 전달."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1a336002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 어휘사전 어휘 수: 10개\n",
    "# 임베딩 벡터 차원 수: 3차원\n",
    "e_layer = nn.Embedding(\n",
    "    num_embeddings = 10,\n",
    "    embedding_dim = 3\n",
    ")\n",
    "# 10 x 3 weight 행렬을 생성 -> weight 행렬이 전체 어휘들의 embidding vector들 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "62087f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4045, -0.1565,  1.0083],\n",
       "        [-0.6456, -0.3483, -0.5890],\n",
       "        [ 1.4992,  0.0121, -0.5478],\n",
       "        [ 0.1059, -0.8331,  0.3483],\n",
       "        [ 0.0957, -0.8184,  1.2346],\n",
       "        [ 0.1539,  1.5637, -0.3046],\n",
       "        [-0.6230, -0.0877,  1.2724],\n",
       "        [-1.2257, -1.6301,  1.4811],\n",
       "        [ 0.1024, -1.4615, -0.3925],\n",
       "        [-0.5138, -1.1440, -1.5887]], requires_grad=True)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4dc12d",
   "metadata": {},
   "source": [
    "`requires_grad=True`: 학습할 때는 \"가중치\"이지만, 학습이 끝나고 나면 \"임베딩 벡터\"가 된다~!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fab05cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4045, -0.1565,  1.0083],\n",
       "        [ 0.0957, -0.8184,  1.2346],\n",
       "        [ 1.4992,  0.0121, -0.5478]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"오늘 날씨 좋다\" # [\"오늘\", \"날씨\", \"좋다\"]\n",
    "# token = tokenizer.encode(sent).ids # 토큰화 # [\"오늘\", \"날씨\", \"좋다\"]\n",
    "token = torch.tensor([0, 4, 2], dtype=torch.int64) # [\"오늘\":0, \"날씨\":4, \"좋다\":2]\n",
    "e_layer(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd9882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca59e731-381d-4a79-83c4-1fc20ba006e1",
   "metadata": {},
   "source": [
    "# 네이버 영화 댓글 감성분석(Sentiment Analysis)\n",
    "\n",
    "## 감성분석(Sentiment Analysis) 이란\n",
    "입력된 텍스트가 **긍적적인 글**인지 **부정적인**인지 또는 **중립적인** 글인지 분석하는 것을 감성(감정) 분석이라고 한다.   \n",
    "이를 통해 기업이 고객이 자신들의 기업 또는 제품에 대해 어떤 의견을 가지고 있는지 분석한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7034ada-08b9-4163-b18d-ce429aef275b",
   "metadata": {},
   "source": [
    "# Dataset, DataLoader 생성\n",
    "\n",
    "## Korpora에서 Naver 영화 댓글 dataset 가져오기\n",
    "- https://ko-nlp.github.io/Korpora/ko-docs/corpuslist/nsmc.html\n",
    "- http://github.com/e9t/nsmc/\n",
    "    - input: 영화댓글\n",
    "    - output: 0(부정적댓글), 1(긍정적댓글)\n",
    "### API\n",
    "- **corpus 가져오기**\n",
    "    - `Korpora.load('nsmc')`\n",
    "- **text/label 조회**\n",
    "    - `corpus.get_all_texts()` : 전체 corpus의 text들을 tuple로 반환\n",
    "    - `corpus.get_all_labels()`: 전체 corpus의 label들을 list로 반환\n",
    "- **train/test set 나눠서 조회**\n",
    "    - `corpus.train`\n",
    "    - `corpus.test`\n",
    "    - `LabeledSentenceKorpusData` 객체에 text와 label들을 담아서 제공.\n",
    "        - `LabeledSentenceKorpusData.texts`: text들 tuple로 반환.\n",
    "        - `LabeledSentenceKorpusData.labels`: label들 list로 반환."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0e2ea3-6123-4ebd-8e98-27b1db6406ed",
   "metadata": {},
   "source": [
    "## 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aebdb0ac-eaaa-47aa-a0de-11d49e8a427b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\안수민\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\안수민\\Korpora\\nsmc\\ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from Korpora import Korpora\n",
    "\n",
    "corpus = Korpora.load(\"nsmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c08b08d5-bfcd-4430-b4c0-44b19bbf4022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_inputs = corpus.get_all_texts()     # inputs: 댓글들 전체\n",
    "all_labels = corpus.get_all_labels()    # outputs: labels 전체 - 0: 부정, 1: 긍정\n",
    "all_inputs[:5]\n",
    "all_labels[:5]\n",
    "\n",
    "len(all_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "232a23c0-06c5-49b7-b601-1ede1198b4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NSMC.train: size=150000\n",
       "  - NSMC.train.texts : list[str]\n",
       "  - NSMC.train.labels : list[int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4bd276d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NSMC.test: size=50000\n",
       "  - NSMC.test.texts : list[str]\n",
       "  - NSMC.test.labels : list[int]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91d9957e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('굳 ㅋ',\n",
       " 'GDNTOPCLASSINTHECLUB',\n",
       " '뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아',\n",
       " '지루하지는 않은데 완전 막장임... 돈주고 보기에는....',\n",
       " '3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.test.texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1adf81de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.test.labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a30059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ec5476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2e7e54a-548d-4bf3-81aa-357ab249f41a",
   "metadata": {},
   "source": [
    "## 토큰화\n",
    "1. 형태소 단위 token화(분절)를 먼저 한다.\n",
    "    - konlpy로 token화 한 뒤 다시 한 문장으로 만든다.\n",
    "2. 1에서 처리한 corpus를 BPE 로 token화\n",
    "\n",
    "\n",
    "형태소 분석 후 BPE를 쓰는 이유? 조사 분리 등 언어학적 정보와 서브워드 일반화를 모두 활용하려고.   \n",
    "꼭 필요한가? ❌ BPE 하나만 사용해도 충분하다. \n",
    "   \n",
    "### 전처리 함수\n",
    "\n",
    "#### 형태소 단위 분절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0f162bdf-fac9-4468-a264-c656e4b3164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import string\n",
    "import re\n",
    "\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3526a561",
   "metadata": {},
   "source": [
    "전처리 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d0bbb39b-9f49-4d29-a969-4839c01f430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    1. 영문 -> 소문자로 변환\n",
    "    2. 구두점 제거\n",
    "    3. 형태소 기반 토큰화: okt.morphs()\n",
    "    4. 형태소로 토큰화 한 뒤 다시 하나의 문자열로 묶어서 반환.\n",
    "    \"\"\"\n",
    "    text = text.lower()# 구두점 제거 (stop word(불용어))\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \" \", text)\n",
    "\n",
    "    # 정규화\n",
    "    tokens = okt.morphs(text, stem=True) # stem : 원형복원.\n",
    "    return ' '.join(tokens)     # [\"단어\",\"단어\",..] -> str \"단어 단어 단어\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6e658962-2fd6-4b1d-b4ef-a38de3ecc847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "재미없다 진심 1 이훨 나 캐스팅 두 못 한 듯\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'재미없다 진심 1 이훨 나 캐스팅 두 못 하다 듯'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(all_inputs[101])\n",
    "text_preprocessing(all_inputs[101])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeb9c35",
   "metadata": {},
   "source": [
    "전처리 함수에 문장들을 넣는다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "18a25c93-7e26-4512-a0c0-56218fcda100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 걸린 시간(초): 771.5486371517181\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "# train set 전처리\n",
    "train_texts = corpus.train.texts\n",
    "train_inputs = [text_preprocessing(txt) for txt in train_texts]\n",
    "train_labels = corpus.train.labels\n",
    "\n",
    "# test set 전처리\n",
    "test_texts = corpus.test.texts\n",
    "test_inputs = [text_preprocessing(txt) for txt in test_texts]\n",
    "test_labels = corpus.test.labels\n",
    "e = time.time()\n",
    "\n",
    "print(\"전처리 걸린 시간(초):\", e-s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0460f42c",
   "metadata": {},
   "source": [
    "전처리한 trainset과 testset을 피클로 저장한다.   \n",
    "딕셔너리 형태로 저장한다. 이때 input은 train_inputs=전처리한 문장 리스트이고, outputs는 train_labels=문장들의 라벨들(0, 1)이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c7cad7-23d9-4212-b07a-0e5da2ff73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import os\n",
    "\n",
    "os.makedirs(\"datasets/nsmc\", exist_ok=True)\n",
    "with open(\"datasets/nsmc/preprocessing_trainset.pkl\", \"wb\") as fw:\n",
    "    pickle.dump({\"input\":train_inputs, \"outputs\":train_labels}, fw)\n",
    "\n",
    "with open(\"datasets/nsmc/preprocessing_testset.pkl\", \"wb\") as fw:\n",
    "    pickle.dump({\"input\":test_inputs, \"outputs\":test_labels}, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae4ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = train_inputs + test_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f3720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 50000, 200000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_inputs), len(test_inputs), len(all_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1165a2",
   "metadata": {},
   "source": [
    "pickle로 저장한 전처리한 데이터셋을 읽어온다.   \n",
    "```python\n",
    "with open(\"피클 파일 위치\", \"형식\") as fr:\n",
    "    dict = pickle.load(fr)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fbb8886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle로 저장한 전처리한 데이터셋 읽어오기\n",
    "import pickle\n",
    "\n",
    "with open(\"datasets/nsmc/preprocessing_trainset.pkl\", \"rb\") as fr:\n",
    "    train_dic = pickle.load(fr)\n",
    "\n",
    "with open(\"datasets/nsmc/preprocessing_testset.pkl\", \"rb\") as fr:\n",
    "    test_dic = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6f6af774",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = train_dic['input']\n",
    "train_labels = train_dic['outputs']\n",
    "\n",
    "test_inputs = test_dic['input']\n",
    "test_labels = test_dic['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d9ce3ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = train_inputs + test_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e6f023f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 50000, 200000)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_inputs), len(test_inputs), len(all_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2638b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743e402f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e519a68-d3a0-4481-bcf7-b121d8ba813f",
   "metadata": {},
   "source": [
    "### 토큰화\n",
    "- Subword 방식 토큰화 적용\n",
    "- Byte Pair Encoding 방식으로 huggingface tokenizer 사용\n",
    "    - BPE: 토큰을 글자 단위로 나눈뒤 가장 자주 등장하는 글자 쌍(byte paire)를 찾아 합친뒤 어휘사전에 추가한다.\n",
    "    - https://huggingface.co/docs/tokenizers/quicktour\n",
    "    - `pip install tokenizers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e998d38b-e762-4fd5-b37f-8f8a2b6f5849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, Unigram, WordPiece\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "vocab_size = 30_000 # 어휘사전 최대 단어 수\n",
    "min_frequency = 5 # 사전에 추가할 최소 빈도수\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    BPE(unk_token=\"[UNK]\"), \n",
    ")\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size = vocab_size,\n",
    "    min_frequency=min_frequency,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\"],\n",
    "    continuing_subword_prefix=\"##\"\n",
    "    # 단어의 중간에 나오는 subword일 경우 앞에 ##을 붙인다. \n",
    "    # \"시작하는\" -> \"시작\", \"하는\" => \"시작\", \"##하는\"\n",
    ")\n",
    "tokenizer.train_from_iterator(all_inputs, trainer=trainer) # vocab 생성 == tokenizer 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e5a5866a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26739"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 총 vocab size:\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c201f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "os.makedirs(\"saved_models/nsmc\", exist_ok=True)\n",
    "tokenizer.save(\"saved_models/nsmc/tokenizer_bpe.json\")\n",
    "\n",
    "# load_tokenizer = Tokenizer.from_file(\"saved_models/nsmc/tokenizer_bpe.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b09aa2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정말 최고 의 명작 성인 이 되다 보다 이집트 의 왕자 는 또 다른 감동 그 자체 네 요\n",
      "[5420, 5438, 2203, 5530, 6570, 2206, 5425, 5410, 14758, 2203, 9123, 923, 1152, 5617, 5450, 651, 5641, 856, 2128]\n",
      "['정말', '최고', '의', '명작', '성인', '이', '되다', '보다', '이집트', '의', '왕자', '는', '또', '다른', '감동', '그', '자체', '네', '요']\n"
     ]
    }
   ],
   "source": [
    "idx = 1000\n",
    "print(all_inputs[1000])\n",
    "tokens = tokenizer.encode(all_inputs[idx])\n",
    "print(tokens.ids)\n",
    "print(tokens.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1b75c501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'정말 최고 의 명작 성인 이 되다 보다 이집트 의 왕자 는 또 다른 감동 그 자체 네 요'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d858f560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a0ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09f5c31d-633c-4a31-8f66-dff2ecf8e86a",
   "metadata": {},
   "source": [
    "## Dataset, DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09415389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1986, 5881, 5426, 5667, 6087], 0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dset[0]\n",
    "tokenizer.encode(train_inputs[0]).ids, train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c847e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch 사용자 정의 Custom Dataset 정의\n",
    "## 1. 데이터셋 상속\n",
    "## 2. __len__(self): 총 데이터의 개수 반환\n",
    "## 3. __getitem__(self, index): index의 x, y 반환\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be87e5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27280e-dfb7-4947-9192-777e6984286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NSMCDataset(Dataset):\n",
    "    def __init__(self, texts:list, labels, max_length, tokenizer):\n",
    "        \"\"\"\n",
    "        texts: list - 댓글 목록. 리스트에 댓글들을 담아서 받는다. [\"댓글\", \"댓글\", ...]\n",
    "        labels: list - 댓글 Label(긍정/부정정) 목록. \n",
    "        max_length: 개별 댓글의 최대 token 개수. 모든 댓글의 토큰수를 max_length에 맞춘다.\n",
    "        tokenizer: Tokenizer\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = labels\n",
    "        # self.texts: 입력 댓글 - token id로 변환된 댓글(문서). 글자수는 max_length에 맞춤\n",
    "        #                        토큰이 max_length보다 적으면 [PAD] 추가, max_length보다 많으면 잘라낸다.\n",
    "        self.texts = [self.__pad_token_sequences(tokenizer.encode(txt).ids) for txt in texts]\n",
    "\n",
    "    ###########################################################################################\n",
    "    # id로 구성된 개별 문장 token list를 받아서 패딩 추가 [20, 2, 1] => [20, 2, 1, 0, 0, 0, ..]\n",
    "    # max_length에 토큰리스트의 개수를 맞춰주는 함수.\n",
    "    ############################################################################################\n",
    "    def __pad_token_sequences(self, token_sequences):\n",
    "        \"\"\"\n",
    "        id로 구성된 개별 문서(댓글)의 token_id list를 받아서 max_length 길이에 맞추는 메소드\n",
    "        max_length 보다 토큰수가 적으면 [PAD] 추가, 많으면 max_length 크기로 줄인다.\n",
    "            ex) max_length = 5, [PAD] toekn id가 0\n",
    "            - [20, 2, 1] => [20, 2, 1, 0, 0, 0, ..]\n",
    "            - [20, 30, 40, 50, 60, 70, 80][:5] => [20, 30, 40, 50, 60]\n",
    "        \"\"\"\n",
    "        pad_token_id = self.tokenizer.token_to_id(\"[PAD]\")\n",
    "        seq_len = len(token_sequences) # 입력 받은 토큰 개수\n",
    "        result = None\n",
    "        if seq_len > self.max_length: # 잘라내기\n",
    "            result = token_sequences[:self.max_length]\n",
    "        else: \n",
    "            result = token_sequences + ([pad_token_id] * (self.max_length - seq_len)) # 추가할 값을 리스트에 넣어놓고 계산\n",
    "        return result\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels) # 총 데이터 개수를 반환.\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        idx 번째 text와 label을 학습 가능한 type으로 변환해서 반환\n",
    "        Parameter\n",
    "            idx: int 조회할 index\n",
    "        Return\n",
    "            tuple: (torch.LongTensor, torch.FloatTensor) - 댓글 토큰_id 리스트, 정답 Label\n",
    "        \"\"\"\n",
    "        txt = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return (torch.tensor(txt, dtype=torch.int64), torch.tensor([label], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9515bb6f-703d-4277-b645-8869267f5297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id(\"[PAD]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "161f63e1-4ba8-4ccc-8c62-422b1f30b431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 11, 10, 9, 22]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 댓글의 토큰 수를 조회\n",
    "all_input_length = [len(tokenizer.encode(txt)) for txt in all_inputs]\n",
    "all_input_length[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee963d58-0008-4fa6-b426-e3e332591f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 89)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.min(all_input_length), np.max(all_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eea5f00e-70fa-475b-8e2d-d06d120e3bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29., 41.])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(all_input_length, q=[0.9, 0.95])\n",
    "# 전체 중 90%의 토큰 수는 29 미만, 95%는 41개 미만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ad9921cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 30\n",
    "trainset = NSMCDataset(train_inputs, train_labels, MAX_LENGTH, tokenizer)\n",
    "testset = NSMCDataset(test_inputs, test_labels, MAX_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2c20a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 50000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "002b549e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  540, 11354,   506,  2408,  5414,  5426,  2408,  5414,   119,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " tensor(1.))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb4d64",
   "metadata": {},
   "source": [
    "위에서 0은 PAD를 의미. \n",
    "\n",
    "label 1은 긍정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb07a55d",
   "metadata": {},
   "source": [
    "데이터셋 만들었으니, 데이터 로더 만들어야겠죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b5c65bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(testset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdb9153",
   "metadata": {},
   "source": [
    "데이터 로더에 `len()` -> step 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602aa6bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2343, 782)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c68f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42b5f038-b32c-4e4e-82c8-956c7cbe0c4d",
   "metadata": {},
   "source": [
    "# 모델링\n",
    "- Embedding Layer를 이용해 Word Embedding Vector를 추출한다.\n",
    "- LSTM을 이용해 Feature 추출\n",
    "- Linear + Sigmoid로 댓글 긍정일 확률 출력\n",
    "  \n",
    "![outline](figures/rnn/RNN_outline.png)\n",
    "\n",
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ac88afd6-5c8f-4ade-b930-86c9425e86e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "30e594de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class NSMCClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, bidirectional=True, dropout_rate=0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size(int) - 어휘사전 총 어휘수\n",
    "            embedding_dim(int) - word embedding vector의 차원수\n",
    "            hidden_size(int) - LSTM의 hidden state의 feature 수\n",
    "            num_layers(int) - LSTM의 layer의 개수\n",
    "            bidirectional(bool) - LSTM의 양방향 여부\n",
    "            dropout_rate(float) - LSTM이 두 개 이상의 layer로 구성된 경우 적용할 dropout 비율. \n",
    "                                  Dropout Layer의 dropout 비율\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 모델을 구성할 Layer들을 정의 - Embedding, LSTM, Dropout, Linear(추론기), Sigmoid\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,      # 총 단어 수 -> tokenizer에 등록된 총 단어수\n",
    "            embedding_dim=embedding_dim,    # embedding vector의 차원 수\n",
    "            padding_idx=0,                  # \"[PAD]\"의 토큰 ID(tokenizer.token_to_index(\"[PAD]\")) \n",
    "                                            # 패딩 토큰은 학습하지 않는다. \n",
    "\n",
    "        )\n",
    "        # embedding layer의 출력 shape: (batch_size:64, seq_length: 문서 토큰 수인 max_length 30, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,   # 개별 토큰(단어)의 feature 수 (embedding -> LSTM)\n",
    "            hidden_size=hidden_size,    # 파라미터로 받은 hidden size\n",
    "            num_layers=num_layers,      # 마찬가지로 파라미터로 받은 거 넣기\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0,   # stacked rnn일 경우 설정.                                                    \n",
    "                                                                         \n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate) # LSTM과 Linear 사이에 과적합 방지를 위해서 사용한다. \n",
    "\n",
    "        # lstm의 출력: out, (hidden, cell)\n",
    "        # out: 모든 타임스텝의 hidden state 값 # [seq_len, batch, hidden * bidirectional]\n",
    "        # hidden: 마지막 타임스텝의 hidden state (단기기억) [bidirectional(2) * num_layers, batch, hidden]\n",
    "        # cell: 마지막 타임스텝의 cell state (장기기억) \n",
    "        input_features = hidden_size * 2 if bidirectional else hidden_size\n",
    "        self.classifier = nn.Linear(input_features, 1) # 출력 노드의 개수는 1: 이진분류 -> Positive일 확률\n",
    "        self.sigmoid = nn.Sigmoid() # classifier의 출력값을 확률(0~1)값으로 변환하는 함수. 로지스틱\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            X(tensor): 입력 문서 토큰 리스트. shape: [batch_size, seq_length(max_length: 문서 구성 토큰 수)]\n",
    "        \"\"\"\n",
    "        embedding_vectors = self.embedding(X)\n",
    "        # [batch, seq_len] -> embedding -> [batch, seq_len, embedding 차원 수]\n",
    "        # LSTM - batch_first=False: 입력 shape - [seq_len, batch_size, embedding_dim]\n",
    "        # embedding_vectors의 batch 축과 seq_len >>축<<(값의 위치)를 바꿔준다. \n",
    "        embedding_vectors = embedding_vectors.transpose(1, 0) # 1번 축과 0번 축의 위치를 바꿔준다.\n",
    "        out, _ = self.lstm(embedding_vectors)\n",
    "        # out.shape: [seq_len, batch, hidden_size * (2 if bidirectional else 1)]\n",
    "        # classifier(linear)에는 out의 마지막 index(마지막 seq) 값을 입력\n",
    "        output = self.dropout(out[-1])\n",
    "        output = self.classifier(out[-1])\n",
    "        last_output = self.sigmoid(output)\n",
    "        return last_output\n",
    "\n",
    "\n",
    "        self.lstm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72901b5a",
   "metadata": {},
   "source": [
    "[20, 30, 9, 0, 0] -> embedding layer -> [[0.1, 0.3, 0.7], [1.2, -0.4, 2.1], [0.3, 3.0, 1.2], [0, 0, 0], [0, 0, 0]]\n",
    "\n",
    "padding_idx를 0으로 지정하면 -> 학습을 안 하기 때문에 -> embedding vector 확인 했을 때 0번 벡터는 학습이 안 된 채로 영벡터가 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18596fc",
   "metadata": {},
   "source": [
    "(참고) `transpose()`와 `permute()`\n",
    "- transpose는 축 두 개를, permute는 축 여러 개의 위치를 바꿀 때 쓴다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4739c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose()\n",
    "# 코드 실행하지 말아용 필기한 거니까 🩷\n",
    "ev.transpose(1, 0)\n",
    "# 파라미터의 순서: index를 이동시킬 축 위치\n",
    "# 파라미터 값: 이동할 대상 index의 축 위치\n",
    "# 예) [2, 1] -> [1, 2]\n",
    "\n",
    "ev = [\n",
    "    [10, 20, 30],\n",
    "    [40, 50, 60]\n",
    "] # 2 x 3 tensor라면 # shape: (2, 3)\n",
    "a = ev.transpose(1, 0)\n",
    "# 10 idx: [0, 0] -> [0, 0]\n",
    "# 20 idx: [0, 1] -> [1, 0]\n",
    "# 30 idx: [0, 2] -> [2, 0]\n",
    "# 40 idx: [1, 0] -> [0, 1]\n",
    "a = [\n",
    "    [10, 40],\n",
    "    [20, 50],\n",
    "    [30, 60]\n",
    "]# shape: (3, 2)\n",
    "\n",
    "b = a.reshape(3, 2) # 원소 순서대로 재배열\n",
    "b = [\n",
    "    [10, 20],\n",
    "    [30, 40],\n",
    "    [50, 60]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a0171-371e-4cb5-9bd5-ad1e3c14480d",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "25edf7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [a[0].max().item() for a in trainset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9efef2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26738"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5aa97e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26739"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7830d2b5-d5ed-4b53-a442-bebc83077aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSMCClassifier(\n",
      "  (embedding): Embedding(26739, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 64, num_layers=2, dropout=0.3, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (classifier): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = tokenizer.get_vocab_size()  # 총 어휘수\n",
    "EMBEDDING_DIM = 100  # 하이퍼파라미터\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 2\n",
    "BIDIRECTIONAL = True  \n",
    "DROPOUT_RATE = 0.3\n",
    "\n",
    "# 모델의 복잡도를 올린다. => EMBEDDING_DIM, HIDDEN_SIZE, NUM_LAYERS를 크게 잡으면 된다.\n",
    "# Auto regressive(자기 회귀) 모형이 아니면 BIDIRECTIONAL = True\n",
    "\n",
    "model = NSMCClassifier(\n",
    "    vocab_size = VOCAB_SIZE,\n",
    "    embedding_dim = EMBEDDING_DIM,\n",
    "    hidden_size = HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "66cbb192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "NSMCClassifier                           [64, 1]                   --\n",
       "├─Embedding: 1-1                         [64, 30, 100]             2,673,900\n",
       "├─LSTM: 1-2                              [30, 64, 128]             184,320\n",
       "├─Dropout: 1-3                           [64, 128]                 --\n",
       "├─Linear: 1-4                            [64, 1]                   129\n",
       "├─Sigmoid: 1-5                           [64, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 2,858,349\n",
       "Trainable params: 2,858,349\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 525.03\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 3.50\n",
       "Params size (MB): 11.43\n",
       "Estimated Total Size (MB): 14.95\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary\n",
    "i = torch.randint(1, 10, (64, MAX_LENGTH)) # int64 타입의 dummy 입력 데이터터\n",
    "# 입력 shape: (batch, seq_len)\n",
    "summary(model, input_data=i, device=device)\n",
    "# summary(모델, input_shape) => 내부적으로 입력데이터(float32)를 생성해서 추론론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae04a25a-c6dc-4833-883b-54972a16c171",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe5b3d-8b2f-4164-9b97-29e6aadced5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d229e-9a99-4a28-9dfd-9582686ba052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bdd5885-8150-4529-ad3a-84931a8824c5",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a1bf6-d8eb-42d0-996e-f975e93888af",
   "metadata": {},
   "source": [
    "### Train/Test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46099bec-eee3-4cef-921b-ce9ee6cf0f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 epoch train 하는 함수\n",
    "def train(model, dataloader, loss_fn, optimizer, device=\"cpu\"):\n",
    "    # 1. 모델을 train 모드로 변환\n",
    "    model.train()\n",
    "    # 2. 모델을 device로 이동\n",
    "    model = model.to(device)\n",
    "    total_loss = 0.0 # step별 loss를 저장.\n",
    "    # step 단위로 모델 학습 (batch)\n",
    "    # 데이터로더로부터 1 배치를 받아온다.\n",
    "    for X, y in dataloader:\n",
    "        # 1. X, y를 device로 이동\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # 2. 추론\n",
    "        pred = model(X)\n",
    "        # 3. loss 계산\n",
    "        loss = loss_fn(pred, y) # 여기까지 순전파\n",
    "        # 순전파에서 grad가 만들어짐 -> 역전파에서 쓴다.\n",
    "        # 4. gradient 계산\n",
    "        loss.backward()\n",
    "        # 5. 파라미터 업데이트 w.data - w.grad * lr\n",
    "        optimizer.step()\n",
    "        # 6. gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "        # loss 누적\n",
    "        total_loss += loss.item()\n",
    "    # 1 epoch 학습 완료\n",
    "    return total_loss / len(dataloader) # 1 epoch의 train loss를 반환. (total loss / step수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5eb3ea-78e8-4248-a8ce-d07a4c362d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 epoch 평가/검증 함수\n",
    "def test(model, dataloader, loss_fn, device=\"cpu\"):\n",
    "    # 1. 모델을 eval 모드로 변경, 모델의 디바이스 이동\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    # loss, accuracy\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    with torch.no_grad(): # 순전파할 때 grad(도함수)는 만들지 마라~\n",
    "        for X, y in dataloader: \n",
    "            # 1 step 처리\n",
    "            # 1. X, y를 device로 이동\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # 2. 추론\n",
    "            pred_proba = model(X) # 양성일 확률 \n",
    "            pred_label = (pred_proba).type(torch.int32) \n",
    "            total_loss += loss_fn(pred_proba, y).item()\n",
    "            total_acc += (pred_label == y).sum().item() # [1]\n",
    "        \n",
    "        # return loss, acc\n",
    "        return total_loss / len(dataloader), total_acc / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5570ed",
   "metadata": {},
   "source": [
    "[1]\n",
    "\n",
    "1. `pred_label == y`: 정답을 맞혔나요? T/F\n",
    "2. `(pred_label == y).sum()`: 정답을 맞춘 횟수\n",
    "3. `item()`: tensor형이니까 뒤에 `item()` 붙여서 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de42da4-8991-4bcb-9ce9-18155459dec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c8853d0-b137-47bb-8f0d-fc4f05700cf2",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cd806dda-5058-4c44-a3f4-28cadc8a90d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "EPOCHS = 3\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e18c2-17bf-4621-b630-b47e16c34bf4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e2b6ec21-1d3c-48c2-b7c3-314daea6576c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/3] train loss: 0.42536706964251225, val loss: 0.4136254567738689, val acc: 0.49654\n",
      "[1/3] train loss: 0.38486712115267835, val loss: 0.39310799735357693, val acc: 0.49654\n",
      "[2/3] train loss: 0.3605955367891187, val loss: 0.3972244799289557, val acc: 0.49654\n",
      "499.6189196109772\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s = time.time()\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train(model, train_loader, loss_fn, optimizer, device) # 1 epoch 학습\n",
    "    val_loss, val_acc = test(model, test_loader, loss_fn, device)\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_acc)\n",
    "    print(f\"[{epoch}/{EPOCHS}] train loss: {train_loss}, val loss: {val_loss}, val acc: {val_acc}\")\n",
    "\n",
    "e = time.time()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32690441-482a-46b1-b91b-b85329d2141f",
   "metadata": {},
   "source": [
    "## 모델저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "80c16618-1517-4371-8e37-ae3cf03428b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"saved_models/nsmc/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2e2d41e8-0715-4f50-aa37-11a8e142706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "load_model = torch.load(\"saved_models/nsmc/model.pt\", weights_only=False) # 모델 자체를 저장한 거야, weight만 저장한 게 아니라~!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830cc10e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc5917d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3de7ed5-f7f6-4206-b16f-f8535a03405c",
   "metadata": {},
   "source": [
    "# 서비스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827bdaa3-008d-4a93-aee6-0877e829ef32",
   "metadata": {},
   "source": [
    "## 전처리 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1a2661a9-3964-4117-b273-e5d8bd4194b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "morph_tokenizer = Okt()\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]+\", ' ', text)\n",
    "    return ' '.join(morph_tokenizer.morphs(text, stem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "315603df-159b-4317-9fb4-7897546b7cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_token_sequences(token_sequences, max_length):\n",
    "    \"\"\"padding 처리 메소드.\"\"\"\n",
    "    pad_token = tokenizer.token_to_id('[PAD]')  \n",
    "    seq_length = len(token_sequences)           \n",
    "    result = None\n",
    "    if seq_length > max_length:                 \n",
    "        result = token_sequences[:max_length]\n",
    "    else:                                            \n",
    "        result = token_sequences + ([pad_token] * (max_length - seq_length))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8d73070a-0ee5-4f35-996c-b0d11ba08516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data_preprocessing(text_list):\n",
    "    \"\"\"\n",
    "    모델에 입력할 수있는 input data를 생성\n",
    "    Parameter:\n",
    "        text_list: list - 추론할 댓글리스트\n",
    "    Return\n",
    "        torch.LongTensor - 댓글 token_id tensor\n",
    "    \"\"\"\n",
    "\n",
    "    # cleansing + 정규화\n",
    "    text_list = [text_preprocessing(txt) for txt in text_list]\n",
    "    # text -> 토큰화화\n",
    "    token_list = [tokenizer.encode(txt).ids for txt in text_list]\n",
    "    # 토큰리스트의 크기(size)를 max_length에 맞추기\n",
    "    token_list = [pad_token_sequences(token, MAX_LENGTH) for token in token_list]\n",
    "    return torch.tensor(token_list, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b895f244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb5c413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1e19997-6b61-446f-ac72-376cd34ee495",
   "metadata": {},
   "source": [
    "## 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a6fb00ab-60d0-45f2-bb16-505a5f5cc056",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_list = [\"아 진짜 재미없다.\", \"여기 식당 먹을만 해요\", \"이걸 영화라고 만들었냐?\", \"기대 안하고 봐서 그런지 괜찮은데.\", \"이걸 영화라고 만들었나?\", \"아! 뭐야 진짜.\", \"재미있는데.\", \"연기 짱 좋아. 한번 더 볼 의향도 있다.\", \"뭐 그럭저럭\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5f9a6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "499c330d-69ff-43fb-9ee9-ad1c6054f9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 30])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = predict_data_preprocessing(comment_list)\n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5273ac02-81f3-4391-b802-f9dca1f8d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, comment_list:list[str], input_tensor:torch.tensor, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    model로 comment_list(토큰id로 변환된 tensor)를 추론해서 긍정/부정적인 댓글인지 출력\n",
    "    출력 형식\n",
    "        comment(댓글) label 학률\n",
    "        \"아 재미없다.\"  부정  0.9 (부정적인 댓글일 확률)\n",
    "        \"재밌다.\"      긍정  0.87 (긍정적인 댓글일 확률)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(input_tensor) # shape: (batch, 1) -> positive일 확률\n",
    "        for txt, pos_proba in zip(comment_list, pred):\n",
    "            label = \"긍정\" if pos_proba.item() > 0.5 else \"부정\"\n",
    "            proba = pos_proba.item() if pos_proba.item() > 0.5 else 1 - pos_proba.item() # 확률\n",
    "            print(txt, label, round(proba, 3), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a0510a47-9189-4c2a-a6e9-33b04971f2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 진짜 재미없다.\t부정\t0.985\n",
      "여기 식당 먹을만 해요\t긍정\t0.745\n",
      "이걸 영화라고 만들었냐?\t부정\t0.925\n",
      "기대 안하고 봐서 그런지 괜찮은데.\t긍정\t0.821\n",
      "이걸 영화라고 만들었나?\t부정\t0.925\n",
      "아! 뭐야 진짜.\t부정\t0.656\n",
      "재미있는데.\t긍정\t0.98\n",
      "연기 짱 좋아. 한번 더 볼 의향도 있다.\t긍정\t0.985\n",
      "뭐 그럭저럭\t부정\t0.903\n"
     ]
    }
   ],
   "source": [
    "predict(model, comment_list, input_tensor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f2f19d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분석하려는 댓글을 입력하세요. 종료하려면 !quit을 입력하세요요\n",
      "돈이 아까워\t부정\t0.987\n",
      "대박이다\t긍정\t0.98\n",
      "짜장면\t긍정\t0.55\n",
      "우와 완전 못 만들었다.. 감동적이야..\t긍정\t0.971\n",
      "ㅋ\t긍정\t0.735\n",
      "종료\n"
     ]
    }
   ],
   "source": [
    "print(\"분석하려는 댓글을 입력하세요. 종료하려면 !quit을 입력하세요요\")\n",
    "\n",
    "while True:\n",
    "    comment = input(\"댓글: \")\n",
    "    if comment == \"!quit\":\n",
    "        print(\"종료\")\n",
    "        break\n",
    "    comment_list = [comment]\n",
    "    input_tensor = predict_data_preprocessing([comment])\n",
    "    predict(model, comment_list, input_tensor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c6411e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
