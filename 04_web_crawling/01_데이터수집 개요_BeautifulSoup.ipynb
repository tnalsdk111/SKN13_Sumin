{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 수집에 도움이 되는 사이트\n",
    "\n",
    "- **국가 통계포털**\n",
    "    - https://kosis.kr\n",
    "    - 통계청에서 관리하는 공공데이터 포털로 다양한 카테고리의 국가 통계데이터를 제공한다.\n",
    "- **공공데이터 포털**\n",
    "    - https://www.data.go.kr\n",
    "    - 행정 안전부에서 제공하는 정부 데이터 포털\n",
    "- **Kaggle**\n",
    "    - https://kaggle.com\n",
    "    - 데이터과학 관련 경진대회 플랫폼\n",
    "    - 다양한 데이터들을 제공한다.\n",
    "- **구글 데이터셋 서치**\n",
    "    - https://datasetsearch.research.google.com\n",
    "    - 구글에서 제공하는 데이터셋 검색 사이트\n",
    "    - 키워드를 이용해 다양한 데이터셋을 검색하고 다운로드 받을 수 있다.\n",
    "- **AI Hub**\n",
    "    - https://aihub.or.kr\n",
    "    - 국내외 기관/기업에서 추진한 지능정보산업 인프라 조성사업에서 공개한 AI 학습용 데이터셋들을 제공한다.\n",
    "- **Roboflow Universe**\n",
    "    - https://universe.roboflow.com/\n",
    "    - Roboflow 라는 인공지능 회사에서 운영하는 데이터 저장소 사이트로 컴퓨터비전 관련 데이터셋을 주로 제공한다.\n",
    "- 기타\n",
    "    - **지자체**: 서울시 열린 데이터광장, 경기 데이터 드림\n",
    "    - **금융관련**: 한국거래소, 금융통계정보시스템등\n",
    "    - **영화관련**: 영화진흥위원회\n",
    "    - **대중교통**: 국가교통데이터베이스, 교통카드 빅데이터 통합정보시스템등    \n",
    "    - **관광관련**: 한국 관광 데이터랩등\n",
    "    - **날씨정보**: 기상청 기상자료 개방포털, 네이버 날씨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [크롬개발자 도구](https://developers.google.com/web/tools/chrome-devtools/)\n",
    "\n",
    "- 크롬 개발자 도구는 웹 개발 및 디버깅을 위한 강력한 도구로 크롬 웹브라우저에 내장되어 있다.\n",
    "    - `F12` 나 팝업 메뉴에서 `검사`를 선택한다.\n",
    "    - 엣지 브라우저도 같은 개발자 도구를 제공한다.\n",
    "- 웹 페이지의 HTML, CSS, JavaScript 코드를 검사하고 수정할 수 있으며, 네트워크 요청 응답 내용 분석, 성능 분석, 콘솔 로그 등 다양한 기능을 제공한다.\n",
    "- 주요 기능\n",
    "    - **요소 검사:** 웹 페이지의 특정 요소를 선택하여 HTML 구조, CSS 스타일, selector 등을 확인한다.\n",
    "    - **콘솔:** JavaScript 코드를 실행할 수 있고 Javascript 실행시 발생한 오류 메시지 등을 확인할 수 있다.\n",
    "    - **소스:** 웹 페이지의 JavaScript 코드를 확인할 수있고 디버깅을 위한 중단점(break point)를 설정하고 디버깅할 수 있다.\n",
    "    - **네트워크:** 웹 페이지를 요청할 때 발생하는 요청 및 응답 데이터를 분석하고 성능을 측정할 수 있다.\n",
    "    - **성능:** 웹 페이지의 로딩 시간, 렌더링 성능에 걸린 시간등을 분석할 수 있다.\n",
    "    - **애플리케이션:** 쿠키, 로컬 스토리지, 세션 스토리지 등 클라이언트 저장 데이터 확인 할 수 있다.\n",
    "- 개발자 도구는 크롤링 시 필수적인 도구이며, 수집할 페이지를 분석하는데 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup\n",
    "- Markup 언어 parsing 라이브러리\n",
    "    - HTML이나 XML 문서 내에서 원하는 정보를 가져오기 위한 파이썬 라이브러리.\n",
    "- https://www.crummy.com/software/BeautifulSoup/\n",
    "- https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "- 설치(아나콘다 프롬프트에)\n",
    "    - beautifulsoup4 설치\n",
    "        - pip install beautifulsoup4\n",
    "    - lxml 설치(html/xml parser)\n",
    "        - pip install lxml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코딩 패턴\n",
    "1. 조회할 HTML내용을 전달하여 BeautifulSoup 객체 생성 \n",
    "1. BeautifulSoup객체의 메소드들을 이용해 문서내에서 필요한 정보 조회\n",
    "    - 태그이름과 태그 속성으로 조회\n",
    "    - css selector를 이용해 조회\n",
    "    - . 표기법을 이용한 탐색(Tree 구조 순서대로 탐색)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup 객체 생성\n",
    "- BeautifulSoup(html str [, 파서])\n",
    "    - 매개변수\n",
    "        1. 정보를 조회할 html을 string으로 전달\n",
    "        2. 파서\n",
    "            - html.parser(기본파서)\n",
    "            - lxml : 매우 빠르다. html, xml 파싱 가능(xml 파싱은 lxml만 가능)\n",
    "                - 사용시 install 필요 \n",
    "                - `conda install lxml`\n",
    "                - `pip install lxml`\n",
    "                - install 후 커널 restart 시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\SKN13SM\\SKN13_Sumin\\04_web_crawling\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example.html 생성 완료!\n"
     ]
    }
   ],
   "source": [
    "## example.html 생성\n",
    "# 문서 내용\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "  <head>\n",
    "    <title>Example Page</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Hello, BeautifulSoup!</h1>\n",
    "    <p>This is a paragraph.</p>\n",
    "    <a href=\"https://example.com\">Visit Example.com</a>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# 파일 생성\n",
    "with open(\"example.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "print(\"example.html 생성 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<html>\n",
      "  <head>\n",
      "    <title>Example Page</title>\n",
      " \n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "## 파일 불러오기\n",
    "\n",
    "# HTML을 파싱하기 위한 BeautifulSoup 라이브러리를 불러온다.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# example.html 파일을 읽기 모드로 연다.\n",
    "# r: read, 읽기 전용 / t: text, 텍스트 모드 \n",
    "with open(\"example.html\", \"rt\", encoding='utf-8') as fr:\n",
    "    html_doc = fr.read() # html의 전체 내용을 html_doc에 문자열로 저장한다.\n",
    "\n",
    "# 잘 불러와졌는지 확인 - 처음 50글자만 출력해서 내용을 확인한다. \n",
    "print(html_doc[:50])\n",
    "print(type(html_doc)) # str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## html_doc을 beautifulsoup으로 파싱해서 HTML 구조를 분석한다.\n",
    "soup = BeautifulSoup(html_doc, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Example Page\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>\n",
      "   Hello, BeautifulSoup!\n",
      "  </h1>\n",
      "  <p>\n",
      "   This is a paragraph.\n",
      "  </p>\n",
      "  <a href=\"https://example.com\">\n",
      "   Visit Example.com\n",
      "  </a>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 구조화된 HTML이 예쁘게 출력된다. \n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서내에서 원하는 정보 검색\n",
    "\n",
    "### Tag 객체\n",
    "- 하나의 태그(element)에 대한 정보를 다루는 객체.\n",
    "    - BeautifulSoup 조회 메소드들의 **조회결과의 반환타입.**\n",
    "    - 조회 함수들이 찾은 Element가 하나일 경우 **Tag 객체를, 여러개일 경우 Tag 객체들을 담은 List(ResultSet)**를 반환한다.\n",
    "    - Tag 객체는 찾은 정보를 제공하는 메소드와 Attribute를 가지고 있다. 또 찾은 Tag가 하위 element를 가질 경우 찾을 수 있는 조회 메소드를 제공한다.\n",
    "- 주요 속성/메소드\n",
    "    - **태그의 속성값 조회**\n",
    "        - tag객체.get('속성명') \n",
    "        - tag객체\\['속성명'\\]\n",
    "        - ex) tag.get('href') 또는 tag\\['href'\\]\n",
    "    - **태그내 text값 조회**\n",
    "        - tag객체.get_text()\n",
    "        - tag객체.text\n",
    "        - ex) tag.get_text() 또는 tag.text\n",
    "    - **contents 속성**\n",
    "        - 조회한 태그의 모든 자식 요소들을 리스트로 반환\n",
    "        - ex) child_list = tag.contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 조회 함수\n",
    "- **태그의 이름으로 조회**\n",
    "    - find_all()\n",
    "    - find()\n",
    "- **css selector를 이용해 조회**\n",
    "    - select(), select_one()\n",
    "- **`.` 표기법(dot notation)**\n",
    "    - dom tree 구조의 계층 순서대로 조회\n",
    "    - 위의 두방식으로 찾은 tag를 기준으로 그 주위의 element 들을 찾을 때 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 태그의 이름으로 조회\n",
    "- **find_all**(name=태그명, attrs={속성명:속성값, ..})\n",
    "   - 이름의 모든 태그 element들을 리스트에 담아 반환.\n",
    "   - 여러 이름의 태그를 조회할 경우 List에 태그명들을 묶어서 전달한다.\n",
    "   - 태그의 attribute 조건으로만 조회할 경우 name을 생략한다. \n",
    "- **find**(name=태그명, attrs={속성명:속성값})\n",
    "    - 이름의 태그중 첫번째 태그 element를 반환."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🥰 예습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example_news.html 파일 생성\n",
    "\n",
    "# 문서 내용\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "  <body>\n",
    "    <div class=\"news\">\n",
    "      <h2>오늘의 뉴스</h2>\n",
    "      <p class=\"content\">날씨가 맑습니다.</p>\n",
    "    </div>\n",
    "    <div class=\"news\">\n",
    "      <h2>스포츠 뉴스</h2>\n",
    "      <p class=\"content\">축구에서 승리했습니다.</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# 파일 생성\n",
    "# open() 함수로 파일 열기\n",
    "# 주요 파라미터: 파일명, 모드, 인코딩\n",
    "with open(\"example_news.html\", \"w\", encoding = \"utf-8\") as f: \n",
    "    f.write(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<html>\n",
      "  <body>\n",
      "    <div class=\"news\">\n",
      "      <h2>오늘의 뉴스</h2>\n",
      "      <p class=\"content\">날씨가 맑습니다.</p>\n"
     ]
    }
   ],
   "source": [
    "## 파일 불러오기\n",
    "\n",
    "# HTML을 파싱하기 위한 BeautifulSoup 라이브러리를 불러온다. \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# example_news.html을 읽기 모드로 연다.\n",
    "with open(\"example_news.html\", \"rt\", encoding = \"utf-8\") as fr:\n",
    "    html_doc = fr.read()\n",
    "\n",
    "# 잘 불러와졌는지 확인\n",
    "print(html_doc[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🥰 파싱이란? (Parsing)\n",
    "- **파싱 = \"문장을 분석해서 구조를 이해하는 일\"이야!**\n",
    "\n",
    "- 사람이 글을 읽으면 문장의 구조를 파악하잖아?\n",
    "→ 주어, 동사, 목적어처럼.\n",
    "\n",
    "- 컴퓨터도 HTML 같은 문서를 그냥 읽을 수는 없고,\n",
    "분석해서 **태그의 구조를 이해**해야 다룰 수 있어!\n",
    "\n",
    "- 그게 바로 파싱이야. 💡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`soup = BeautifulSoup(html_doc, \"lxml\")` 이 코드는 **HTML 문서를 파싱**해서 **BeautifulSoup 객체로 만드는 코드**야."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_doc을 BeautifulSoup으로 파싱\n",
    "soup = BeautifulSoup(html_doc, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **html_doc**: 문자열로 되어있는 **html 문서**\n",
    "- **\"lxml\"**: 어떤 **파서(parser)**를 쓸 건지 지정. lxml 파서는 빠르고 신뢰성이 좋다. \n",
    "- **soup**: 태그 구조가 분석된 html 문서를 담고 있는 **BeautifulSoup 객체**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<html>\n",
      "  <body>\n",
      "    <div class=\"news\">\n",
      "      <h2>오늘의 뉴스</h2>\n",
      "      <p class=\"content\">날씨가 맑습니다.</p>\n",
      "    </div>\n",
      "    <div class=\"news\">\n",
      "      <h2>스포츠 뉴스</h2>\n",
      "      <p class=\"content\">축구에서 승리했습니다.</p>\n",
      "    </div>\n",
      "  </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(html_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<body>\n",
      "<div class=\"news\">\n",
      "<h2>오늘의 뉴스</h2>\n",
      "<p class=\"content\">날씨가 맑습니다.</p>\n",
      "</div>\n",
      "<div class=\"news\">\n",
      "<h2>스포츠 뉴스</h2>\n",
      "<p class=\"content\">축구에서 승리했습니다.</p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup) # 파싱 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <div class=\"news\">\n",
      "   <h2>\n",
      "    오늘의 뉴스\n",
      "   </h2>\n",
      "   <p class=\"content\">\n",
      "    날씨가 맑습니다.\n",
      "   </p>\n",
      "  </div>\n",
      "  <div class=\"news\">\n",
      "   <h2>\n",
      "    스포츠 뉴스\n",
      "   </h2>\n",
      "   <p class=\"content\">\n",
      "    축구에서 승리했습니다.\n",
      "   </p>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🥰 태그 조회의 추천 학습 흐름\n",
    "- 1단계) 태그명/클래스로 요소 찾기: `find()`, `find_all()`\n",
    "\n",
    "- 2단계) 텍스트 꺼내기: `get_text()`\n",
    "\n",
    "- 3단계) 속성 꺼내기:`[\"href\"]`, `get(\"src\")`\n",
    "\n",
    "- 4단계) CSS 선택자로 정밀하게 찾기:`select()`, `select_one()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1단계: 태그명, 클래스로 요소 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 모든 뉴스 구역 가져오기: class = \"news\"인 것\n",
      "\n",
      "[<div class=\"news\">\n",
      "<h2>오늘의 뉴스</h2>\n",
      "<p class=\"content\">날씨가 맑습니다.</p>\n",
      "</div>, <div class=\"news\">\n",
      "<h2>스포츠 뉴스</h2>\n",
      "<p class=\"content\">축구에서 승리했습니다.</p>\n",
      "</div>] \n",
      "\n",
      "\n",
      "2. 각 뉴스의 본문만 가져오기 - div(구역 나누는 태그) 아래 p(본문 태그):\n",
      "\n",
      "[<p class=\"content\">날씨가 맑습니다.</p>, <p class=\"content\">축구에서 승리했습니다.</p>]\n"
     ]
    }
   ],
   "source": [
    "# 모든 뉴스 구역 가져오기\n",
    "print(\"1. 모든 뉴스 구역 가져오기: class = \\\"news\\\"인 것\\n\")\n",
    "print(soup.find_all(\"div\", class_=\"news\"), \"\\n\\n\") \n",
    "\n",
    "# 각 뉴스의 본문만\n",
    "print(\"2. 각 뉴스의 본문만 가져오기 - div(구역 나누는 태그) 아래 p(본문 태그):\\n\")\n",
    "print(soup.select(\"div.news > p.content\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2단계: 텍스트 꺼내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 제목 가져오기:\n",
      "오늘의 뉴스\n"
     ]
    }
   ],
   "source": [
    "print(\"뉴스 제목 가져오기:\")\n",
    "print(soup.find(\"h2\").get_text())\n",
    "# 1. find(\"h1\"): \"h2\" 태그를 가진 요소를 찾아서\n",
    "# 2. get_text(): 택스트를 가져온다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🩷 (참고) AttributeError: 본문에 \"h1\" 태그가 없는데 find(\"h1\") 한 경우\n",
    "- AttributeError: 'NoneType' object has no attribute 'get_text'\n",
    "- 이 말은 쉽게 말해서:\n",
    "  - soup.find(\"h1\")가 아무것도 못 찾았어 → 그래서 None이 나왔고\n",
    "  - 그 None한테 .get_text() 하니까 에러 난 거야! ❌\n",
    "\n",
    "- ☑️ 해결방법 1: HTML에 \"h1\" 태그가 있는지 확인한다. \n",
    "- ☑️ 해결방법 2: if문으로 None 여부 체크 (바로 아래 코드)\n",
    "- ☑️ 해결방법 3: `print(soup.prettify())` 코드로 구조 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 태그를 찾을 수 없습니다 😥\n"
     ]
    }
   ],
   "source": [
    "title_tag = soup.find(\"h1\")\n",
    "\n",
    "if title_tag:\n",
    "    print(\"뉴스 제목:\", title_tag.get_text())\n",
    "else:\n",
    "    print(\"h1 태그를 찾을 수 없습니다 😥\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <div class=\"news\">\n",
      "   <h2>\n",
      "    오늘의 뉴스\n",
      "   </h2>\n",
      "   <p class=\"content\">\n",
      "    날씨가 맑습니다.\n",
      "   </p>\n",
      "   <a href=\"https://search.naver.com/search.naver?where=nexearch&amp;sm=top_hty&amp;fbm=0&amp;ie=utf8&amp;query=%EB%82%A0%EC%94%A8\">\n",
      "    오늘 날씨 보기\n",
      "   </a>\n",
      "  </div>\n",
      "  <div class=\"news\">\n",
      "   <h2>\n",
      "    스포츠 뉴스\n",
      "   </h2>\n",
      "   <p class=\"content\">\n",
      "    축구에서 승리했습니다.\n",
      "   </p>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 잠시 수정!\n",
    "\n",
    "# <p> 태그 선택\n",
    "p_tag = soup.find(\"p\", class_=\"content\")\n",
    "# (참고) 첫 번째 p 태그 뒤에 내용을 붙일 거라 find()를 씀. \n",
    "# 두 번째 것을 수정하고 싶다면 find_all() 써서 다 불러오자.\n",
    "\n",
    "# <a> 태그 새로 만들기\n",
    "new_a = soup.new_tag(\"a\", href=\"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=%EB%82%A0%EC%94%A8\")\n",
    "new_a.string = \"오늘 날씨 보기\"\n",
    "\n",
    "# <p> 태그 바로 **뒤에** <a> 태그 추가\n",
    "p_tag.insert_after(new_a)\n",
    "\n",
    "# 결과 보기\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 html 파일에 저장하려면 open() 함수를 통해 write() 해주어야 한다.\n",
    "with open(\"example_news.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(str(soup))  # 수정된 soup 객체를 문자열로 변환해서 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3단계: 속성 꺼내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find(\"h2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"example.html\", \"rt\", encoding=\"utf-8\") as fr:\n",
    "    html_doc = fr.read()\n",
    "\n",
    "soup = BeautifulSoup(html_doc, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = soup.find_all(\"div\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<div class=\"news\">\n",
       " <h2>오늘의 뉴스</h2>\n",
       " <p class=\"content\">날씨가 맑습니다.</p><a href=\"https://search.naver.com/search.naver?where=nexearch&amp;sm=top_hty&amp;fbm=0&amp;ie=utf8&amp;query=%EB%82%A0%EC%94%A8\">오늘 날씨 보기</a>\n",
       " </div>,\n",
       " <div class=\"news\">\n",
       " <h2>스포츠 뉴스</h2>\n",
       " <p class=\"content\">축구에서 승리했습니다.</p>\n",
       " </div>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(result))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content: \n",
      "오늘의 뉴스\n",
      "날씨가 맑습니다.오늘 날씨 보기\n",
      " \n",
      "오늘의 뉴스\n",
      "날씨가 맑습니다.오늘 날씨 보기\n",
      "\n",
      "class속성값: ['news'] ['news']\n"
     ]
    }
   ],
   "source": [
    "tag1 = result[0]\n",
    "print(\"content:\", tag1.text, tag1.get_text())\n",
    "print(\"class속성값:\", tag1.get(\"class\"), tag1['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n",
      "--------------------------------------------------\n",
      "<div class=\"news\">\n",
      "<h2>오늘의 뉴스</h2>\n",
      "<p class=\"content\">날씨가 맑습니다.</p><a href=\"https://search.naver.com/search.naver?where=nexearch&amp;sm=top_hty&amp;fbm=0&amp;ie=utf8&amp;query=%EB%82%A0%EC%94%A8\">오늘 날씨 보기</a>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "result = soup.find(\"div\")\n",
    "print(type(result))\n",
    "print(\"-\"*50)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content text: \n",
      "오늘의 뉴스\n",
      "날씨가 맑습니다.오늘 날씨 보기\n",
      "\n",
      "--------------------------------------------------\n",
      "content text: \n",
      "오늘의 뉴스\n",
      "날씨가 맑습니다.오늘 날씨 보기\n",
      "\n",
      "--------------------------------------------------\n",
      "attribue의 value: ['news']\n",
      "--------------------------------------------------\n",
      "attribue의 value: ['news']\n"
     ]
    }
   ],
   "source": [
    "# 태그의 content와 attribute 조회\n",
    "\n",
    "print(\"content text:\", result.text)\n",
    "print(\"-\"*50)\n",
    "print(\"content text:\", result.get_text())\n",
    "print(\"-\"*50)\n",
    "print(\"attribue의 value:\", result.get(\"class\"))\n",
    "print(\"-\"*50)\n",
    "print(\"attribue의 value:\", result[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " <h2>오늘의 뉴스</h2>,\n",
       " '\\n',\n",
       " <p class=\"content\">날씨가 맑습니다.</p>,\n",
       " <a href=\"https://search.naver.com/search.naver?where=nexearch&amp;sm=top_hty&amp;fbm=0&amp;ie=utf8&amp;query=%EB%82%A0%EC%94%A8\">오늘 날씨 보기</a>,\n",
       " '\\n']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 태그의 모든 자식 요소들 조회\n",
    "result.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"https://search.naver.com/search.naver?where=nexearch&amp;sm=top_hty&amp;fbm=0&amp;ie=utf8&amp;query=%EB%82%A0%EC%94%A8\">오늘 날씨 보기</a>]\n",
      "--------------------------------------------------\n",
      "[<a href=\"https://search.naver.com/search.naver?where=nexearch&amp;sm=top_hty&amp;fbm=0&amp;ie=utf8&amp;query=%EB%82%A0%EC%94%A8\">오늘 날씨 보기</a>]\n",
      "--------------------------------------------------\n",
      "[]\n",
      "--------------------------------------------------\n",
      "[]\n",
      "--------------------------------------------------\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "result = soup.find_all(\"a\") \n",
    "print(result)\n",
    "print(\"-\"*50)\n",
    "result = soup.find_all([\"a\", \"span\"])  #한번에 여러이름의 태그드을 조회.\n",
    "print(result)\n",
    "print(\"-\"*50)\n",
    "result = soup.find_all(\"div\", attrs={\"class\":\"name\"}) # 태그이름 + 속성\n",
    "print(result)\n",
    "print(\"-\"*50)\n",
    "result = soup.find_all(\"div\", attrs={\"class\":\"animal_info\", \"id\":\"animal1\"}) # 속성 조건이 여러개\n",
    "print(result)\n",
    "print(\"-\"*50)\n",
    "result = soup.find_all(\"a\", attrs={\"href\":\"https://www.coexaqua.com\"})\n",
    "\n",
    "# import re\n",
    "# result = soup.find_all(\"a\", attrs={\"href\":re.compile(r\".com$\")}) # 정규표현식-.com으로 끝나는.\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for tag in result:\n",
    "    print(tag.text, tag['href'])\n",
    "    result_list.append([tag.text, tag['href']]) # list[text, href]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSS Selector를 이용해 조회\n",
    "- **select(selector='css셀렉터')**\n",
    "    - css 셀렉터와 일치하는 tag들을 반환한다.\n",
    "- **select_one(selector='css셀렉터')**\n",
    "    - css 셀렉터와 일치하는 tag를 반환한다.\n",
    "    - 일치하는 것이 여러개일 경우 첫번째 것 하나만 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"example.html\", \"rt\", encoding=\"utf-8\") as fr:\n",
    "    html_doc = fr.read()\n",
    "\n",
    "soup = BeautifulSoup(html_doc, \"lxml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"https://search.naver.com/search.naver?where=nexearch&amp;sm=top_hty&amp;fbm=0&amp;ie=utf8&amp;query=%EB%82%A0%EC%94%A8\">오늘 날씨 보기</a>]\n"
     ]
    }
   ],
   "source": [
    "# css selector를 이용한 조회\n",
    "\n",
    "result = soup.select(\"a\")         #  태그이름(a)  \n",
    "# result = soup.select(\"a, span\") # 태그이름(여러개)\n",
    "# result = soup.select(\"ul a\")    # ul의 자손인 a태그 찾는다.\n",
    "\n",
    "# result = soup.select_one(\"#animal1\")             # 모든 태그중 id=animal1\n",
    "# result = soup.select(\"ul + div\")                 # ul의 다음 형제 태그중 div\n",
    "# result = soup.select(\"body > div:nth-child(3)\")  # body의 3번째 자식 div\n",
    "\n",
    "# result = soup.select(\"a[href]\")                        # href 속성이 있는 a 태그들\n",
    "# result = soup.select(\"a[href='http://www.naver.com']\") # href='http://www.naver.com' 속성을 가진 a 태그\n",
    "# result = soup.select('a[href$=\".do\"]')                 # $=  href 속성값이 .do로 끝나는 a태그들\n",
    "# result = soup.select('a[href^=\"https\"]')               # =^  href 속성값이 https로 시작하는 a태그\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in result:\n",
    "    print(tag.text, tag['href'], tag.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
