{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4766ebb-432c-4bdb-8050-23df797098be",
   "metadata": {},
   "source": [
    "<!-- # Langchain은 다양한 LLM(대규모 언어 모델)을 지원한다\n",
    "-\t대규모 언어 모델(LLM, Large Language Model)을 개발하는 회사들은 사용자가 자신의 애플리케이션에서 LLM을 손쉽게 활용할 수 있도록 API(Application Programming Interface) 서비스를 제공하고 있다.\n",
    "-\t하지만 각 LLM은 고유한 API 호출 라이브러리(Library)를 제공하기 때문에, 개발자는 동일한 작업을 수행하더라도 LLM에 따라 다른 코드를 작성해야 하는 번거로움이 있다.\n",
    "-\tLangchain은 이러한 문제를 해결하기 위해 다양한 LLM의 API를 통합적으로 지원한다.\n",
    "-\t여러 LLM을 동일한 인터페이스(interface)로 호출할 수 있게 하여 특정 모델에 종속되지 않도록 하고, 필요에 따라 쉽게 다른 모델로 전환할 수 있다.\n",
    "-\tLangchain이 지원하는 주요 LLM 목록\n",
    "    - https://python.langchain.com/docs/integrations/chat/#featured-providers -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e3b65a-1b4e-4fcc-99ac-ec7624dac9ad",
   "metadata": {},
   "source": [
    "## 설치\n",
    "```bash\n",
    "pip install langchain langchain-community  -qU\n",
    "pip install python-dotenv -qU \n",
    "pip install ipywidgets -qU\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9e5829-972a-4345-858d-e479dab7c320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda create -n lang_env python=3.12 -y\n",
    "\n",
    "#conda activate lang_env\n",
    "\n",
    "# pip install ipykernel ipywidgets\n",
    "\n",
    "# pip install langchain langchain-community python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b09f9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.25'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9025ef1-ede0-4f3b-b8fe-1b9e348391ba",
   "metadata": {},
   "source": [
    "# OpenAI 모델 사용\n",
    "- https://platform.openai.com\n",
    "  \n",
    "## 결제\n",
    "1. 로그인 후 Billing 페이지로 이동.\n",
    "   - setting -> Billing\n",
    "  \n",
    "   ![openai_payment.png](figures/openai_payment.png)\n",
    "\n",
    "2. Payment methods 탭을 선택하고 카드를 등록한다. \n",
    "   \n",
    "   ![openai_payment2.png](figures/openai_payment2.png)\n",
    "\n",
    "   - 등록이 끝나면 최초 구매를 진행한다. $5 ~ $100 사이의 금액을 선택할 수 있다.\n",
    "   - 자동 충전을 설정하고 싶다면 automatic recharge 를 활성화 하고 아래 추가 설정에 입력한다. \n",
    "     - 자동 충전은 특정 금액 이하로 떨어지면 자동으로 충전한다. (**비활성화**) \n",
    "  \n",
    "   ![openai_payment3.png](figures/openai_payment3.png)\n",
    "   \n",
    "3. 수동으로 **추가 결제하기**\n",
    "   - Billing 페이지의 Overview에서 `Add to credit balance` 를 클릭한 뒤 금액을 입력하고 결제한다.\n",
    "\n",
    "## 사용량 확인\n",
    "- profile/설정 -> Usage 에서 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f99e0-d15b-4769-8d2c-b6e0a9a26237",
   "metadata": {},
   "source": [
    "## API Key 생성\n",
    "  \n",
    "![openai_create_apikey.png](figures/openai_create_apikey.png)\n",
    "\n",
    "- 로그인 -> Dashboard -> API Keys -> Create New Secreat Key\n",
    "> Settings -> API Keys\n",
    "\n",
    "## API Key 등록\n",
    "- 환경변수에 등록\n",
    "  - 변수이름: OPENAI_API_KEY\n",
    "  - 값: 생성된 키\n",
    "- dotenv를 이용해서 load\n",
    "  - Working directory에  `.env` 파일 생성하고 `OPENAI_API_KEY=생성된키` 추가한다.\n",
    "  - load_dotenv() 호출 하면 .env 파일에 있는 값을 읽은 뒤 환경변수로 등록한다.\n",
    "- **주의**\n",
    "  - 생성된 API Key는 노출되면 안된다.\n",
    "  - API Key가 저장된 파일(코드나 설정파일)이 github에 올라가 공개되서는 안된다.\n",
    "\n",
    "## 사용 비용 확인\n",
    "- settings -> Usage 에서 확인\n",
    "\n",
    "## OpenAI LLM 모델들\n",
    "-  OpenAI LLM 모델: https://platform.openai.com/docs/models\n",
    "-  모델별 가격: https://platform.openai.com/docs/pricing\n",
    "-  토큰사이즈 확인: https://platform.openai.com/tokenizer\n",
    "   -  1토큰: 영어 3\\~4글자 정도, 한글: 대략 1\\~2글자 정도\n",
    "   -  모델이 업데이트 되면서 토큰 사이즈도 조금씩 커지고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c5a12d-8e7d-4a29-ac91-edb62c56bfe7",
   "metadata": {},
   "source": [
    "## OpenAI 를 연동하기 위한 package 설치\n",
    "```bash\n",
    "pip install langchain-openai -qU\n",
    "```\n",
    "\n",
    "- OpenAI 자체 라이브러리 설치\n",
    "    - `pip install openai -qU`\n",
    "    - langchain-openai를 설치하면 같이 설치 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29772e79-1e31-421d-b5c1-625dc029c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-openai -qU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a486e5a1-6c59-499d-a275-41060c8b0a8a",
   "metadata": {},
   "source": [
    "## OpenAI Library 를 이용한 API 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c9bb769-eb14-43d5-9c26-98bb77153b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "flag = load_dotenv() \n",
    "print(flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "851be9d2-58e5-464b-87cb-52f09a9a146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()  # 환경변수에 OPENAI_API_KEY 이름으로 등록되어있으면 apikey는 생략.\n",
    "message = [\n",
    "    {\n",
    "        \"role\":\"user\",  # 누가 말하고 있는지 (발화자 - user, assistant, system)\n",
    "        \"content\":\"OpenAI의 LLM 모델은 무엇이 있나요?\" # 질의내용 -> 프롬프트.\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\", # 요청할 gpt 모델 종류\n",
    "    messages=message\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cb3d729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BetkqklNmW7r0VeOEAb7WtHEMvPCJ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='OpenAI의 LLM(대형 언어 모델)에는 여러 버전이 있으며, 주요 모델은 다음과 같습니다:\\n\\n1. **GPT-3**: Generative Pre-trained Transformer 3는 2020년에 출시된 모델로, 다양한 자연어 처리 작업에서 뛰어난 성능을 보여주었습니다.\\n\\n2. **GPT-3.5**: GPT-3의 후속 모델로, 더 향상된 성능과 다양한 기능을 지원합니다. \\n\\n3. **ChatGPT**: 사용자가 대화 형태로 질문을 하고 응답을 받을 수 있도록 특화된 모델입니다. GPT-3.5와 같은 기술 기반으로 하며, 사용자와의 상호작용을 보다 원활하게 하기 위해 튜닝되었습니다.\\n\\n4. **GPT-4**: 2023년에 출시된 모델로, GPT-3.5보다 더 높은 성능을 제공하며, 더 많은 데이터를 기반으로 학습되었습니다.\\n\\n이 외에도 OpenAI는 특정 분야나 목적에 맞춰 튜닝된 여러 가지 모델을 개발하고 있습니다. OpenAI의 LLM 모델들은 주로 자연어 이해(NLU), 자연어 생성(NLG), 질문 응답(QA) 등 다양한 작업에서 사용됩니다.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1749087140, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_62a23a81ef', usage=CompletionUsage(completion_tokens=258, prompt_tokens=19, total_tokens=277, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05c5d587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI의 LLM(대형 언어 모델)에는 여러 버전이 있으며, 주요 모델은 다음과 같습니다:\n",
      "\n",
      "1. **GPT-3**: Generative Pre-trained Transformer 3는 2020년에 출시된 모델로, 다양한 자연어 처리 작업에서 뛰어난 성능을 보여주었습니다.\n",
      "\n",
      "2. **GPT-3.5**: GPT-3의 후속 모델로, 더 향상된 성능과 다양한 기능을 지원합니다. \n",
      "\n",
      "3. **ChatGPT**: 사용자가 대화 형태로 질문을 하고 응답을 받을 수 있도록 특화된 모델입니다. GPT-3.5와 같은 기술 기반으로 하며, 사용자와의 상호작용을 보다 원활하게 하기 위해 튜닝되었습니다.\n",
      "\n",
      "4. **GPT-4**: 2023년에 출시된 모델로, GPT-3.5보다 더 높은 성능을 제공하며, 더 많은 데이터를 기반으로 학습되었습니다.\n",
      "\n",
      "이 외에도 OpenAI는 특정 분야나 목적에 맞춰 튜닝된 여러 가지 모델을 개발하고 있습니다. OpenAI의 LLM 모델들은 주로 자연어 이해(NLU), 자연어 생성(NLG), 질문 응답(QA) 등 다양한 작업에서 사용됩니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "972082ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "OpenAI의 LLM(대형 언어 모델)에는 여러 버전이 있으며, 주요 모델은 다음과 같습니다:\n",
       "\n",
       "1. **GPT-3**: Generative Pre-trained Transformer 3는 2020년에 출시된 모델로, 다양한 자연어 처리 작업에서 뛰어난 성능을 보여주었습니다.\n",
       "\n",
       "2. **GPT-3.5**: GPT-3의 후속 모델로, 더 향상된 성능과 다양한 기능을 지원합니다. \n",
       "\n",
       "3. **ChatGPT**: 사용자가 대화 형태로 질문을 하고 응답을 받을 수 있도록 특화된 모델입니다. GPT-3.5와 같은 기술 기반으로 하며, 사용자와의 상호작용을 보다 원활하게 하기 위해 튜닝되었습니다.\n",
       "\n",
       "4. **GPT-4**: 2023년에 출시된 모델로, GPT-3.5보다 더 높은 성능을 제공하며, 더 많은 데이터를 기반으로 학습되었습니다.\n",
       "\n",
       "이 외에도 OpenAI는 특정 분야나 목적에 맞춰 튜닝된 여러 가지 모델을 개발하고 있습니다. OpenAI의 LLM 모델들은 주로 자연어 이해(NLU), 자연어 생성(NLG), 질문 응답(QA) 등 다양한 작업에서 사용됩니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77416bf-be44-421b-bed1-c1da8512e983",
   "metadata": {},
   "source": [
    "## Langchain을 이용한 OpenAI API 호출\n",
    "\n",
    "- **ChatOpenAI**\n",
    "    - chat (대화-채팅) 기반 모델 model.\n",
    "    - Default 로 gpt-3.5-turbo 사용\n",
    "    - llm 전달 입력과 llm 응답 출력 타입:  Message\n",
    "> - **OpenAI**\n",
    ">     - 문장 완성 모델. (text completion) model\n",
    ">     - Default로 gpt-3.5-turbo-instruct 사용\n",
    ">       - instruct 모델만 사용가능\n",
    ">     - llm전달 입력과 llm 응답 출력 타입: str\n",
    "- Initializer 주요 파라미터\n",
    "    -  **temperature**\n",
    "        -  llm 모델의 출력 무작위성을 지정한다. \n",
    "        -  0 ~ 2 사이 실수를 설정하며 클 수록 무작위성이 커진다. 기본값: 0.7\n",
    "        -  정확한 답변을 얻어야 하는 경우 작은 값을 창작을 해야 하는 경우 큰 값을 지정한다.\n",
    "    -  **model_name**\n",
    "        -  사용할 openai 모델 지정\n",
    "    - **max_tokens**:\n",
    "        - llm 모델이 응답할 최대 token 수.\n",
    "    - **api_key**\n",
    "        - OpenAI API key를 직접 입력해 생성시 사용.\n",
    "        - API key가 환경변수에 설정 되있으면 생략한다. \n",
    "-  메소드\n",
    "    - **`invoke(message)`** : LLM에 질의 메세지를 전달하며 LLM의 응답을 반환한다.\n",
    "> - **Message**\n",
    ">     - Langchain 다양한 상황과 작업 마다 다양한 값들로 구성된 입출력 데이터를 만든다. \n",
    ">     - Langchain은 그 상황들에 맞는 다양한 Message 클래스를 제공한다. 이것을 이용하면 특정 작업에 적합한 입력값을 설정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "124af8a3-d624-4399-8062-35a6d60c63b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4de19fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "response = model.invoke(\"OpenAI LLM 모델 종류를 알려줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83d0668a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='OpenAI에서는 여러 종류의 LLM(대형 언어 모델)을 개발하였습니다. 주요 모델은 다음과 같습니다:\\n\\n1. **GPT-3**: 1750억 개의 파라미터를 가지고 있는 모델로, 다양한 자연어 처리 작업에 뛰어난 성능을 보여줍니다.\\n\\n2. **GPT-3.5**: GPT-3의 후속 모델로 더 나은 성능과 정확성을 제공합니다. ChatGPT라는 형태로 사용자와의 대화에 최적화되어 있습니다.\\n\\n3. **GPT-4**: GPT-3.5보다 더 발전된 모델로, 다양한 작업에서 더욱 향상된 이해력과 생성 능력을 가지고 있습니다.\\n\\n이 외에도 OpenAI는 특정 용도에 맞춰 개발된 다양한 연구 모델과 전용 모델을 보유하고 있습니다. 또한, OpenAI는 지속적으로 연구 및 개발을 진행하여 새로운 모델을 추가하거나 기존 모델을 개선하고 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 199, 'prompt_tokens': 17, 'total_tokens': 216, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'id': 'chatcmpl-Betu6WqiHctOw3Q2NyxHLDukLKT4w', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--85915ae3-7756-4b6f-9883-01b499a2f687-0', usage_metadata={'input_tokens': 17, 'output_tokens': 199, 'total_tokens': 216, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "974d734e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI에서는 여러 종류의 LLM(대형 언어 모델)을 개발하였습니다. 주요 모델은 다음과 같습니다:\n",
      "\n",
      "1. **GPT-3**: 1750억 개의 파라미터를 가지고 있는 모델로, 다양한 자연어 처리 작업에 뛰어난 성능을 보여줍니다.\n",
      "\n",
      "2. **GPT-3.5**: GPT-3의 후속 모델로 더 나은 성능과 정확성을 제공합니다. ChatGPT라는 형태로 사용자와의 대화에 최적화되어 있습니다.\n",
      "\n",
      "3. **GPT-4**: GPT-3.5보다 더 발전된 모델로, 다양한 작업에서 더욱 향상된 이해력과 생성 능력을 가지고 있습니다.\n",
      "\n",
      "이 외에도 OpenAI는 특정 용도에 맞춰 개발된 다양한 연구 모델과 전용 모델을 보유하고 있습니다. 또한, OpenAI는 지속적으로 연구 및 개발을 진행하여 새로운 모델을 추가하거나 기존 모델을 개선하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fde0555c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='저는 인공지능 언어 모델이기 때문에 이름은 없습니다. 하지만 필요하신 정보를 제공해드리거나 질문에 답변해드릴 수 있습니다. 어떻게 도와드릴까요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 13, 'total_tokens': 55, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'id': 'chatcmpl-Betw8AZUrqAL4QIfzMTzbbHJzUg22', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--cf5465e2-513a-438e-9204-1563c5ede397-0', usage_metadata={'input_tokens': 13, 'output_tokens': 42, 'total_tokens': 55, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"너 이름이 뭐니?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3964571",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", # gpt-4.1-mini, gpt-4.1\n",
    "    temperature=1,    # 무작위성을 설정. 0 이상의 실수 설정(0 ~ 1). 클 수록 무작위성이 커짐.\n",
    "    # max_tokens=100,   # 응답 토큰 수 제한.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1948856-b1e4-4da0-9311-11f798b66a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "태양계를 구성하는 행성들의 이름을 태양에서 가까운 순서대로 알려줘.\n",
    "\n",
    "<답변형식>\n",
    "목록형식으로 답변해줘.\n",
    "- 한글이름(영어이름): 행성에 대한 간단한 설명\n",
    "\"\"\"\n",
    "res = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afed81fb-5249-4bd3-91b0-4f86a0a75f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 수성(Mercury): 태양에 가장 가까운 행성으로, 대기가 거의 없어 낮과 밤의 온도 차이가 매우 큽니다.\n",
      "- 금성(Venus): 지구와 비슷한 크기를 가진 행성이지만, 두꺼운 이산화탄소 대기로 인해 극심한 온실 효과가 발생합니다.\n",
      "- 지구(Earth): 생명체가 존재하는 유일한 행성으로, 물이 액체 상태로 존재할 수 있는 조건을 갖추고 있습니다.\n",
      "- 화성(Mars): 붉은 색을 띠는 행성으로, 과거에 물이 존재했을 가능성이 있으며, 현재 탐사 missions이 활발히 진행되고 있습니다.\n",
      "- 목성(Jupiter): 태양계에서 가장 큰 행성으로, 가스 거인으로 분류되며, 강력한 자기장을 가지고 있습니다.\n",
      "- 토성(Saturn): 아름다운 고리로 유명한 가스 거인으로, 많은 위성을 가지고 있습니다.\n",
      "- 천왕성(Uranus): 독특하게도 옆으로 기울어진 축을 가지고 있는 가스 거인으로, 푸른색의 대기를 가지고 있습니다.\n",
      "- 해왕성(Neptune): 태양계에서 가장 먼 행성으로, 강한 바람과 대기에서의 푸른색이 특징입니다.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "312e426f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 수성(Mercury): 태양에 가장 가까운 행성으로, 대기가 거의 없어 낮과 밤의 온도 차이가 매우 큽니다.\n",
      "- 금성(Venus): 지구와 비슷한 크기를 가진 행성이지만, 두꺼운 이산화탄소 대기로 인해 극심한 온실 효과가 발생합니다.\n",
      "- 지구(Earth): 생명체가 존재하는 유일한 행성으로, 물이 액체 상태로 존재할 수 있는 조건을 갖추고 있습니다.\n",
      "- 화성(Mars): 붉은 색을 띠는 행성으로, 과거에 물이 존재했을 가능성이 있으며, 현재 탐사 missions이 활발히 진행되고 있습니다.\n",
      "- 목성(Jupiter): 태양계에서 가장 큰 행성으로, 거대한 가스 행성이며, 수많은 위성을 가지고 있습니다.\n",
      "- 토성(Saturn): 아름다운 고리로 유명한 행성으로, 목성과 마찬가지로 가스 행성입니다.\n",
      "- 천왕성(Uranus): 독특하게도 옆으로 누워 있는 회전축을 가진 행성으로, 푸른색의 대기를 가지고 있습니다.\n",
      "- 해왕성(Neptune): 태양계에서 가장 먼 행성으로, 강한 바람과 대기에서 발생하는 폭풍이 특징입니다.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee485861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 수성(Mercury): 태양계에서 가장 작은 행성이자 태양에 가장 가까운 행성으로, 대기가 거의 없어 낮과 밤의 온도 차이가 매우 큽니다.\n",
      "- 금성(Venus): 지구와 비슷한 크기를 가지고 있지만 매우 두꺼운 대기와 강한 온실 효과로 인해 극도로 뜨거운 환경을 가집니다.\n",
      "- 지구(Earth): 생명체가 존재하는 유일한 행성으로, 물이 액체 상태로 존재할 수 있는 적절한 온도를 유지하고 있습니다.\n",
      "- 화성(Mars): '붉은 행성'으로 알려져 있으며, 물의 흔적이 발견되었고, 과거에 생명체가 존재했을 가능성이 제기되고 있습니다.\n",
      "- 목성(Jupiter): 태양계에서 가장 큰 행성으로, 강력한 자기장과 수많은 위성을 가지고 있으며, 가장 유명한 대적점이 있습니다.\n",
      "- 토성(Saturn): 특징적인 고리로 유명하며, 많은 위성을 가지고 있는 행성입니다. 주로 가스와 액체로 구성되어 있습니다.\n",
      "- 천왕성(Uranus): 독특한 축으로 회전하여 '측면으로 굴러가는' 행성으로, 푸른색의 대기는 메탄에 의한 것입니다.\n",
      "- 해왕성(Neptune): 태양계에서 가장 먼 행성으로, 강력한 바람과 매우 어두운 대기를 가지고 있으며, 파란색을 띱니다.\n"
     ]
    }
   ],
   "source": [
    "print(res.content) # temperature: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4fbfb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채팅형태 프롬프트 - role과 내용을 묶어서 전달.\n",
    "prompt = [\n",
    "    (\"user\", \"막걸리 제조법을 알려줘.\"), # (\"role\", \"대화내용\")\n",
    "]\n",
    "res = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8c00d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "막걸리는 전통 한국 주류로, 쌀과 물, 누룩을 주재료로 사용하여 발효시켜 만듭니다. 아래는 간단한 막걸리 제조법입니다.\n",
      "\n",
      "### 재료\n",
      "1. **쌀**: 500g (일반 쌀 또는 찹쌀 사용 가능)\n",
      "2. **물**: 약 1.5리터\n",
      "3. **누룩**: 100g (막걸리 누룩 또는 발효 누룩)\n",
      "4. **설탕**: 선택적으로 약간\n",
      "\n",
      "### 도구\n",
      "- 큰 그릇\n",
      "- 체\n",
      "- 발효 용기 (유리병 또는 플라스틱 용기)\n",
      "- 깨끗한 주걱 또는 나무 숟가락\n",
      "- 거즈 또는 깨끗한 천\n",
      "\n",
      "### 제조 과정\n",
      "\n",
      "1. **쌀 씻기**: 쌀을 여러 번 씻어 찬물에 담가 2-3시간 정도 불립니다. 불린 쌀은 물기를 빼줍니다.\n",
      "\n",
      "2. **쌀 찌기**: 찐 솥에 쌀을 넣고 물을 약간 넣어 쪄줍니다. 약 30-40분 동안 쪄서 찐 쌀을 만듭니다.\n",
      "\n",
      "3. **식히기**: 쪄진 쌀을 넓은 용기에 담아 실온에서 식히세요. 약 30도 정도로 식혀야 합니다.\n",
      "\n",
      "4. **누룩 혼합**: 식혀진 쌀에 누룩을 고루 뿌리고, 물을 약간 더 부어 잘 섞어줍니다. 이때 설탕을 추가할 수도 있습니다.\n",
      "\n",
      "5. **발효**: 준비한 혼합물을 깨끗한 발효 용기에 담고, 위에 거즈나 천으로 덮어줍니다. 실온에서 1-2주 정도 발효시킵니다. (온도에 따라 발효 시간 조절)\n",
      "\n",
      "6. **체 걸러내기**: 발효가 끝나면 거즈나 체를 사용해 발효된 막걸리를 걸러냅니다.\n",
      "\n",
      "7. **병에 담기**: 걸러진 막걸리를 깨끗한 유리병에 담고 냉장 보관합니다.\n",
      "\n",
      "8. **숙성**: 몇 일간 숙성시키면 맛이 더 좋아집니다.\n",
      "\n",
      "막걸리는 신선하게 마시는 것이 가장 맛있으며, 여러 가지 재료와 조화를 이뤄 다양한 맛을 느낄 수 있습니다. 즐겁게 만들어 보세요!\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "839f3af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 567,\n",
       "  'prompt_tokens': 16,\n",
       "  'total_tokens': 583,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-4o-mini-2024-07-18',\n",
       " 'system_fingerprint': 'fp_34a54ae93c',\n",
       " 'id': 'chatcmpl-BeuWyYLxJTupnHGjuYVTtcYRpC49T',\n",
       " 'service_tier': 'default',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71861354-39b6-4a7d-842d-8939d3a3e5bc",
   "metadata": {},
   "source": [
    "# Hugging Face 모델 사용\n",
    "\n",
    "## Local 에 설치된 모델 사용\n",
    "- HuggingFacePipeline 에 Model id를 전달해 Model객체를 생성한다.\n",
    "- huggingface transformers 라이브러리를 이용해 model을 생성 한 뒤 HuggingFacePipeline 에 넣어 생성한다.\n",
    "- 모델이 local에 없는 경우 다운로드 받는다.\n",
    "\n",
    "### HuggingFace 모델을 사용하기 위한 package 설치\n",
    "```bash\n",
    "pip install langchain-huggingface -qU\n",
    "```\n",
    "- nvidia GPU가 있는 경우 `torch cuda`  버전을 먼저 설치하고 `langchain-huggingface`를 설치 해야 한다. 아니면 `torch cpu` 버전이 설치된다.\n",
    "- `transformers`, `tokenizers`, `huggingface-hub` 등 huggingface 관련 package들이 같이 설치된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ff21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "login(os.getenv('HUGGINGFACE_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a6c8929-6c61-4e85-95b9-40857fd6bff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96da5218a2d6450d8f7f5adbd2f0c030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\miniconda3\\envs\\lang_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--google--gemma-3-1b-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b1c45a91c04c7ea147b58951006a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89660acd801c4c008ba9340332246dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d8962bb3b14ecbbc5e821164a401e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22962d0ab69f42709d58495b850616d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7cc7a9a666a404180e7867e91316187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00741eaf811c46d1809a9c0fdc0ae79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa2ee713c634e358431182aad3ed73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"  \n",
    "# 1b: 파라미터수(10억), it-(instruction training - 질문/지시-응답 행식으로 파인튜닝한 모델)\n",
    "model_hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\":50}  # transformers.pipeline()의 설정을 하는 파라미터.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7562fc5a-6c5f-456b-aafd-ec8dbf9ef54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model_hf.invoke(\"한국의 수도는 어디인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aef895fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한국의 수도는 어디인가요?\\n\\n정답은 서울입니다.\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b653f31b-82aa-462f-a140-545b5c57d485",
   "metadata": {},
   "source": [
    "# Anthropic의 Claude 모델 사용\n",
    "\n",
    "- Anthropic사의 Claude 모델은 (성능 순으로) **Haiku, Sonnet, Opus** 세가지 모델이 있다.  \n",
    "- [Anthropic사 사이트](https://www.anthropic.com/)\n",
    "- [Claude 서비스 사이트](https://claude.ai)\n",
    "- API 가격: https://docs.anthropic.com/en/docs/about-claude/pricing\n",
    "- Langchain으로 Anthropic claude 모델 사용: https://python.langchain.com/docs/integrations/chat/anthropic/\n",
    "\n",
    "## API Key 발급받기\n",
    "1. https://console.anthropic.com/ 이동 후 가입한다.\n",
    "2. 로그인 하면 Dashboard로 이동한다. Dashbord에서 `Get API Keys`를 클릭해 이동한다.\n",
    "\n",
    "![anthropic_apikey1.png](figures/anthropic_apikey1.png)\n",
    "\n",
    "1. Create key 클릭해서 API Key를 생성한다.\n",
    "\n",
    "2. 생성된 API Key를 복사한 뒤 저장. (다시 볼 수 없다.)\n",
    "   - 환경변수에 등록\n",
    "      - 변수이름: ANTHROPIC_API_KEY\n",
    "      - 값: 생성된 키\n",
    "\n",
    "## 결제 정보 등록 및 결제 (최소 $5)\n",
    "   - Settings -> Billing \n",
    "  \n",
    "![anthropic_apikey3.png](figures/anthropic_apikey3.png)\n",
    "  - 설문조사 후 카드 등록한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8174de-b6a1-423b-8cce-271917ae8dc6",
   "metadata": {},
   "source": [
    "## Anthropic의 Claude 모델 사용\n",
    "- 모델 확인: https://docs.anthropic.com/en/docs/about-claude/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8193ce-fa8b-4208-a7d9-af794044e61c",
   "metadata": {},
   "source": [
    "### Claude 모델 사용을 위한 package 설치\n",
    "\n",
    "```bash\n",
    "pip install langchain-anthropic -qU\n",
    "```\n",
    "- `anthropic`package도 같이 설치 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3a23f-c491-4245-9013-83c5706fd4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0298545f-64b9-41aa-a375-7296030bb108",
   "metadata": {},
   "source": [
    "### Langchain-antropic 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba755283-5d24-464d-8c81-21d496100256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic# , Anthropic 지원하는 모델이 다른 것 같다.\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model=\"claude-sonnet-4-20250514\"\n",
    "llm = ChatAnthropic(\n",
    "    model=model,\n",
    "    temperature=0.2,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "result = llm.invoke(\"Anthropic의 LLM 모델은 어떤 것이 있는지 알려주고 간단한 설명도 부탁해.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07811e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Anthropic의 주요 LLM 모델들을 소개해드릴게요:\\n\\n## **Claude 3 시리즈 (최신)**\\n\\n### **Claude 3 Opus**\\n- 가장 강력한 플래그십 모델\\n- 복잡한 추론, 창작, 코딩 등 고난도 작업에 특화\\n- 높은 성능이지만 상대적으로 느리고 비용이 높음\\n\\n### **Claude 3 Sonnet**\\n- 성능과 속도의 균형을 맞춘 모델\\n- 일반적인 업무용 작업에 적합\\n- 가성비가 좋은 중간급 모델\\n\\n### **Claude 3 Haiku**\\n- 가장 빠르고 경량화된 모델\\n- 간단한 질답, 요약 등 기본적인 작업에 최적화\\n- 저비용, 고속 처리가 장점\\n\\n## **이전 세대**\\n\\n### **Claude 2/2.1**\\n- Claude 3 이전의 주력 모델\\n- 긴 문맥 처리 능력이 특징\\n- 현재는 Claude 3로 대부분 대체됨\\n\\n### **Claude Instant**\\n- Claude 2의 경량 버전\\n- 빠른 응답이 필요한 용도로 사용\\n\\n**특징**: 모든 Claude 모델은 안전성과 유용성을 중시하며, Constitutional AI 기법으로 훈련되어 도움이 되고 무해한 응답을 제공하도록 설계되었습니다.', additional_kwargs={}, response_metadata={'id': 'msg_01A6T4PGdtuUcrnPdPHVsR37', 'model': 'claude-sonnet-4-20250514', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 49, 'output_tokens': 462, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-sonnet-4-20250514'}, id='run--d9db3121-ba0e-4510-bd5b-580502690cec-0', usage_metadata={'input_tokens': 49, 'output_tokens': 462, 'total_tokens': 511, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebb8db47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic의 주요 LLM 모델들을 소개해드릴게요:\n",
      "\n",
      "## **Claude 3 시리즈 (최신)**\n",
      "\n",
      "### **Claude 3 Opus**\n",
      "- 가장 강력한 플래그십 모델\n",
      "- 복잡한 추론, 창작, 코딩 등 고난도 작업에 특화\n",
      "- 높은 성능이지만 상대적으로 느리고 비용이 높음\n",
      "\n",
      "### **Claude 3 Sonnet**\n",
      "- 성능과 속도의 균형을 맞춘 모델\n",
      "- 일반적인 업무용 작업에 적합\n",
      "- 가성비가 좋은 중간급 모델\n",
      "\n",
      "### **Claude 3 Haiku**\n",
      "- 가장 빠르고 경량화된 모델\n",
      "- 간단한 질답, 요약 등 기본적인 작업에 최적화\n",
      "- 저비용, 고속 처리가 장점\n",
      "\n",
      "## **이전 세대**\n",
      "\n",
      "### **Claude 2/2.1**\n",
      "- Claude 3 이전의 주력 모델\n",
      "- 긴 문맥 처리 능력이 특징\n",
      "- 현재는 Claude 3로 대부분 대체됨\n",
      "\n",
      "### **Claude Instant**\n",
      "- Claude 2의 경량 버전\n",
      "- 빠른 응답이 필요한 용도로 사용\n",
      "\n",
      "**특징**: 모든 Claude 모델은 안전성과 유용성을 중시하며, Constitutional AI 기법으로 훈련되어 도움이 되고 무해한 응답을 제공하도록 설계되었습니다.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161bc9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1684d4b-a5f9-4e24-8263-b30d997833b0",
   "metadata": {},
   "source": [
    "# Ollama 모델 사용\n",
    "\n",
    "Ollama는 로컬 환경에서 오픈소스 LLM을 쉽게 실행할 수 있도록 지원하는 플랫폼이다.\n",
    "\n",
    "- 주요특징\n",
    "\n",
    "  - **다양한 모델 지원**: Llama 3, Mistral, Phi 3 등 여러 오픈소스 LLM을 지원.\n",
    "  - **편리한 모델 설치 및 실행**: 간단한 명령어로 모델을 다운로드하고 실행할 수 있습니다.\n",
    "  - **운영체제 호환성**: macOS, Windows, Linux 등 다양한 운영체제에서 사용 가능하다.\n",
    "\n",
    "## 설치\n",
    "- https://ollama.com/download 에서 운영체제에 맞는 버전을 설치\n",
    "-  Windows 버전은 특별한 설정 없이 바로 install 실행하면 된다.\n",
    "\n",
    "## 모델 검색\n",
    "- https://ollama.com/search\n",
    "- 모델을 검색한 후 상세페이지로 이동하면 해당 모델을 실행할 수있는 명령어가 나온다.\n",
    "\n",
    "![ollama_down.png](figures/ollama_down.png)\n",
    "\n",
    "\n",
    "## 실행 명령어\n",
    "- `ollama pull 모델명`\n",
    "  - 모델을 다운로드 받는다. (다운로드만 받고 실행은 하지 않은다.)\n",
    "- `ollama run 모델명`\n",
    "  - 모델을 실행한다. \n",
    "  - 최초 실행시 모델을 다운로드 받는다.\n",
    "  - 명령프롬프트 상에서 `프롬프트`를 입력하면 모델의 응답을 받을 수 있다.\n",
    "\n",
    "## Python/Langchain API\n",
    "- ollama api\n",
    "  - https://github.com/ollama/ollama-python\n",
    "- langchain-ollama\n",
    "  - https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "- 설치\n",
    "  - `pip install langchain-ollama`\n",
    "  - `ollama` package도 같이 설치 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38928d8",
   "metadata": {},
   "source": [
    "GUI 환경: open-webui 설치 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c2254-7dd3-48b1-9de4-d25e278a2266",
   "metadata": {},
   "source": [
    "## Langchain-ollama 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0410c58d-5d0d-42a8-a2d6-0dfa0f7772f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"qwen3:0.6b\")\n",
    "response = model.invoke(\"대한민국의 수도의 이름은?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ffd2dbcc-d3f6-4e30-86dd-f33db4f69a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking for the capital of the Republic of Korea. I need to confirm the correct answer. Let me think... I know that Korea is a country, and its capital is Seoul. Wait, is there any possibility of confusion with another country? Like, in some languages, the capital might be translated differently? No, in Korean, the capital is definitely Seoul. I should make sure there's no other capital mentioned, and explain why Seoul is the correct answer. Also, check if there's any recent information that might change the capital, but I don't think so. So the final answer should be Seoul.\n",
      "</think>\n",
      "\n",
      "대한민국의 수도는 **세oul**입니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "832b1cd0-e201-478f-85ae-f7ab40c778dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking for the name of the Korean capital. Let me think. I know that Korea's capital is Seoul. But wait, maybe they're referring to the capital in another country? No, the question is specific to Korean capital. I should confirm that Seoul is indeed the capital. Also, maybe they want more information about it, like its location or history. Should I explain that Seoul is the capital and mention its significance? Yeah, that makes sense. Make sure the answer is clear and concise.\n",
      "</think>\n",
      "\n",
      "The Korean capital is **Seoul**. It is the political, economic, and cultural hub of the nation and is the capital of South Korea.\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(\"What is name of Korean capital?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ca5f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "588b1d44",
   "metadata": {},
   "source": [
    "# Gemini\n",
    "- 모델: https://ai.google.dev/gemini-api/docs/models?hl=ko\n",
    "- 가격정책: https://ai.google.dev/gemini-api/docs/pricing?hl=ko\n",
    "\n",
    "## API Key 생성\n",
    "\n",
    "1. https://aistudio.google.com/\n",
    "    - 연결 후 로그인(구글계정)\n",
    "2. Get API Key 클릭\n",
    "   \n",
    "    ![img](figures/gemini_api1.png)\n",
    "\n",
    "3. `API Key 만들기` 선택\n",
    "4. 프로젝트 선택 후 `기존 프로젝트에서 API 키 만들기` 선택\n",
    "\n",
    "## 환경변수\n",
    "- `GOOGLE_API_KEY` 환경변수에 생성된 API Key를 등록한다.\n",
    "## 설치\n",
    "- `pip install langchain_google_genai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9b2e8fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# load_dotenv(r\"c:\\env\\env.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3742cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-preview-05-20\"\n",
    ")\n",
    "\n",
    "response = model.invoke(\"gemini와 gemma의 차이는?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fa61bdfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Gemini와 Gemma는 모두 Google에서 개발한 AI 모델이지만, **목적, 규모, 접근 방식, 활용 범위**에서 중요한 차이가 있습니다.\\n\\n가장 핵심적인 차이점은 다음과 같습니다:\\n\\n*   **Gemini (제미나이):** Google의 **가장 강력하고 최첨단 대규모 AI 모델**입니다. 주로 Google의 핵심 제품(Bard/Gemini 챗봇, Duet AI 등)에 통합되거나, 복잡하고 광범위한 AI 기능을 제공하는 데 사용되는 **독점적인(Proprietary) 모델**입니다.\\n*   **Gemma (젬마):** Gemini 모델의 연구 및 기술을 기반으로 개발된 **오픈소스 경량 AI 모델군**입니다. 개발자, 연구자, 기업들이 자신들의 애플리케이션에 쉽게 통합하고, 수정하며, 배포할 수 있도록 설계된 **개방형 모델**입니다.\\n\\n자세한 차이점은 다음과 같습니다.\\n\\n---\\n\\n### 1. Gemini (제미나이)\\n\\n*   **유형:** Google의 플래그십(Flagship)이자 가장 강력한 **대규모 다중 모달(Multimodal) AI 모델**입니다.\\n*   **목표:**\\n    *   **최고 수준의 성능:** 복잡한 추론, 창의적인 콘텐츠 생성, 다양한 형태(텍스트, 이미지, 오디오, 비디오)의 정보 이해 및 생성에 특화되어 있습니다.\\n    *   **Google 제품 통합:** Bard/Gemini 챗봇, Google Cloud AI 서비스, Duet AI 등 Google의 다양한 제품 및 서비스의 핵심 기반이 됩니다.\\n*   **규모:** 매우 거대하며(수천억~1조 이상의 매개변수 추정), 다양한 크기(Nano, Pro, Ultra)로 제공됩니다.\\n*   **접근성:** 주로 Google의 제품을 통해 간접적으로 접근하거나, Google Cloud의 API를 통해 사용할 수 있습니다. 모델 자체의 가중치(Weights)는 공개되지 않습니다.\\n*   **활용:** 복잡한 문제 해결, 고급 대화형 AI, 대규모 데이터 분석 및 생성, 다중 모달 상호작용 등.\\n\\n### 2. Gemma (젬마)\\n\\n*   **유형:** Gemini 모델의 연구 및 기술을 기반으로 개발된 **경량(Lightweight) 오픈소스 AI 모델군**입니다.\\n*   **목표:**\\n    *   **개발자/연구자 접근성:** 개발자와 연구자들이 AI 모델을 자유롭게 활용, 수정, 배포하고, 그 위에 새로운 애플리케이션을 구축할 수 있도록 지원합니다.\\n    *   **효율성:** 비교적 작은 크기로 설계되어 개인 기기, 소규모 서버, 또는 온디바이스(On-device) 환경에서도 효율적으로 실행될 수 있습니다.\\n*   **규모:** 2B(20억) 및 7B(70억) 매개변수 버전 등, Gemini에 비해 훨씬 작은 크기로 제공됩니다.\\n*   **접근성:** 모델의 가중치(Weights)가 공개되어 누구나 다운로드하여 로컬 환경에서 실행하거나, 파인튜닝(Fine-tuning)할 수 있습니다. 상업적 용도로도 사용 가능합니다.\\n*   **활용:** 소규모 애플리케이션 개발, 온디바이스 AI, 특정 도메인에 대한 파인튜닝, AI 연구 및 교육, 커뮤니티 기반의 혁신 등.\\n\\n---\\n\\n### 요약 비교표\\n\\n| 특성       | Gemini (제미나이)                                   | Gemma (젬마)                                      |\\n| :--------- | :-------------------------------------------------- | :------------------------------------------------ |\\n| **개발사** | Google                                              | Google                                            |\\n| **유형**   | 최첨단 대규모 다중 모달 AI 모델 (독점)              | 경량 오픈소스 AI 모델 (모델군)                    |\\n| **목표**   | 광범위한 복잡한 AI 기능 제공, Google 제품 통합      | 개발자/연구자의 활용, 연구, 커뮤니티 기여 유도    |\\n| **크기**   | 매우 큼 (수천억~1조 이상 매개변수 추정)             | 작음 (2B, 7B 등)                                  |\\n| **접근성** | API, Google 제품 내 통합 (Bard/Gemini 등)           | 모델 가중치 직접 다운로드 및 로컬 실행 가능       |\\n| **활용**   | 복잡한 추론, 창의적 콘텐츠 생성, 다중 모달 작업     | 파인튜닝, 소규모 앱 개발, 연구, 온디바이스 AI     |\\n| **관계**   | Google의 핵심 AI 기술                               | Gemini 기술 기반으로 파생된, 개발자 친화적 모델   |\\n\\n---\\n\\n**쉽게 비유하자면:**\\n\\n*   **Gemini**는 Google이 최고급 재료와 비법으로 만든 **고급 레스토랑의 특별 요리**와 같습니다. 맛보려면 레스토랑에 가야 하고, 레시피는 공개되지 않습니다.\\n*   **Gemma**는 그 레스토랑의 셰프(Google)가 자신의 비법을 담아 **집에서 누구나 만들 수 있도록 간소화하여 공개한 레시피 키트**와 같습니다. 재료를 직접 사서 원하는 대로 변형하여 만들 수 있습니다.\\n\\n따라서 Gemini는 Google의 AI 역량을 보여주는 최첨단 모델이며, Gemma는 그 기술을 바탕으로 AI 생태계의 확산과 개발자 커뮤니티의 성장을 돕기 위해 공개된 모델이라고 할 수 있습니다.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'models/gemini-2.5-flash-preview-05-20', 'safety_ratings': []}, id='run--1a629b94-726e-4141-ad72-9ccbf458ed9e-0', usage_metadata={'input_tokens': 10, 'output_tokens': 1231, 'total_tokens': 2474, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1233}})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "85f903c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini와 Gemma는 모두 Google에서 개발한 AI 모델이지만, **목적, 규모, 접근 방식, 활용 범위**에서 중요한 차이가 있습니다.\n",
      "\n",
      "가장 핵심적인 차이점은 다음과 같습니다:\n",
      "\n",
      "*   **Gemini (제미나이):** Google의 **가장 강력하고 최첨단 대규모 AI 모델**입니다. 주로 Google의 핵심 제품(Bard/Gemini 챗봇, Duet AI 등)에 통합되거나, 복잡하고 광범위한 AI 기능을 제공하는 데 사용되는 **독점적인(Proprietary) 모델**입니다.\n",
      "*   **Gemma (젬마):** Gemini 모델의 연구 및 기술을 기반으로 개발된 **오픈소스 경량 AI 모델군**입니다. 개발자, 연구자, 기업들이 자신들의 애플리케이션에 쉽게 통합하고, 수정하며, 배포할 수 있도록 설계된 **개방형 모델**입니다.\n",
      "\n",
      "자세한 차이점은 다음과 같습니다.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Gemini (제미나이)\n",
      "\n",
      "*   **유형:** Google의 플래그십(Flagship)이자 가장 강력한 **대규모 다중 모달(Multimodal) AI 모델**입니다.\n",
      "*   **목표:**\n",
      "    *   **최고 수준의 성능:** 복잡한 추론, 창의적인 콘텐츠 생성, 다양한 형태(텍스트, 이미지, 오디오, 비디오)의 정보 이해 및 생성에 특화되어 있습니다.\n",
      "    *   **Google 제품 통합:** Bard/Gemini 챗봇, Google Cloud AI 서비스, Duet AI 등 Google의 다양한 제품 및 서비스의 핵심 기반이 됩니다.\n",
      "*   **규모:** 매우 거대하며(수천억~1조 이상의 매개변수 추정), 다양한 크기(Nano, Pro, Ultra)로 제공됩니다.\n",
      "*   **접근성:** 주로 Google의 제품을 통해 간접적으로 접근하거나, Google Cloud의 API를 통해 사용할 수 있습니다. 모델 자체의 가중치(Weights)는 공개되지 않습니다.\n",
      "*   **활용:** 복잡한 문제 해결, 고급 대화형 AI, 대규모 데이터 분석 및 생성, 다중 모달 상호작용 등.\n",
      "\n",
      "### 2. Gemma (젬마)\n",
      "\n",
      "*   **유형:** Gemini 모델의 연구 및 기술을 기반으로 개발된 **경량(Lightweight) 오픈소스 AI 모델군**입니다.\n",
      "*   **목표:**\n",
      "    *   **개발자/연구자 접근성:** 개발자와 연구자들이 AI 모델을 자유롭게 활용, 수정, 배포하고, 그 위에 새로운 애플리케이션을 구축할 수 있도록 지원합니다.\n",
      "    *   **효율성:** 비교적 작은 크기로 설계되어 개인 기기, 소규모 서버, 또는 온디바이스(On-device) 환경에서도 효율적으로 실행될 수 있습니다.\n",
      "*   **규모:** 2B(20억) 및 7B(70억) 매개변수 버전 등, Gemini에 비해 훨씬 작은 크기로 제공됩니다.\n",
      "*   **접근성:** 모델의 가중치(Weights)가 공개되어 누구나 다운로드하여 로컬 환경에서 실행하거나, 파인튜닝(Fine-tuning)할 수 있습니다. 상업적 용도로도 사용 가능합니다.\n",
      "*   **활용:** 소규모 애플리케이션 개발, 온디바이스 AI, 특정 도메인에 대한 파인튜닝, AI 연구 및 교육, 커뮤니티 기반의 혁신 등.\n",
      "\n",
      "---\n",
      "\n",
      "### 요약 비교표\n",
      "\n",
      "| 특성       | Gemini (제미나이)                                   | Gemma (젬마)                                      |\n",
      "| :--------- | :-------------------------------------------------- | :------------------------------------------------ |\n",
      "| **개발사** | Google                                              | Google                                            |\n",
      "| **유형**   | 최첨단 대규모 다중 모달 AI 모델 (독점)              | 경량 오픈소스 AI 모델 (모델군)                    |\n",
      "| **목표**   | 광범위한 복잡한 AI 기능 제공, Google 제품 통합      | 개발자/연구자의 활용, 연구, 커뮤니티 기여 유도    |\n",
      "| **크기**   | 매우 큼 (수천억~1조 이상 매개변수 추정)             | 작음 (2B, 7B 등)                                  |\n",
      "| **접근성** | API, Google 제품 내 통합 (Bard/Gemini 등)           | 모델 가중치 직접 다운로드 및 로컬 실행 가능       |\n",
      "| **활용**   | 복잡한 추론, 창의적 콘텐츠 생성, 다중 모달 작업     | 파인튜닝, 소규모 앱 개발, 연구, 온디바이스 AI     |\n",
      "| **관계**   | Google의 핵심 AI 기술                               | Gemini 기술 기반으로 파생된, 개발자 친화적 모델   |\n",
      "\n",
      "---\n",
      "\n",
      "**쉽게 비유하자면:**\n",
      "\n",
      "*   **Gemini**는 Google이 최고급 재료와 비법으로 만든 **고급 레스토랑의 특별 요리**와 같습니다. 맛보려면 레스토랑에 가야 하고, 레시피는 공개되지 않습니다.\n",
      "*   **Gemma**는 그 레스토랑의 셰프(Google)가 자신의 비법을 담아 **집에서 누구나 만들 수 있도록 간소화하여 공개한 레시피 키트**와 같습니다. 재료를 직접 사서 원하는 대로 변형하여 만들 수 있습니다.\n",
      "\n",
      "따라서 Gemini는 Google의 AI 역량을 보여주는 최첨단 모델이며, Gemma는 그 기술을 바탕으로 AI 생태계의 확산과 개발자 커뮤니티의 성장을 돕기 위해 공개된 모델이라고 할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "000ae8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Gemini와 Gemma는 모두 Google에서 개발한 AI 모델이지만, **목적, 규모, 접근 방식, 활용 범위**에서 중요한 차이가 있습니다.\n",
       "\n",
       "가장 핵심적인 차이점은 다음과 같습니다:\n",
       "\n",
       "*   **Gemini (제미나이):** Google의 **가장 강력하고 최첨단 대규모 AI 모델**입니다. 주로 Google의 핵심 제품(Bard/Gemini 챗봇, Duet AI 등)에 통합되거나, 복잡하고 광범위한 AI 기능을 제공하는 데 사용되는 **독점적인(Proprietary) 모델**입니다.\n",
       "*   **Gemma (젬마):** Gemini 모델의 연구 및 기술을 기반으로 개발된 **오픈소스 경량 AI 모델군**입니다. 개발자, 연구자, 기업들이 자신들의 애플리케이션에 쉽게 통합하고, 수정하며, 배포할 수 있도록 설계된 **개방형 모델**입니다.\n",
       "\n",
       "자세한 차이점은 다음과 같습니다.\n",
       "\n",
       "---\n",
       "\n",
       "### 1. Gemini (제미나이)\n",
       "\n",
       "*   **유형:** Google의 플래그십(Flagship)이자 가장 강력한 **대규모 다중 모달(Multimodal) AI 모델**입니다.\n",
       "*   **목표:**\n",
       "    *   **최고 수준의 성능:** 복잡한 추론, 창의적인 콘텐츠 생성, 다양한 형태(텍스트, 이미지, 오디오, 비디오)의 정보 이해 및 생성에 특화되어 있습니다.\n",
       "    *   **Google 제품 통합:** Bard/Gemini 챗봇, Google Cloud AI 서비스, Duet AI 등 Google의 다양한 제품 및 서비스의 핵심 기반이 됩니다.\n",
       "*   **규모:** 매우 거대하며(수천억~1조 이상의 매개변수 추정), 다양한 크기(Nano, Pro, Ultra)로 제공됩니다.\n",
       "*   **접근성:** 주로 Google의 제품을 통해 간접적으로 접근하거나, Google Cloud의 API를 통해 사용할 수 있습니다. 모델 자체의 가중치(Weights)는 공개되지 않습니다.\n",
       "*   **활용:** 복잡한 문제 해결, 고급 대화형 AI, 대규모 데이터 분석 및 생성, 다중 모달 상호작용 등.\n",
       "\n",
       "### 2. Gemma (젬마)\n",
       "\n",
       "*   **유형:** Gemini 모델의 연구 및 기술을 기반으로 개발된 **경량(Lightweight) 오픈소스 AI 모델군**입니다.\n",
       "*   **목표:**\n",
       "    *   **개발자/연구자 접근성:** 개발자와 연구자들이 AI 모델을 자유롭게 활용, 수정, 배포하고, 그 위에 새로운 애플리케이션을 구축할 수 있도록 지원합니다.\n",
       "    *   **효율성:** 비교적 작은 크기로 설계되어 개인 기기, 소규모 서버, 또는 온디바이스(On-device) 환경에서도 효율적으로 실행될 수 있습니다.\n",
       "*   **규모:** 2B(20억) 및 7B(70억) 매개변수 버전 등, Gemini에 비해 훨씬 작은 크기로 제공됩니다.\n",
       "*   **접근성:** 모델의 가중치(Weights)가 공개되어 누구나 다운로드하여 로컬 환경에서 실행하거나, 파인튜닝(Fine-tuning)할 수 있습니다. 상업적 용도로도 사용 가능합니다.\n",
       "*   **활용:** 소규모 애플리케이션 개발, 온디바이스 AI, 특정 도메인에 대한 파인튜닝, AI 연구 및 교육, 커뮤니티 기반의 혁신 등.\n",
       "\n",
       "---\n",
       "\n",
       "### 요약 비교표\n",
       "\n",
       "| 특성       | Gemini (제미나이)                                   | Gemma (젬마)                                      |\n",
       "| :--------- | :-------------------------------------------------- | :------------------------------------------------ |\n",
       "| **개발사** | Google                                              | Google                                            |\n",
       "| **유형**   | 최첨단 대규모 다중 모달 AI 모델 (독점)              | 경량 오픈소스 AI 모델 (모델군)                    |\n",
       "| **목표**   | 광범위한 복잡한 AI 기능 제공, Google 제품 통합      | 개발자/연구자의 활용, 연구, 커뮤니티 기여 유도    |\n",
       "| **크기**   | 매우 큼 (수천억~1조 이상 매개변수 추정)             | 작음 (2B, 7B 등)                                  |\n",
       "| **접근성** | API, Google 제품 내 통합 (Bard/Gemini 등)           | 모델 가중치 직접 다운로드 및 로컬 실행 가능       |\n",
       "| **활용**   | 복잡한 추론, 창의적 콘텐츠 생성, 다중 모달 작업     | 파인튜닝, 소규모 앱 개발, 연구, 온디바이스 AI     |\n",
       "| **관계**   | Google의 핵심 AI 기술                               | Gemini 기술 기반으로 파생된, 개발자 친화적 모델   |\n",
       "\n",
       "---\n",
       "\n",
       "**쉽게 비유하자면:**\n",
       "\n",
       "*   **Gemini**는 Google이 최고급 재료와 비법으로 만든 **고급 레스토랑의 특별 요리**와 같습니다. 맛보려면 레스토랑에 가야 하고, 레시피는 공개되지 않습니다.\n",
       "*   **Gemma**는 그 레스토랑의 셰프(Google)가 자신의 비법을 담아 **집에서 누구나 만들 수 있도록 간소화하여 공개한 레시피 키트**와 같습니다. 재료를 직접 사서 원하는 대로 변형하여 만들 수 있습니다.\n",
       "\n",
       "따라서 Gemini는 Google의 AI 역량을 보여주는 최첨단 모델이며, Gemma는 그 기술을 바탕으로 AI 생태계의 확산과 개발자 커뮤니티의 성장을 돕기 위해 공개된 모델이라고 할 수 있습니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
