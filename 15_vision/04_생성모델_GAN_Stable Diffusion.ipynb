{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN (Generative Adversarial Network-적대적 생성 네트워크)\n",
    "- 구글의 AI 연구원 Ian Goodfellow가 고안한 딥러닝을 이용한 생성모델로 [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)라는 논문으로 처음 발표 되었다.\n",
    "- **GAN은 단순한 구조임에도 꽤 사실적 이미지를 생성**할 수 있었기 때문에 빠르게 이미지 생성 모델의 대세가 되었다.\n",
    "- 이후 계속 발전 하면서 다양한 변형 모델들이 나오며 발전 하였으며, 이미지 생성 뿐 아니라 소리 생성에도 적용되면서 좋은 성능을 보여주었다.\n",
    "- 2021년 Diffusion 모델이 등장 하면서 최고의 자리에서는 내려왔지만 아직도 여러 곳에서 사용되고 있다.\n",
    "    - **Diffusion 모델**은 2015년 처음 제안된 모델이었으나 GAN에 비해 낮은 성능, 복잡한 학습 과정 그리고 느린 생성 속도등으로 주목을 받지 못하다 2019년 부터 단점을 개선한 연구들이 나오며 최근 **이미지 생성 분야의 대세 기술**로 자리잡게 되었다.\n",
    "      \n",
    "## GAN 개요\n",
    "### 이미지를 생성한 다는 것 \n",
    "- GAN은 이미지를 생성하는 모델이다. 여기서 생성 모델이란 **그럴 듯한 가짜를** 만들어내는 모델을 의미한다. \n",
    "- **\"그럴듯 하다\"** 의 수학적 의미\n",
    "    -  두 데이터가 비슷한 분포를 가진다는 것을 말한다.\n",
    "- 32 X 32 X 3 크기의 사진을 이용한 생성모델의 경우 총 변수가 3072개로 그 조합에 따라 다양한 형태가 나온다.\n",
    "    - 만약 사람 얼굴 이미지를 생성하려고 한다면, 사람 얼굴 이미지들의 pixcel 분포를 학습하여 이를 재현하는 것이 목표이다.\n",
    "\n",
    "![분포](figures/gan/distribution.png)\n",
    "\n",
    "$P_{data}(x)$: 실제 이미지의 분포, $P_{model}(x)$: 모델이 생성한 이미지의 분포"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversairal (적대적) - 두개의 모델을 적대적으로 경쟁시켜 두 모델을 성능을 높인다. \n",
    "- GAN은 두 개의 딥러닝 모델을 학습시킨다.\n",
    "  1. **생성자(Generator) 모델**\n",
    "      - 이미지를 생성하는 모델. (Fake Image)\n",
    "  3. **판별자(Discriminator) 모델**\n",
    "      - 입력된 이미지가 진짜(real)인지 가짜(fake) 인지 판별하는 모델.\n",
    "      - 생성자를 학습시키는 역할.\n",
    "- 학습시 이 두 모델을 서로 적대적(Adversarial)인 관계로 경쟁을 붙여 성능을 올린다.\n",
    "    - **생성자**는 Fake Image를 만들어 판별자를 속이고 **판별자**는 생성자가 만든 이미지를 가짜로 판별해 내도록 학습하는 것을 적대적으로 표현.\n",
    "- 학습이 잘 끝나면 **\"진짜와 구별이 안되는 가짜를 만드는 생성자(Generator) 모델을 만들 수 있다\"**  \n",
    "    - 생성자(Generator)를 사용해 이미지를 생성한다. \n",
    "- Ian Goodfellow는 논문에서 이 둘의 관계를 위조지폐범(Generator)와 경찰(Discriminator)로 설명\n",
    "    - 위조 지폐범은 경찰을 속이기 위해 점점 위조 기술을 발전시키고 경찰은 위조지폐를 찾기위해 위폐를 식별하는 기술을 발전 시킨다.\n",
    "    - 적대하는 상대를 이기기 위해 서로의 기술을 발전시키다보면 경찰은 위폐판별능력이, 위조 지폐범의 위폐 제조 기술이 완벽해질 것이다.\n",
    "      \n",
    "    ![개요](figures/gan/intro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generator와 Discriminator 네트워크\n",
    "- <span style='font-size:1.2em;font-weight:bold'>Generator</span> \n",
    "    - 입력: 랜덤한 숫자로 구성된 벡터(잡음). **Latent Vector 라고 한다.**\n",
    "    - 출력: 최대한 진짜 처럼 보이는 가짜 샘플.\n",
    "    - 목표: 훈련 데이터셋에 있는 샘플과 구별이 불가능한 가짜 샘플만들기.\n",
    "- <span style='font-size:1.2em;font-weight:bold'>Discriminator</span>\n",
    "    - 입력\n",
    "        1. 훈련데이터셋에 있는 진짜 샘플.\n",
    "        2. Generator가 생성한 가짜 샘플.\n",
    "    - 출력: 입력 샘플이 진짜일 예측 확률.\n",
    "    - 목표: 생성자가 만든 가짜 샘플과 훈련 데이터셋의 진짜 샘플을 구별하기."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련\n",
    "\n",
    "![train](figures/gan/train.png)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style='font-size:1.2em;font-weight:bold'>Train의 목적은 좋은 성능을 내는 Generator 모델을 학습시키는 것</span>\n",
    "- **Discriminator 훈련**\n",
    "    - 진짜 샘플과 Generator가 생성한 가짜 샘플을 잘 분류하도록 학습한다.\n",
    "    - 일반 classification 문제와 동일하다.\n",
    "- **Generator 훈련**\n",
    "    - Generator와 Discriminator로 구성된 GAN 모델을 학습시킨다.\n",
    "    - **Discriminator는 Frozen** 시키고 Generator의 파라미터들만 업데이트 되도록 한다.\n",
    "    1. Generator가 생성한 가짜 샘플을 생성한다.\n",
    "    2. 가짜 샘플의 label을 진짜로 하여 Discriminator가 판별하게 한다.\n",
    "        - Discriminator가 Generator가 만든 Fake 이미지를 진짜로 판별하면 Generator 입장에서는 **성공한** 것이므로 loss가 작아져야 한다.\n",
    "        - Discriminator가 Generator가 만든 Fake 이미지를 가짜로 판별하면 Generator 입장에서는 **실패한** 것이므로 loss가 켜져야 한다.\n",
    "    3. Loss에 대한 파라미터의 gradient를 계산하여 **Generator의 파라미터들을 업데이트 한다.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN 단점\n",
    "1. **학습 불안정**\n",
    "    - 학습 할 때 생성자와 판별자의 성능 균형을 맞추는 것이 매우 중요하다. 이 균형이 깨지면 학습이 불안 정해지고 좋은 결과를 얻기 어렵다.\n",
    "    - 그런데 이 균형을 맞춰 학습하는 것이 어렵다.\n",
    "2. **모드 붕괴(Mode Collapse)**\n",
    "    - 특정 샘플이나 스타일만 생성하는 현상으로, 다양성이 제한되는 문제가 발생한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion\n",
    "\n",
    "## Diffusion 모델의 개념\n",
    "- Diffusion 모델은 입력 이미지에 정규 분포의 노이즈(noise)를 여러 단계에 걸처 조금씩 추가하여 완전 노이즈 상태로 만든다.\n",
    "    - 노이즈가 점점 확산되는 방식이기 때문에 diffusion 모델이라고 한다.\n",
    "- 완전 노이즈 상태를 같은 횟수의 단계에 걸쳐 노이즈를 조금씩 제거하여 입력 이미지와 유사한(확률 분포) 이미지를 생성한다.\n",
    "![diffusion](figures/diffusion1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Diffusion Process\n",
    "- T 번의 단계에 걸쳐 가우시안분포(정규분포)를 따르는 난수의 노이즈(noise)를 추가한다.\n",
    "- T 번의 단계에서는 원래 이미지의 형태는 사라지고 완전한 노이즈로 변해버린다. 이때 노이즈도  가우시안 분포가 된다.\n",
    "  \n",
    "![img](figures/diffusion2_forward.png)\n",
    "\n",
    "![img](figures/diffusion3_forward.png)\n",
    "\n",
    "<small>가우시안 분포 난수의 노이즈가 추가되면서 결국 최종적으로 전체 픽셀의 분포가 가우시안 분포인 노이즈가 된다.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse Diffusion Process\n",
    "- 생성 된 노이즈를 T 단계에 걸쳐 다시 노이즈를 제거해 나간다.\n",
    "- 노이즈 제거에 UNet 모델을 사용한다.\n",
    "    - UNet 모델을 이용해 Reverse Diffusion Process(noise 제거)를 학습시킨다. \n",
    "\n",
    "![img](figures/diffusion4_reverse.png)\n",
    "\n",
    "![img](figures/diffusion5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기존 Diffusion 모델의 단점\n",
    "\n",
    "- 노이즈 추가와 제거 과정이 이미지의 픽셀 공간에서 작동 한다\n",
    "    - 이미지 자체에 노이즈를 추가하고 제거하는 과정을 진행함.\n",
    "    - 그로인해 엄청난 계산량과 메모리를 필요로 했고 이로 인해 생성속도가 너무 느렸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable  Diffusion\n",
    "- Stable Diffusion은 기존 Diffusion모델의 단점인 큰 메모리 사용과 느린 생성속도를 해결하기 위해 VAE를 이용해 이미지의 크기를 줄여서 noising/denoising process를 수행한다.\n",
    "    - **VAE**(Variational AutoEncoder)를 활용해 이미지를 픽셀 공간에서 더 작은 잠재 공간(Latent Space)으로 압축하여 이미지의 더 근본적인 의미론적 의미를 포착한다.\n",
    "    - VAE는 인코더와 디코더로 구성되며, 인코더는 512x512 픽셀 이미지를 64x64 크기의 잠재 공간으로 압축하고, 디코더는 이를 다시 풀사이즈 이미지로 복원한다 \n",
    "    - 이를 통해 적은 8GB 정도의 VRAM으로도 구동이 가능하여 일반사용자들에게도 사용할 수있게 하였다.\n",
    "- Denoising 과정(이미지 생성) 에 **텍스트 프롬프트 임베딩 벡터를 추가해** 사용자가 원하는 형태의 이미지가 나오도록 한다.\n",
    "\n",
    "\n",
    "## 주요 구성 요소\n",
    "\n",
    "**1. VAE (Variational AutoEncoder)**\n",
    "- 입력 이미지를 저차원의 잠재 공간(Latent Space, Latent Vector)으로 압축하고 복원하는 역할을 수행.\n",
    "- **인코더**가 입력 이미지를 의미론적 특징을 포함하는 잠재 벡터로 압축 한다. \n",
    "- 최종 단계에서 잠재 벡터를 다시 픽셀 공간의 이미지로 복원.\n",
    "    - 노이즈가 제거된 잠재 벡터를 **VAE 디코더**가 최종 이미지로 변환.\n",
    "- 잠재 공간에서 노이즈 확산 과정과 노이즈 제거과정이 진행되어 메모리 효율성 향상.\n",
    "\n",
    "**2. U-Net**\n",
    "- 역방향 확산의 노이즈 제거를 위한 핵심 모델.\n",
    "- ResNet 백본 구조를 기반으로 함.\n",
    "- Encoder-Decoder 구조를 활용해 이미지의 중요 정보를 보존하면서 점진적으로 노이즈를 제거.\n",
    "- Cross Attention 메커니즘을 통해 텍스트로 입력된 이미지 생성 조건을 반영한다.\n",
    "\n",
    "**3. 텍스트 인코더**\n",
    "- 텍스트와 이미지 간의 연관성을 학습한 CLIP 모델의 텍스트 인코더를 사용.\n",
    "- 입력된 텍스트를 벡터로 변환하여 U-Net에 조건으로 제공.\n",
    "- 텍스트로 guide한 이미지 생성을 가능하게 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습\n",
    "\n",
    "### VAE 학습\n",
    "\n",
    "- **인코더 학습**\n",
    "    - 원본 이미지를 저차원의 잠재 공간으로 압축하는 방법을 학습\n",
    "    - 512x512 크기의 이미지를 64x64 크기의 잠재 벡터로 압축\n",
    "    - 이미지의 의미론적 특징을 보존하면서 압축하도록 학습\n",
    "- **디코더 학습**\n",
    "    - 노이즈가 제거된 잠재 벡터를 다시 원본 이미지로 복원하는 방법을 학습\n",
    "    - 압축된 정보로부터 고품질 이미지를 생성할 수 있도록 최적화\n",
    "\n",
    "### U-Net 학습\n",
    "\n",
    "- **노이즈 제거 과정**\n",
    "    - 잠재 공간에서 점진적으로 노이즈를 제거하는 방법을 학습\n",
    "    - Encoder-Decoder 구조를 활용해 이미지의 중요 정보를 보존하며 노이즈 제거 한다\n",
    "    - scheduler을 통해 노이즈 제거 강도를 단계적으로 조절\n",
    "\n",
    "### 텍스트 인코더 학습\n",
    "\n",
    "- **CLIP 모델 활용**\n",
    "    - OpenAI의 CLIP 모델을 사용하여 텍스트와 이미지 간 연관성 학습\n",
    "    - 텍스트를 벡터로 변환하여 U-Net의 조건으로 제공\n",
    "    - Imagen의 영향으로 텍스트 인코더는 freeze 상태로 유지\n",
    "\n",
    "### Cross Attention 메커니즘\n",
    "\n",
    "- **조건 학습**\n",
    "    - U-Net 내부에서 텍스트 조건과 이미지 특징을 연결한다.\n",
    "    - 각 denoising 단계(reverse diffusion)에서 텍스트 정보를 이미지 생성에 반영\n",
    "    - 텍스트의 의미에 맞는 이미지 생성이 가능하도록 attention 가중치 학습\n",
    "\n",
    "이러한 컴포넌트들의 통합적인 학습을 통해 Stable Diffusion은 텍스트 프롬프트에 따른 고품질 이미지 생성이 가능해 진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stable_diffusion_train](figures/stable_diffusion_train.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론 \n",
    "\n",
    "### 입력 데이터\n",
    "- **프롬프트 텍스트 인코딩**\n",
    "    - CLIP 의 텍스트 인코더가 입력된 텍스트 프롬프트를 벡터로 변환한다.\n",
    "    - 텍스트의 의미적 특징을 추출하여 어떤 이미지를 생성할지 조건으로 활용한다.\n",
    "- **초기 노이즈 생성**\n",
    "    - 가우시안 분포를 따르는 random 값 노이즈로 구성된 초기 입력을 생성한다.\n",
    "    - 노이즈를 단계적으로 추가해서 만드는 것이 아니라 random 값으로 생성한다.\n",
    "\n",
    "### 이미지 생성\n",
    "- **노이즈 제거 프로세스**\n",
    "    - U-Net이 점진적으로 노이즈를 제거하며 이미지 형태를 구체화한다.\n",
    "    - Cross attention이 적용되어 텍스트 조건을 반영하여 원하는 이미지 특성이 적용된다.\n",
    "\n",
    "### 최종 이미지 생성\n",
    "\n",
    "- **이미지 복원**\n",
    "    - VAE 디코더가 노이즈가 제거된 잠재 벡터를 최종 픽셀 이미지로 변환한다.\n",
    "    - 고해상도의 완성된 이미지 출력."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고\n",
    "## VAE(Variational Autoencoder, 변분 오토인코더)\n",
    "\n",
    "- 딥러닝에서 사용되는 생성 모델 중 하나로, 데이터를 **압축**하여 잠재 표현(latent representation)을 학습하고, 이를 통해 **새로운 데이터 생성이나 데이터 복원, 노이즈 제거**등의 작업을 수행한다. \n",
    "- VAE는 크게 Encoder, Latent Space, Decoder로 구성된다.  \n",
    "    - Encoder는 데이터를 잠재 공간으로 변환하고, Decoder는 이를 다시 원본 데이터로 복원하는 과정을 수행한다.\n",
    "### Encoder\n",
    "- 입력 데이터를 잠재 벡터(Latent Vector)로 변환하는 역할을 한다.  \n",
    "- **잠재 벡터**는 입력 데이터 $x$를 압축한 중요한 정보(Feature Vector)이다.  \n",
    "- 일반적인 오토인코더의 Encoder는 입력 데이터를 하나의 고정된 잠재 벡터로 변환한다. 하지만 VAE의 Encoder는 하나의 고정된 값이 아니라 \"확률 분포\"를 출력한다.\n",
    "  - 이는 오토인코더가 \"이 이미지는 정확히 이 지점에 위치한다\" 라면 변분오토인코더는  \"이 이미지는 이 영역 어디쯤에 위치할 가능성이 높다\"는 방식으로 표현하는 것으로 볼 수있다.\n",
    "- Encoder는 잠재 벡터가 어떤 분포를 따르는지 추측해야 하지만, 이를 정확히 계산하기 어렵기 때문에, **근사 분포**를 학습하는 방식을 사용한다.  \n",
    "  - 예를 들어, 근사 분포를 **정규 분포**로 선택하면, Encoder는 입력 데이터의 분포를 나타내는 **평균($\\mu$)과 표준편차($\\sigma$)를** 학습한다.\n",
    "    - 인코더 모델은 Fully Connected Layer를 두개를 사용해서 평균, 표준편차 출력하도록 한다.\n",
    "  - 학습한 평균과 표준편차를 이용해 표준 정규 분포에서 샘플링한 값을 변환하여 잠재 벡터를 생성한다.  ($\\mu + \\sigma \\times \\epsilon$)\n",
    "    - sampling한 값을 이용하므로 같은 입력에 대해서 실행시마다 다른 값이 나온다.\n",
    "  - 이 과정을 통해 Encoder는 **입력 데이터를 잠재 공간(latent space)으로 매핑**한다.  \n",
    "\n",
    "### Decoder\n",
    "- Encoder가 생성한 잠재 벡터를 입력으로 받아 원래 데이터로 복원하는 역할을 한다.  \n",
    "- Decoder는 학습한 잠재 벡터의 정보를 바탕으로, 입력 데이터와 최대한 비슷한 데이터를 재구성한다.  \n",
    "\n",
    "\n",
    "![VAE](figures/vae.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet\n",
    "- 2015년도에 발표한 의료영상 segmentation(이미지 세분화)을 위한 Encoder-Decoder 기반의 CNN 모델.\n",
    "- Downsampling과 Upsampling이 연결된 U 자 모양이어서 U-Net이란 이름이 붙여짐.\n",
    "- https://arxiv.org/abs/1505.04597\n",
    "\n",
    "![u-net architecture](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qNdglJ1ORP3Gq77MmBLhHQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델구조\n",
    "- U-Net 모델은 다음 세가지 구조로 나눠진다.\n",
    "    1. Contracting Path (수축경로)\n",
    "        - 점진적으로 넓은 범위의 이미지를 참고하여 Context 정보(특징 정보)를 추출한다.\n",
    "    2. Expanding Path (확장 경로)\n",
    "        - Context 정보를 픽셀의 위치정보와 결합해 각 픽셀이 어떤 객체에 속하는지 구분한다.\n",
    "    3. Bottlenet \n",
    "        - Contracting path에서 Expanding path로 전환되는 구간.\n",
    "- Convolution 연산과정에서 패딩을 사용하지 않기 때문에 모델의 output size가 input size보다 작다. \n",
    "    - input: 572 X 572 X 3\n",
    "    - output: 388 X 388 X (class개수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP(Contrastive Language-Image Pre-training) 모델\n",
    "\n",
    "- CLIP은 2021년 OpenAI에서 개발한 **멀티 모달(Multi modal)** 딥러닝 모델이다.\n",
    "  > 멀티 모달(Multimodal)이란 텍스트, 이미지, 음성 등 서로 다른 형태의 데이터를 동시에 처리하고 이해할 수 있는 방식을 말한다.\n",
    "- CLIP모델의 핵심은 이미지와 텍스트를 동시에 이해하고, 두 정보 간의 관계성을 학습한다는 점이다.\n",
    "  - 사람이 그림을 보고 \"이건 고양이야\"라고 말하는 것처럼, 이미지와 텍스트 사이의 관계를 파악할 수 있도록 학습한 모델이다.\n",
    "- CLIP는 인터넷에서 수집한 4억개의 이미지와 그 이미지에 대한 설명 텍스트 쌍(pair)로 한 학습 데이터를 학습했다.\n",
    "- 핵심학습 방법은 **대조 학습(Contrastive Learning)** 이다.\n",
    "- **대조학습**\n",
    "    - 대조 학습은 자기지도학습 방법 중 하나로, 같은 쌍의 데이터는 가까워지도록, 다른 쌍의 데이터는 멀어지도록 학습하는 방식이다\n",
    "    - 이를 통해서  고양이 사진과 \"고양이\" 텍스트는 가깝게, 고양이 사진과 \"자동차\" 텍스트는 멀리 배치되도록 조정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP 구조\n",
    "- 다음 세가지 부분으로 구성되어있다.\n",
    "1. **Image Encoder**\n",
    "   - ResNet 시리즈 또는 Vision Transformer(ViT)를 사용한다.\n",
    "   - 입력 이미지의 시각적 특성을 추출한 고차원 벡터를 반환한다.\n",
    "2. **Text Encoder**\n",
    "   - Transformer 기반 모델을 사용한다.\n",
    "   - 입력 텍스트의 특성을 추출한 고정길이 벡터로 변환한다.\n",
    "3. **Projection Head**\n",
    "    - 두 인코드의 출력을 선형 변환을 통해 같은 차원으로 변환한다.\n",
    "    - 이렇게 생성된 이미지 임베딩과 텍스트 임베딩은 코사인 유사도를 통해 서로의 관련성을 비교할 수 있게 된다\n",
    "\n",
    "### 학습과정\n",
    "1. 한 배치에 N개의 **이미지, 텍스트쌍**이 포함된다\n",
    "2. 각 인코더를 통해 N개의 이미지 임베딩과 N개의 텍스트 임베딩을 생성한다\n",
    "3. 모든 이미지-텍스트 조합에 대해 N×N개의 코사인 유사도를 계산한다\n",
    "4. 대조손실 함수를 이용해 올바른 쌍의 유사도는 최대화하고, 잘못된 쌍의 유사도는 최소화하도록 학습한다\n",
    "\n",
    "- 많은 양의 데이터 쌍을 학습하면서 **이미지와 텍스트 간의 의미적 관련성을 학습**하게 된다.\n",
    "\n",
    "![clip](figures/clip1.png)\n",
    "\n",
    "![clip](figures/clip2.png)\n",
    "\n",
    "### 활용 분야\n",
    "- **이미지 검색과 분류**\n",
    "    - 자연어 질의를 통한 이미지 검색이 가능하며, 유튜브나 넷플릭스 같은 영상 플랫폼에서 특정 장면을 텍스트로 검색하는 데 활용될 수 있다.\n",
    "- **멀티모달 시스템의 기반**\n",
    "    -  CLIP은 텍스트와 이미지를 연결하는 기반 모델로서, 이미지 생성, 편집, 캡셔닝 등 다양한 멀티모달 AI 시스템의 핵심 구성 요소로 사용된다\n",
    "- CLIP 모델은 이미지 검색, 이미지 분류, Text to Image 생성등의 작업에 backbone등으로 활용될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Stable Diffusion은 CLIP의 Text encoder를 사용**함.\n",
    "    - 1.5에서는 CLIP-ViT-L/14 모델을 사용.\n",
    "    - 토크나이저: BPE 기반\n",
    "    - 최대 입력 토큰수: 77 토큰\n",
    "    - 텍스트 인코더는 입력 텍스트 프롬프트를 텍스트 임베딩 벡터로 변환하는 역할을 수행하며, 임베딩 벡터를 디퓨전 프로세스에 사용한다.\n",
    "    - 텍스트 인코더는 Transformer 기반으로 12개의 Layer를 연결해서 구성\n",
    "      - **CLIP Skip**설정\n",
    "          - 12개의 layer를 얼마나 통과할 지 설정\n",
    "          - 1: 모든 layer를 다 통과한다.\n",
    "          - 2: 마지막 Layer를 skip, 3: 마지막 2개 Layer를 skip, ... 한다.\n",
    "          - layer를 많이 skip 할 수록\n",
    "              - 이미지 생성시 프롬프트 반영정도가 약해진다. 그래서 이미지의 다양성이 증가할 수지만 프롬프트에 명시된 세부정보를 제대로 반영 못하게 된다.\n",
    "              - 이미지 생성속도가 빨라진다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
